\section{Реализация на Xeon Phi}
В 2012 году компания Intel представила первый сопроцессор с архитектурой Intel MIC (Intel® Many Integrated Core Architecture). Архитектура MIC позволяет использовать большое количество вычислительных ядер архитектуры x86 в одном процессоре. В результате для параллельного программирования могут быть использованы стандартные технологии, такие как OpenMP и MPI. 
\par
Архитектура Intel Xeon Phi поддерживает несколько режимов использования сопроцессора, которые можно комбинировать для достижения максимальной производительности в зависимости от характеристик решаемой задачи.
\par
В режиме Offload процессы MPI выполняются только на CPU, а на сопроцессоре происходит запуск отдельных функций, аналогично использованию графических ускорителей.
\par
В режиме MPI базовая система и каждый сопроцессор Intel Xeon Phi рассматриваются как отдельные равноправные узлы, и процессы MPI могут выполняться на центральных процессорах и сопроцессорах Xeon Phi в произвольных сочетаниях.
\subsection{Режим Offload}
Вначале рассмотрим ситуацию, когда проведение одного испытания является трудоемкой операцией. В этом случае ускоритель Xeon Phi может быть использован в режиме Offload для параллельного проведения сразу многих испытаний на одной итерации метода. Пересылки данных от CPU к Xeon Phi будут минимальны --- требуется лишь передать на сопроцессор координаты точек испытаний, и получить обратно значения функции в этих точках. Функции, определяющие обработку результатов испытаний в соответствии с алгоритмом и требующие работы с большим объемом накопленной поисковой информацией, могут быть эффективно реализованы на CPU. Общая схема организации вычислений с использованием Xeon Phi будет следующий.
\par
На CPU выполняются шаги 1 – 4 параллельного алгоритма глобального поиска из п. 2. При этом на каждой итерации происходит накопление координат точек испытания в буфере, и этот буфер передается на сопроцессор. На Xeon Phi выполняется параллельное вычисление значений функции в этих точках (шаг 5 алгоритма). Используется OpenMP распараллеливание цикла, на каждой итерации которого происходит вычисление значений функции. По завершению испытаний происходит передача вычисленных значений функции на CPU. 
\subsection{Режим MPI}
В случае, если проведение одного поискового испытания является относительно простой операцией, параллельное проведение многих итераций в режиме Offload не дает большого ускорения (сказывается влияние накладных расходов на передачу данных). Однако здесь можно увеличить вычислительную нагрузку на Xeon Phi, если применить блочную схему редукции размерности из п. 3.2, а для решения возникающих подзадач использовать сопроцессор в режиме MPI.
\par
Для организации параллельных вычислений будем использовать небольшое (2-3) число уровней вложенности в блочной схеме, при котором исходная задача большой размерности разбивается на 2-3 вложенные подзадачи меньшей размерности. Тогда, применяя в блочной рекурсивной схеме (\ref{blockNested}) для решения вложенных подзадач (\ref{subTasks}) параллельный алгоритм глобальной оптимизации, мы получим схему параллельных вычислений с широкой степенью вариативности (например, можно варьировать количество процессоров на различных уровнях оптимизации, т.е. при решении подзадач по различным переменным \(u_i\)).
\par
Общая схема организации вычислений с использованием нескольких узлов кластера и нескольких сопроцессоров состоит в следующем. Процессы параллельной программы образуют дерево, соответствующее уровням вложенных подзадач, при этом вложенные подзадачи
\begin{displaymath}
\phi_i(u_1,\dots,u_i)=\min_{u_{i+1}\in D_{i+1}}\phi_{i+1}(u_1,\dots,u_i,u_{i+1})
\end{displaymath}
при \(i=1,\dots,M-2\) решаются только с использованием CPU. Непосредственно в данных подзадачах вычислений значений оптимизируемой функции не происходит: вычисление значения функции \(\phi_i(u_1,\dots,u_i)\) --- это решение задачи минимизации следующего уровня. Каждая подзадача решается в отдельном процессе; обмен данными осуществляется лишь между процессами-предками и процессами-потомками.
\par
Подзадача последнего \((M–1)\)-го уровня 
\begin{displaymath}
\phi_{M-1}(u_1,\dots,u_{M-1})=\min_{u_{M}\in D_{M}}\phi_{M}(u_1,\dots,u_i,u_{M})
\end{displaymath} 
отличается от всех предыдущих подзадач --- в ней происходит вычисление значений оптимизируемой функции, т.к. \(\phi_M(u_1,\dots,u_M)=\phi(y_1,\dots,y_N)\). Подзадачи этого уровня решаются на сопроцессоре, и каждое ядро сопроцессора будет решать свою подзадачу \((M–1)\)-го уровня в отдельном MPI-процессе.
\par
Самый простой вариант использования данной схемы будет соответствовать двухкомпонентному вектору распараллеливания \(\pi=(\pi_1,\pi_2)\). Здесь \(\pi_1+1\) будет соответствовать числу MPI-процессов на CPU, а \(\pi_2\) --- числу MPI-процессов на Xeon Phi; тем самым общее количество процессов будет определяться как \(1+\pi_1+\pi_1\pi_2\).
\par
Отметим, что синхронный параллельный алгоритм глобальной оптимизации, описанный в п.3, в сочетании с блочной схемой редукции размерности обладает существенным недостатком, связанным с возможными простоями всех процессов, кроме корневого. Простои могут возникать в том случае, если часть потомков некоторого процесса закончили решение своих подзадач и отправили данные родителю раньше остальных. Для преодоления данной проблемы был применен предложенный в \cite{examinArtcle} асинхронный вариант параллельного алгоритма. Конкретные детали реализация блочной схемы в сочетании с асинхронным алгоритмом описаны в \cite{examinArtcle}.
