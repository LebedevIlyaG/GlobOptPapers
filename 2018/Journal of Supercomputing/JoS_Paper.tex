%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn

\usepackage[utf8x]{inputenc}
\usepackage[english,russian]{babel}


\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{multirow}
%
%\usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
 \journalname{The Journal of Supercomputing}
%
\begin{document}

\title{Parallel Computing for Constrained Global Optimization Problems
\thanks{The study was supported by the Russian Science Foundation, project No 16-11-10150.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Konstantin Barkalov          \and
        Ilya Lebedev %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{K. Barkalov  \at
               Lobachevsky State University of Nizhni Novgorod, Nizhni Novgorod, Russia \\
              \email{konstantin.barkalov@itmm.unn.ru} 
           \and
           I. Lebedev \at
					Lobachevsky State University of Nizhni Novgorod, Nizhni Novgorod, Russia \\
							\email{ilya.lebedev@itmm.unn.ru} 
}


\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
This paper considers a parallel algorithm for solving multiextremal problems with non-convex constrains. Within the framework of the approach proposed here, the solution of the initial multidimensional problem is reduced to solving a set of linked one-dimensional problems in parallel. The parallelization scheme does not require a single control process since the solution of any one-dimensional problem is a solution of the initial multidimensional one. The non-convex constraints are accounted for by a novel index scheme without utilizing the ideas of penalty functions. The efficiency of the parallel algorithm was confirmed experimentally by solving numerically several hundred multidimensional multiextremal problems with non-convex constrains generated randomly.
\keywords{Global optimization \and Constrained problems \and Non-convex constraints \and Dimensionality reduction \and Parallel algorithms}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

The aim of this paper is to examine parallel methods for solving global optimization problems with non-convex constraints. The objective function and constraints are assumed to satisfy the Lipschitz condition with a priori unknown Lipschitz constants. The analytical form of the problem's functions may also be unknown, i.e. they can be set by an algorithm computing their values at various points within the search domain (the so-called ``black-box'' functions). It is also supposed that even a single computation of the function value can be time-consuming, since it requires performing numerical simulation in applied problems. 

The numerical algorithms for solving the problems of the above-mentioned class are noted for an exponential growth of the computational costs with increasing dimensionality (the so-called ``curse of dimensionality''). The use of supercomputers expands the field of application of the global optimization methods and, however, at the same time, it gives rise to the problem of efficient parallelization of the search process.

Parallel algorithms utilizing heuristic approaches and based in one way or another on the idea of random search are at present quite popular (see \cite{RefFerreiro,RefZhu,Korosec,Guerrero}). However, because of their probabilistic nature, the algorithms of such type guarantee the convergence to the global optimum solution in the probabilistic sense only, which makes them less attractive than deterministic parallel global optimization algorithms \cite{Jones2001,Paulavicius2011,Evtushenko2013,Strongin2000}. As a rule, efficient deterministic algorithms are intended to solve unconstrained optimization problems. The problems with constraints are solved using the penalty function method that has a number of disadvantages. In particular, this method cannot be applied to the problems with the \textit{partially defined functions} (i.e. when any constraint is violated, the values of all the other functions of the problem remain undefined).

\Russian
Данная ситуация является типичной для многих задач оптимального проектирования, когда при нарушении некоторых условий функционирования моделируемого объекта другие его характеристики оказываются неопределенными. Например, моделируемым объектом является некоторое электронное устройство, и минимизируется время его срабатывания. Однако время срабатывания является неопределенным, если при данном сочетании параметров устройство вообще не работает.

%Another example is the optimal control problem described through system of ordinary differential equations with a certain matrix $A$ on the right side \cite{Balandin2017}. It is only possible to calculate the optimality criteria for these problems if the matrix $A$ is a Hurwitz matrix, i.e. every eigenvalue of $A$ has a strictly negative real part. Otherwise, the value of the criteria is indeterminate.

In this study, we used a novel approach to the minimization of multiextremal functions with non-convex constraints developed in \cite{Strongin2000,Sergeyev2001,Barkalov2002}. The approach referred to as the index method of accounting for the constraints is based on accounting for each constraint of the problem separately and is not related to the use of the penalty functions. The solution of the initial multidimensional problem is reduced (by using the Peano-type space-filling curves) to solving an equivalent set of one-dimensional problems, which can be solved in parallel in such a way that each of these processes shares the information obtained by the other processes. The proposed scheme was implemented on the Lobachevsky supercomputer with the use of the distributed memory as well as the shared one.

The main part of the paper has the following structure. Section 2 states the constrained optimization problem, reviews the index method and an approach to reducing dimensionality by using Peano curves. Section 3 presents a parallel implementation of the index method using a set of space-filling curves. Section 4 presents the results of numerical experiments. Section 5 concludes the paper.


\section{Problem Statement}
\label{sec:2}

A constrained global optimization problem can be formulated as follows
%Let us consider the $N$-dimensional global optimization problem
\begin{equation}\label{problem}
\varphi(y^\ast)=\min{\left\{\varphi(y):y\in D, \; g_i(y)\leq 0, \; 1 \leq i \leq m\right\}},
\end{equation}
\begin{equation}\label{D}
D=\left\{y\in R^N: a_j\leq y_j \leq b_j, 1\leq j \leq N \right\}.
\end{equation}
The objective function $\varphi(y)$ (hereafter denoted by $g_{m+1}(y)$) and the left-hand sides $g_i(y), \; 1\leq i \leq m,$ of the constraints satisfy the Lipschitz condition 
\[
\left|g_i(y_1)-g_i(y_2)\right|\leq L_i\left\|y_1-y_2\right\|, \;1\leq i\leq m+1, \; y_1,y_2 \in D,\;
\]
with a priori unknown constants $L_i, \; 1 \leq i \leq m+1,$ and may be multiextremal. It is assumed that the functions $g_i(y)$ are defined and computable only at the points $y \in D$ satisfying the conditions
\begin{equation}\label{g_k}
g_k(y) \leq 0, \; 1 \leq k < i.
\end{equation}

By employing the continuous single-valued Peano curve $y(x)$ mapping the unit interval $[0,1]$ on the $x$-axis onto the $N$-dimensional domain (\ref{D}), it is possible to find the minimum in (\ref{problem}) by solving the one-dimensional problem
\[
\varphi(y(x^\ast))=\min \left\{\varphi(y(x)): x \in [0,1], \; g_i(y(x))\leq 0, \; 1 \leq i \leq m\right\}.
\]
Algorithms for numerical construction of Peano curve approximation (\textit{evolvent}) are considered in \cite{Sergeyev2013}. Due to (\ref{g_k}) the functions $g_i(y(x))$ are defined and computable in the subranges 
\[
Q_1=[0,1], \; Q_{i+1}=\left\{x \in Q_i : g_i(y(x)) \leq 0 \right\}, \; 1 \leq i \leq m.
\]

These conditions allows us to introduce a classification of the points $x \in [0,1]$ according to the number $\nu (x)$ of the constraints computed at this point. The \textit{index} $\nu(x)$ can also be defined by the conditions
\begin{equation}\label{nu}
g_i(y(x)) \leq 0, \; 1 \leq i < \nu, \; g_\nu(y(x))>0,
\end{equation}
where the last inequality is inessential if $\nu=m+1$.

In the dimensionality reduction scheme considered here, a multidimensional problem with Lipschitzian functions is juxtaposed with a one-dimensional problem, where the corresponding functions satisfy uniform H{\"o}lder condition (see \cite{Sergeyev2013}), i.e.,
\[
\left|g_i(y(x_1))-g_i (y(x_2))\right| \leq H_i \left|x_1-x_2 \right|^{1/N}, \; x_1,x_2\in [0,1], \; 1\leq i \leq m+1.
\]
Here, $N$ is the dimensionality of the initial multidimensional problem and the coefficients $H_i$ are related to the Lipschitz constant $L_i$ of the initial problem as $H_i \leq 2L_i \sqrt{N+3}$.

Thus, \textit{a trial} at a point $x^k \in [0,1]$ executed at the $k$-th iteration of the algorithm will consist of the following sequence of operations:
\begin{itemize}
	\item Determine the \textit{image} $y^k=y(x^k)$ in accordance with the mapping $y(x)$;
	\item Compute the values $g_1(y^k),..., g_\nu(y^k),$ where $\nu = \nu(x^k)$ is from (\ref{nu}). 
\end{itemize}
The dyad $ \{ \nu=\nu(x^k), \; z^k=g_\nu(y(x^k)) \} $ will be referred to as the \textit{trial outcome}.

The scheme of the serial index algorithm is as follows. The first trial is executed at an arbitrary internal point $x_1 \in (0,1)$. The selection of the point $x^{k+1}, \; k \geq 1,$ of any next trial is determined by the following steps.

Step 1. Renumber the points $x^1,...,x^k$ of the preceding trials by the lower indices in the increasing order of the coordinate values, i.e.
\[
0=x_0<x_1<\dots <x_k<x_{k+1}=1,
\]
and juxtapose to them the values $z_i=g_\nu(y(x_i)), \; \nu=\nu(x_i), \; 1 \leq i \leq k,$ computed at these points. The points $x_0=0$ and $x_{k+1}=1$ are introduced additionally and the values $z_0$ and $z_{k+1}$ are not defined.

Rule 2. For each interval ($x_{i-1},x_i), \; 1 \leq i \leq k+1,$ compute the \textit{characteristics} $R(i)$ using some formulae \cite{Barkalov2002}.

Rule 3. Find the interval $(x_{t-1},x_t)$ with the maximum characteristic
\begin{equation}\label{MaxR}
R(t)=\max{\left\{R(i): 1 \leq i \leq k+1\right\}}.
\end{equation}

Rule 4. Execute the next trial at the internal point of the interval $(x_{t-1},x_t)$, i.e. $x^{k+1} \in (x_{t-1},x_t)$.

Step 5. Check termination condition $\left|x_t-x_{t-1}\right|^{1/N}\leq \epsilon$, where $t$ is the number of interval with the maximum characteristic and $\epsilon > 0$ is the predefined accuracy.

A detailed description of this algorithm and the corresponding theory of convergence are presented in \cite{Strongin2000,Sergeyev2001,Barkalov2002}.

\Russian
Сравнение двух разных способа учета ограничений - классического метода штрафных функций и индексной схемы - приведено в статье \cite{Barkalov2017}. Экспериментально подтрвеждено превосходство индексного метода над методом штрафных функций как по числу итераций, так и по числу вычсислений ограничений и целевой функции. Для решения задачи с заданной точностью индексному методу требовалось в 3-5 раз меньше итераций (в зависимости от размерности задачи и доли допусимой области). Одновременно с этим значение целевой функции вычислялось в среднем лишь на каждой пятой итерации индексного метода, в отличие от метода штрафных функций, в котором значения целевой функции вычисляются на каждой итерации.


\section{Parallel Index Algorithm with a Set of Evolvents}
\label{sec:3}

The reduction of the multidimensional problems to the one-dimensional ones using evolvents has such important properties as the continuity and preservation of boundedness of function's divided differences. However, a partial loss of information on the nearness of the points in the multidimensional space occurs in this case since a point $x \in [0,1]$ has only the left and the right neighbors while the corresponding point $y(x) \in R^N$ has the neighbors in $2N$ directions. As a result, when using the mappings like a Peano curve, the close images $y', y'' \in D$ can correspond to the remote preimages $x', x'' \in [0,1]$. This property results in excess computations since several limit points $x', x''$ of the trial sequence generated by the index method in the interval $[0,1]$ can correspond to a single limit point $y$ in the $N$-dimensional space.

One of the possible ways to overcome this disadvantage consists in using the multiple mapping $Y^S(x)=\{ y^1 (x),...,y^S(x)\}$ instead of the single evolvent $y(x)$. To construct the set $Y^S(x)$, different approaches can be used. 
%For example, in \cite{Strongin2000} a scheme was implemented, according to which each evolvent $y^s(x)$ from $Y^S(x)$ is constructed as a result of shifting the original evolvent $y(x)$ along the main diagonal of the hypercube $D$. The set of Peano curves thus constructed allows one to obtain close preimages $x', x''$ from the interval $[0,1]$ for the evolvent $y^s(x), 1\leq s \leq S$, for any close multidimensional images $y', y''$ from $D$, which differ only in one coordinate.

%Проверить
For example, a scheme of building of the multiple mapping was proposed  in \cite{6_Gergel2009}. The building of a set of Peano curves by rotation of the evolvents around the coordinate origin is a distinctive feature of this scheme. In the initial non-rotated mapping for close points $y', y''$ in the multidimensional space their preimages  $x', x''$ in the interval $[0,1]$ can be far away from each other. In the rotated scheme there exists a mapping $y^i(x)$ according to which preimages $x', x''$ will be located nearer. The maximum number of various rotations of the evolvents mapping the $N$-dimensional hypercube onto a one-dimensional interval is $2^N$. The employment of all possible rotations might appear to be redundant. In this case, we can select only a part of all rotations. As a possible approach, we propose to generate the new evolvents by the rotation of the initial evolvent on the angle of $\pm\pi/2$ in each of the coordinate planes. Taking into account the initial evolvent the total number of such evolvents for mapping the $N$-dimensional domain onto the corresponding one-dimensional intervals equals to $N(N-1)+1$. This method for building a set of mappings can be ``scaled'' easily to obtain more evolvents (up to $2^N$) if necessary.
\Russian
Именно такой подход к построению multiple mapping был использован в данной статье при проведении вычислительных экспериментов. 

With the use of multiple mapping, it is possible to solve the initial problem (\ref{problem}) by solving in parallel the problems
\[
\min \left\{\varphi(y^s(x)): x \in [0,1], \; g_i(y^s(x))\leq 0, \; 1 \leq i \leq m\right\},\; 1\leq s \leq S.
\]
on a set of intervals $[0,1]$ by the index method. Each one-dimensional problem is solved on a separate processor/node. The trial outcome at the point $x^k$ obtained for the problem being solved by particular processor are interpreted as the outcomes of the trials in the remaining problems (at the corresponding points $x^{k_1},...,x^{k_S}$). In this approach, a trial at the point $x^k \in [0,1]$ executed in the framework of the $s$-th problem, consists of the following sequence of operations.

\begin{enumerate}
	
	\item Determine the image $y^k=y^s(x^k)$ for the evolvent $y^s(x)$.
	
	\item Inform the rest of processors about the start of the trial execution at the point $y^k$ (\textit{the blocking} of the point $y^k$).
	
	\item Compute the values $g_1(y^k),...,g_\nu(y^k)$, where the index $\nu \leq m$ is determined by the conditions
	\[
		g_i(y^k)\leq 0, \; 1 \leq i < \nu, \; g_\nu(y^k)>0, \; \nu \leq m.
	\]
The trial terminates at the point as soon as the first violation of any constraint occurs. In the case when $y^k$ is a feasible one, the trial includes the computation of the values of all problem functions. In this situation, the index is set to $\nu=m+1$. The triplet
\[
y^s(x^k), \; \nu=\nu(x^k), \; z^k=g_\nu(y^s(x^k))
\]
is the outcome of the trial at the point $x^k$.

\item Determine the preimages $x^{k_s} \in [0,1]$, $1 \leq s \leq S$, of the point $y^k$ and interpret the trial executed at the point $y^k \in D$ as the execution of the trials at the $S$ points  $x^{k_1},...,x^{k_S}$ with the same outcomes
\[
 \nu(x^{k_1}) = ... = \nu(x^{k_S}) = \nu(x^k),
\]
\[
 g_\nu(y^1(x^{k_1})) = ... = g_\nu(y^S(x^{k_S})) = z^k.
\]

\item Inform the rest of processors about the trial outcome at the point $y^k$.
	
\end{enumerate}

The decision rules for the proposed parallel algorithm, in general, are the same as the rules of the sequential algorithm (with the exception the method of the trial execution). To organize the interactions between the processors, the queues are created on each processor, where the processors store the information on the executed iterations in the form of tuples: the processor number $s$, the trial point $x^{k_s}$, the index $\nu(x^{k_s})$, and the value $g_\nu(y^s (x^{k_s} ) )$. Moreover, the index of the blocked point is assumed to be equal to $-1$; the function value at this point is undefined.

The proposed parallelization scheme was implemented with the use of MPI and OpenMP technologies. A separate MPI-process is created for each of $S$ one-dimensional problems being solved, one process per one processor employed. Each process uses $p$ threads for solving a one-dimensional problem, one thread per an accessible core.

\section{Results of Numerical Experiments}
\label{sec:4}

A well-known approach to investigating and comparing the multiextremal optimization algorithms is based on testing these methods by solving a set of problems, chosen randomly from some specially designed class.

In this study, we will use a GKLS generator of the functions of arbitrary dimensionality proposed in \cite{Gaviano2003} to produce constrained problems. The scheme for generating constrained global optimization problems is proposed in \cite{Gergel2017}. 
\Russian
Тестовые задачи с ограничениями, полученные с помощью данной схемы, обладают важным свойством, присущим прикладным задачам оптимизации: условный глобальный минимум находится на границе допустимой области, в то время как глобальный минимум целевой функции без ограничений находится вне допустимой области. При этом можно задавать объем допустимой области и  число ограничений, активных в точке минимума, тем самым усложняя или упрощая постановку задачи.

%Previous investigations have confirmed experimentally that the index method is not inferior to well-known analogues. A comparison of the method with the well known DIRECT method \cite{Jones2001} in solving unconstrained optimization problems has been performed in \cite{Barkalov2015}. 
In the present study, an experimental investigation is carried out of the speedup, which is obtained by the use of the index method in combination with the two-level parallelization scheme from \cite{Sidorov2015}.

The experiments have been carried out by solving a series of $100$ problems with two constraints and one objective function from the \textit{Simple} and \textit{Hard} GKLS classes with the dimensionalities $N=4$, $N=5$. The number of the used cluster nodes $S$ and, correspondingly, the number of evolvents as well as the number of cores $p$ employed at each node have been varied. The problem was considered to be solved, if the algorithm generated a trial point $y^k$ in the $\delta$-vicinity of the global minimum, i.e., $\left\|y^k-y^*\right\|\leq \delta = 0.03\left\|b-a\right\|$. For the purpose of simulation of the computational complexity inherent to applied optimization problems, calculation of the problem functions in all performed experiments was made more complex by additional calculations without changing the type of function and arrangement of its minima (a series summation of $100$ thousand elements).

Computational experiments were carried out on Lobachevsky supercomputer. The node of supercomputer included two Intel Sandy Bridge E5-2660 2.2 GHz CPUs and 64 Gb RAM. The CPU had 8 cores, i.e. each node had a total of 16 cores. The average time and the number of iterations, which were required to solve the problems of the series at various parallelization parameters are given in Tables 1 and 2. Here, \textit{Node(core)} are the numbers of employed  nodes and cores per node, respectively.

\begin{table}
	\caption{Average time}
	\label{tab:1}
	\center
	\begin{tabular}{cccccc}
		\hline\noalign{\smallskip}
		\multirow{2}{*}{\textit{Node(core)}} & \multicolumn{2}{c}{ $N=4$ } & & \multicolumn{2}{c}{$N=5$} \\
		\noalign{\smallskip} \cline{2-3} \cline{5-6} \noalign{\smallskip}
		 & \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard}  \\
		\noalign{\smallskip} \hline \noalign{\smallskip}
1(1)	&	220.5	&	334.8	&	&	1223.6	&	1386.6	\\
1(16)	&	31.3	&	49.1	&	&	211.8	&	547.2	\\
2(1)	&	158.4	&	260	&	&	1052.9	&	1458.1	\\
2(16)	&	22.1	&	35.9	&	&	227.5	&	603	\\
4(1)	&	127.7	&	286.4	&	&	951.3	&	1362.2	\\
4(16)	&	20.9	&	45	&	&	206	&	925.7	\\
8(1)	&	99.3	&	141.8	&	&	700.1	&	897.3	\\
8(16)	&	31	&	77.7	&	&	264.6	&	374	\\
12(1)	&	99.3	&	141.8	&	&	700.1	&	897.3	\\
12(16)	&	31	&	77.7	&	&	264.6	&	374	\\
		\noalign{\smallskip}\hline
	\end{tabular}
\end{table}

%\begin{table}
	%\caption{Average number of iterations}
	%\label{tab:2}
	%\center
	%\begin{tabular}{cccccc}
		%\hline\noalign{\smallskip}
		%\multirow{2}{*}{\textit{Node(core)}}	 & \multicolumn{2}{c}{ $N=4$ } & & \multicolumn{2}{c}{$N=5$} \\
		%\noalign{\smallskip} \cline{2-3} \cline{5-6} \noalign{\smallskip}
		 %& \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard}  \\
		%\noalign{\smallskip} \hline \noalign{\smallskip}
%1(1)	&	58320	&	84546	&	&	266943	&	287102	\\
%1(16)	&	4297	&	6601	&	&	22655	&	56754	\\
%2(1)	&	34791	&	52126	&	&	188465	&	241369	\\
%2(16)	&	2029	&	3239	&	&	16689	&	40763	\\
%4(1)	&	22223	&	47771	&	&	135734	&	180489	\\
%4(16)	&	1281	&	2483	&	&	9241	&	35024	\\
%8(1)	&	13844	&	18933	&	&	77748	&	94563	\\
%8(16)	&	608	&	1473	&	&	5820	&	23033	\\
%12(1)	&	13844	&	18933	&	&	77748	&	94563	\\
%12(16)	&	608	&	1473	&	&	5820	&	23033	\\
		%\noalign{\smallskip}\hline
	%\end{tabular}
%\end{table}

\begin{table}
	\caption{Average number of iterations}
	\label{tab:2}
	\center
	\begin{tabular}{ccccccccc}
		\hline\noalign{\smallskip}
		\multirow{2}{*}{\textit{Node(core)}}	 & \multicolumn{2}{c}{ $N=4$ } & & \multicolumn{2}{c}{$N=5$}& & \multicolumn{2}{c}{$N=6$} \\
		\noalign{\smallskip} \cline{2-3} \cline{5-6} \cline{8-9} \noalign{\smallskip}
		 & \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard}  \\
		\noalign{\smallskip} \hline \noalign{\smallskip}
1(1)	&	58320	&	84546	&	&	266943	&	287102 & & 1 & 2	\\
1(16)	&	4297	&	6601	&	&	22655	&	56754	& &  & \\
2(1)	&	34791	&	52126	&	&	188465	&	241369	& &  &\\
2(16)	&	2029	&	3239	&	&	16689	&	40763	& &  &\\
4(1)	&	22223	&	47771	&	&	135734	&	180489 & &  &	\\
4(16)	&	1281	&	2483	&	&	9241	&	35024	& &  &\\
8(1)	&	13844	&	18933	&	&	77748	&	94563	& &  &\\
8(16)	&	608	&	1473	&	&	5820	&	23033	& &  &\\
12(1)	&	13844	&	18933	&	&	77748	&	94563	& &  &\\
12(16)	&	608	&	1473	&	&	5820	&	23033	& &  &\\
		\noalign{\smallskip}\hline
	\end{tabular}
\end{table}


The results demonstrate the speedup achieved when using the shared memory at a node as well as the distributed memory. In this case, the highest time speedup was 10 (when using 64 cores on 4 cluster nodes), the highest iteration speedup was 95 (when using 128 cores on 8 cluster nodes). The difference in the speedups in time and in the number of iterations can be explained by the effect of the overheads of the data transmission between the processes. Note that when solving applied optimization problems, the computation of the problem function values even in one point is a computationally costly operation. In this case, the data transfer overheads will not affect significantly the total computational costs, and the difference between the time speedup and the iteration speedup will not be so great.


\section{Conclusions}
\label{sec:5}

\Russian
В заключение отметим, что подавляющее большинство оптимизационных алгоритмов (за исключением методов, основанных на идеях случайного поиска) реализуют стратегию целенаправленного выбора. Указанная стратегия состоит в том, чтобы на основе анализа небольшой части вариантов исключить из дальнейшего рассмотрения многие заведомо не перспективные случаи и сосредоточить дальнейший поиск в подмножестве, содержащем лучший вариант. 

Сложность моделируемых процессов и объектов может приводить к тому, что их характеристики не будут обладать свойством монотонности, что обуславливает недостаточность методов локального поиска для оценки наилучшего варианта. В таких задачах должны применяться методы глобального поиска, которые обеспечивают целенаправленность поиска за счет учета ограниченности изменения характеристик моделируемого объекта при ограниченных изменениях его параметров. Математическая формулировка этого факта может иметь форму Lipschitz condition, uniform H\"older condition и т.п. Именно к таким методам, способным решать сложные прикладные задачи оптимизации, относится рассмотренный в данной статье параллельный индексный алгоритм. 

The parallel index method for solving constrained global optimization problems %considered in this paper 
offers the following possibilities:
\begin{itemize}
	\item to solve the initial problem directly, without the use of the penalty functions (thus eliminating the need to select the penalty coefficient and to solve a series of unconstrained problems with different penalty coefficients);
	\item to solve the problems where the values of the problem function are not defined everywhere (for example, the objective function values are undefined outside the feasible domain);
	\item to use the two-level parallelization scheme with the shared and distributed memory proposed earlier for the unconstrained optimization methods.
\end{itemize}

The parallel algorithm has demonstrated a speedup with respect to the number of processors/cores employed. This was confirmed by the results of solving numerically of several hundred test problems using 128 cores of the Lobachevsky supercomputer. 

\Russian
Перспективным направлением дальнейшей работы будет являться применение множественных отображений для редукции размерности в задачах многокритериальной оптимизации. В подобных задачах построение множества Парето требует решения целой серии скалярных задач. При их параллельном решении можно ожидать более высокого ускорения, чем при решении одной скалярной задачи. 




% Non-BibTeX users please use
\begin{thebibliography}{}

\bibitem{RefFerreiro}
Ferreiro, A.M., Garcia, J.A., Lopez-Salas, J.G., Vazquez, C.: An efficient implementation of parallel simulated annealing algorithm in GPUs. J. Glob. Optim. 57(3), 863--890 (2013)

\bibitem{RefZhu}
Zhu, W.: Massively parallel differential evolution--pattern search optimization with graphics hardware acceleration: an investigation on bound constrained optimization problems. J. Glob. Optim. 50(3), 417--437 (2011).

\bibitem{Korosec}
Koro\v sec, P., Vajter\v sic, M.,  \v Silc, J., Kutil, R.: Multi-core implementation of the differential ant-stigmergy algorithm for numerical optimization. J. Supercomput. 63(3), 757--772 (2013)

\bibitem{Guerrero}
Guerrero, G.D., Cecilia, J.M., Llanes, A. et al.: Comparative evaluation of platforms for parallel Ant Colony Optimization. J. Supercomput. 69(1) 318--329 (2014)

\bibitem{Jones2001}
Jones, D.R.: The direct global optimization algorithm. In: Floudas, C.A., Pardalos, P.M. (eds.) The Encyclopedia of Optimization, Second Edition. pp. 725--735. Springer (2009)

\bibitem{Paulavicius2011}
Paulavi\v cius, R., \v Zilinskas, J., Grothey, A.: Parallel branch and bound for global optimization with combination of Lipschitz bounds. Optim. Methods Softw. 26(3), 487--498 (2011)

\bibitem{Evtushenko2013}
Evtushenko, Y.G., Posypkin, M.A.: A deterministic approach to global box-constrained optimization. Optim. Lett. 7(4), 819--829 (2013)

\bibitem{Strongin2000}
Strongin, R.G., Sergeyev, Ya.D.: Global Optimization with Non-Convex Constraints. Sequential and Parallel Algorithms. Kluwer Academic Publishers, Dordrecht (2000)

%\bibitem{Balandin2011} Balandin, D.V., Kogan, M.M.: Optimal linear-quadratic control: From matrix equations to linear matrix inequalities. Automation and Remote Control. 72(11), 2276--2284 (2011)

%\bibitem{Balandin2017}Balandin, D.V., Kogan, M.M.: Pareto-optimal generalized $H_2$-control and vibroprotection problems. Automation and Remote Control. 78(8), 1417--1429 (2017)


\bibitem{Sergeyev2001}
Sergeyev, Y.D., Famularo D., Pugliese P. Index Branch-and-Bound Algorithm for Lipschitz univariate global optimization with multiextremal constraints. J. Glob. Optim. 21(3), 317--341 (2001) 

\bibitem{Barkalov2002}
Barkalov, K.A., Strongin, R.G.: A global optimization technique with an adaptive order of checking for constraints. Comput. Math. Math. Phys. 42(9), 1289--1300 (2002)

\bibitem{Sergeyev2013}
Sergeyev, Ya.D., Strongin, R.G., Lera, D.: Introduction to Global Optimization Exploiting Space-Filling Curves. Springer (2013)

\bibitem{Gaviano2003}
Gaviano, M., Lera, D., Kvasov, D.E., Sergeyev, Ya.D.: Software for generation of classes of test functions with known local and global minima for global optimization. ACM Trans. Math. Software 29, 469--480 (2003)

\bibitem{Gergel2017}
Gergel, V.: An approach for generating test problems of constrained global optimization. LNCS. 10556, 314--319 (2017)

%\bibitem{Barkalov2015} Barkalov, K., Gergel, V., Lebedev, I.: Use of Xeon Phi coprocessor for solving global optimization problems. LNCS. 9251, 307--318 (2015)

\bibitem{Sidorov2015}
Gergel, V., Sidorov, S.: A two-level parallel global search algorithm for solution of computationally intensive multiextremal optimization problems. LNCS. 9251, 505--515 (2015)

\bibitem{Barkalov2017}
Barkalov, K., Lebeled, I.: Comparing Two Approaches for Solving Constrained Global Optimization Problems. LNCS. 10556, 301--306 (2017)

\bibitem{6_Gergel2009}
Strongin, R.G., Gergel, V.P., Barkalov, K.A.: Parallel methods for global optimization problem solving. Journal of instrument engineering. 52, 25--33 (2009) (In Russian)



%
%\bibitem{RefJ}
%% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
%% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
% etc
\end{thebibliography}

\end{document}
% end of file template.tex

