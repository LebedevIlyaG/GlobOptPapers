%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Sample document for preparing papers to  "Avtomatika i Telemekhanika"
%%  charset=windows-1251
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
\usepackage{graphicx}

\begin{document}  %%%!!!

\year{2019}
\title{НАЗВАНИЕ СТАТЬИ ИЛИ ЗАМЕТКИ БЫВАЕТ С ФОРМУЛАМИ $a+b=c$}%
\thanks{Исследование выполнено за счет гранта Российского научного фонда (проект \mbox{№\,16-11-10150}).}

\authors{Р.Г.~СТРОНГИН, д-р~физ.-мат.~наук\\
В.П.~ГГЕРГЕЛЬ, д-р~техн.~наук\\
К.А.~БАРКАЛОВ, канд.~физ.-мат.~наук\\
(Нижегородский государственный университет им. Н.И. Лобачевского)}

\maketitle

\begin{abstract}
Краткая аннотация статьи или заметки. Иногда не бывает. Хх
хххххххххххх хххххх хххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх ххх ххххх хххххх х хххххххххххххх ххх ххххххх
хххх.
\end{abstract}


\section{Введение}

В статье рассматриваются задачи глобальной оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y):y\in D\right\}},\\
& D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Предполагается, что целевая функция может быть многоэкстремальной, задана неявно (функция вида ``черный ящик''), а вычисление ее значений связано с решением задачи численного моделирования и является трудоемкой операцией.

Любая возможность достоверно оценить глобальный оптимум в многоэкстремальной задаче с функциями вида ``черный ящик'' принципиально связана априорной информацией, позволяющей связать возможные значения целевой функции с ее известными значениями в точках проведенных испытаний. Для многих прикладных задачах типичной является ситуация, когда ограниченное изменение вектора параметров $y$ вызывает ограниченное изменение значений $\varphi(y)$. Математической моделью, описывающей указанное предположение, является условие Липшица
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty.
\]
Отметим, что значение константы Липшица $L$ обычно является априори неизвестным, что делает оценку константы одной из ключевых проблем при построении методов липшицевой оптимизации.


Обзор методов Липшицевой оптимизации - взять из диссертации.


Одним из эффективных детерминированных методов решения задач многоэкстремальной оптимизации является \textit{информационно-статистический алгоритм глобального поиска}. Основы информационно-статистического подхода были заложены Ю.И. Неймарком \cite{Neymark66a,Neymark66b} и развиты Р.Г. Стронгиным \cite{Strongin78}. Впоследствии метод, изначально предложенный для решения безусловных задач, был обобщен для решения задач с невыпуклыми ограничениями \cite{Strongin86,Strongin87} и многокритериальных задач \cite{Strongin93}.


Информационно-статистический алгоритм - основы заложены Неймарком, развиты Стронгиным, обобщены учениками Стронгина.

В статье - последние наработки в данном направлении.
Редукция размерности - взять из WCGO

\section{Базовый алгоритм глобального поиска}



\section{Многошаговая схема редукции размерности}

Многошаговая схема редукции размерности (схема вложненной оптимизации) основана на известном соотношении
\begin{equation}\label{nested}
\min_{y \in D}\varphi(y) = \min_{y_1\in\left[a_1,b_1\right]}\min_{y_2\in\left[a_2,b_2\right]}...\min_{y_N\in\left[a_N,b_N\right]}\varphi(y),
\end{equation}
которое позволяет свести решение исходной многомерной задачи (\ref{main_problem}) к решению семейства рекурсивно связанных одномерных позадач.

Для формального описания многошаговой схемы введем семейство функций, определяемых в соответствии с соотношениями 
\begin{equation}\label{nested_N}
\varphi_N(y_1,...,y_N) \equiv \varphi(y_1,...,y_N),
\end{equation}
\begin{equation}\label{nested_i}
\varphi_i(y_1,...,y_i) = \min_{ y_{i+1} \in\left[a_{i+1},b_{i+1}\right]} \varphi_{i+1}(y_1,...,y_i,y_{i+1}), 1\leq i\leq N-1.
\end{equation}

Тогда, в соответствии с (\ref{nested}), для решения многомерной задачи (\ref{main_problem}) достаточно решить одномерную задачу  
\begin{equation}\label{nested_1}
\varphi^* = \min_{y_1\in\left[a_1,b_1\right]}\varphi_1(y_1).
\end{equation}
Однако каждое вычисление значения функции $\varphi_1$ в некоторой фиксированной точке $y_1$ предполагает решение одномерной задачи оптимизации второго уровня 
\begin{equation}
\varphi_1(y_1) = \min_{y_2\in\left[a_2,b_2\right]}\varphi_2(y_1,y_2).
\end{equation}
Вычисление значений функции $\varphi_2$, в свою очередь, требует одномерной минимизации функции $\varphi_3$, и т.д. вплоть до
решения задачи
\begin{equation}
\varphi_{N-1}(y_1,...,y_{N-1}) = \min_{ y_{N} \in\left[a_{N},b_{N}\right]} \varphi_{N}(y_1,...,y_{N}),
\end{equation}
на последнем уровне рекурсии.

Схема вложенной оптмизации (\ref{nested_N})--(\ref{nested_1}) в предположении липшицевости одномерных функций $\overline{\varphi}_i(y_i)=\varphi_i(y_1,...,y_i)$ послужила основой для обобщения методов одномерной оптимизации [7,25,32,34,36 из JOGO] на случай многих переменных [3,12,25,26,36 из JOGO].

Дадим более подробное описание взаимосвязей между одномерными подзадачами в многошаговой схеме. Для удобства последующего изложения введем обозначение
\[
v_i = (y_1,...y_i), \; 1 \leq i\ \leq N.
\]
Тогда одномерные функции $\overline{\varphi}_i(y_i), \; 1 < i\ \leq N$, порожденные в рамках многошаговой схемы, могут быть представлены в виде
\[
\overline{\varphi_i}(y)_i = \varphi_i(v_{i-1},y_i), \; 1 < i\ \leq N,
\]
где вектор $v_{i-1}$ фиксирован. 

В соответствии с многошаговой схемой на некотором шаге поиска $i, 1 \leq i \leq k_1$  алгоритм начинает минимизировать одномерную функцию $\varphi_1(y_1)$. Для этого, используя накопленную поисковую информацию, алгоритм выбирает точку очередной итерации $\widehat{y}_1$ и вычисляет значение функции в ней. Но для того, чтобы получить значение $\varphi_1(\widehat{y}_1)$ в этой точке, должна быть решена задача минимизации функции $\varphi_2(\widehat{y}_1,y_2)$. То есть, алгоритм приостанавливает минимизацию функции $\varphi_1(y_1)$ и начинает минимизировать $\varphi_2(\widehat{y}_1,y_2)$. В процессе минимизации алгоритм генерирует последовательность точек $\{y_2^j, 1 \leq j \leq k_2\}$, для каждой из которых необходимо решить задачу минимизации функции $\varphi_3(\widehat{y}_1,\widehat{y}_2,y_3)$, и т.д. Данный процесс выполняется рекурсивно до последнего $N$-го уровня. Возникающие при этом подзадачи и взаимосвязи между ними отображены на рис. \ref{fig1}.

\begin{figure}
\begin{center}%[scale=0.3][width=0.75\textwidth]
  \includegraphics[width=0.9\textwidth]{fig1.png} 
  \caption{Взаимосвязи между подзадачами в многошаговой схеме}\label{fig1} 
\end{center}
\end{figure}


Наглядно видно, что структура взаимосвязей имеет форму дерева. Указанная структура может меняться в процессе решения многомерной задачи (\ref{main_problem}): вычисление значения функции $\varphi_i(\widehat{v}_{i-1},y_i), \; 1\leq i < N,$  на $i$-м уровне рекурсии требует решения всех подзадач в одном из поддеревьев $(i+1)$-го уровня. При этом функции $\varphi_N(\widehat{v}_{N-1},y_N)$ на $N$-м уровне являются листьями дерева задач, их значения вычисляются непосредственно.  

%недостатки многошаговой схемы

\section{Адаптивная многошаговая схема}

Рассмотрим обобщение схемы вложенной оптимизации (\ref{nested_N})--(\ref{nested_1}), позволяющее преодолеть недостатки указанные выше. Основным отличием является отказ от принципа подчитенности одномерных подзадач на основе функций $\overline{\varphi_i}(y_i), \; 1\leq i \leq N,$ порожденных в рамках многошаговой схемы. В новом подходе все возникающие подзадачи предлагается решать одновременно. Предварительное описание \textit{адаптивной многошаговой схемы} редукции размерности заключается в следующем.







Введем следующее определение.

\begin{definition}
Хххххххххххххх хххххх-хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххх.
\end{definition}

Хххх ххххххх хххххххххххххх х ххххх ххххххххх ххх ххххххх хххх.

Рассмотрим следующую задачу.

\begin{problem} \label{prob:1}
Хххххх хххххххх хххххх хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххххххх х ххххх.
\end{problem}

Хххххх хххххххх хххххх хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххххххх х ххххх.


\section{Заголовок третьего раздела}

\subsection{Заголовок подраздела}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

Сформулируем следующую теорему.

\begin{theorem}[{\cite[c.\,123]{first}}] %
Пусть выполнены следующие условия:

\begin{ruslist}
\item
первое условие;

\item
второе условие.
\end{ruslist}

Тогда справедливы следующие утверждения:

\begin{enumlist}
\item
первое утверждение;

\item
второе утверждение.
\end{enumlist}
\end{theorem}

\begin{proof}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх:
\begin{gather}
    2\times 2=4.
\end{gather}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{proof}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

\begin{corollary}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххх х хххххххххххххх ххх ххххххх хххх.
\end{corollary}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.


\subsection{Заголовок следующего подраздела}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

\begin{lemma}[(см.\ {\cite[c.\,45]{second}})] \label{lm:1}
Хххххххххххххх хххххххххххххх х хххххххххх:
\begin{multline}
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
\\
    \times 2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2
\end{multline}
хххххххх ххххххх ххххххх х хххххххххххххх ххх.
\end{lemma}

Опишем следующий алгоритм.

\begin{algorithm}[(Быстрый)] \label{alg:1}
\ %%<-- этот пробел для того, чтобы первый элемент перечня был
%% на следующей строке, а не в подбор к заголовку окружения

\begin{enumlist}[.] % перечни, нумеруемые 1. 2. и т.д.
% \setcounter{enumlisti}{-1} % <-- эта команда нужна
%% для нумерации элементов перечня с нулевого
\item
Начать.

\item
Изменить.

\item
Закончить.
\end{enumlist}
\end{algorithm}

Следующая теорема утверждает сходимость алгоритма~\ref{alg:1}.

\begin{theorem}[(Теорема сходимости)] \label{th:2}
Алгоритм~{\rm\ref{alg:1}} сходится.
\end{theorem}

Можно привести замечание.

\begin{remark}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{remark}

\section{Четвертый раздел}

Данный раздел содержит несколько примеров различных окружений.

\begin{example}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

Некоторые перечни можно нумеровать русскими буквами:
\begin{ruslist} % перечни, нумеруемые а), б) и т.д.
\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хх
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх;

\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{ruslist}

А некоторые можно~"--- и латинскими буквами:

\begin{latlist} % перечни, нумеруемые а), б) и т.д.
\item
ххххххххххх ххх ххххххх хххх;

\item
хххххххххххх ххх ххххххх хххх.
\end{latlist}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{example}

Можно сформулировать утвеждение.

\begin{statement}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{statement}

Можно сформулировать предложение.

\begin{proposition}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{proposition}

\appendix{1}  % приложения можно нумеровать, если их несколько

\begin{proofoftheorem}{\ref{th:2}}
Докажем сначала, что выполнено следующее соотношение:
\begin{gather} \label{f(x)=y}
    f(x)=y.
\end{gather}
Действительно,~\dots откуда получаем, что
равенство~\eqref{f(x)=y} справедливо. Следовательно,~\dots
окончательно,~\dots

Теорема~\ref{th:2} доказана.
\end{proofoftheorem}

\begin{proofoflemma}{\ref{lm:1}}
Очевидно, что \dots Таким образом, \dots, что и требовалось
доказать.
\end{proofoflemma}


\begin{thebibliography}{10}

\bibitem{Neymark66a}
{\it Неймарк Ю.И., Стронгин Р.Г.} Поиск экстремума функций по принципу максимума информации // АиТ. 1966. № 11. С. 113--118.

\bibitem{Neymark66b}
{\it Неймарк Ю.И., Стронгин Р.Г.} Информационный подход к задаче поиска экстремума функций // Изв. АН СССР, Техническая кибернетика. 1966. № 1. С. 17--26.

\bibitem{Strongin78}
{\it Стронгин Р.Г.} Численные методы в многоэкстремальных задачах (информационно-статистические алгоритмы). М.: Наука, 1978.

\bibitem{Strongin86}
{\it Стронгин Р.Г., Маркин Д.Л.} Минимизация многоэкстремальных функций при невыпуклых ограничениях // Кибернетика. 1986. №4. С.63--69.

\bibitem{Strongin87}
{\it Маркин Д.Л., Стронгин Р.Г.} Метод решения многоэкстремальных задач с невыпуклыми ограничениями, использующий априорную информацию об оценках оптимума // Ж. вычисл. матем. и матем. физ. 1987. Т.27, № 1. С.52--61.

\bibitem{Strongin93}
{\it Маркин Д.Л., Стронгин Р.Г.} О равномерной оценке множества слабоэффективных точек в многоэкстремальных многокритериальных задачах оптимизации // Ж. вычисл. матем. и матем. физ. 1993 Т. 33, № 2. С. 195--205.


\bibitem{Strongin00}
{\it Strongin R.G., Sergeev Ya.D.} Global Optimization with Non-Convex Constraints. Sequential and Parallel Algorithms. Dordrecht: Kluwer Academic Publishers, 2000.

\bibitem{Strongin13}
{\it Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А.} Параллельные вычисления в задачах глобальной оптимизации. М.: Издательство Московского университета, 2013.



\bibitem{third}
{\it Третий Т.Т.}
Публикация в трудах конференции //
Тр. Ин-та такого-то РАН. 2000. Т. 1. № 2. С. 3--4.

\bibitem{forth}
{\it Четвертый Ч.Ч.}
Публикация по теме в серийном издании или сборнике /
Сб. научн. тр. к 20-летию Института. Новосибирск, Изд-во <<Пирогов>>, 2000. Т. 1. № 2. С. 3--4.

\bibitem{fifth}
{\it Fifth F.}
Some journal publication in English //
Appl. Math. Comput. J., Elsevier Publ. 1925. V. 501. No. 1. P. 1234--5678.

\bibitem{sixth}
{\it Sixth J.Th.}
A book in English. Boston: Springer, 1991.

\end{thebibliography}

\AdditionalInformation{Стронгин Р.Г.}{Нижегородский государственный университет им. Н.И. Лобачевского, президент, Нижний Новгород}{strongin@unn.ru}

\AdditionalInformation{Гергель В.П.}{Нижегородский государственный университет им. Н.И. Лобачевского, директор института информационных технологий, математики и механики, Нижний Новгород}{gergel@unn.ru}

\AdditionalInformation{Баркалов К.А.}{Нижегородский государственный университет им. Н.И. Лобачевского, доцент кафедры математического обеспечения и суперкомпьютерных технологий института информационных технологий, математики и механики, Нижний Новгород}{konstantin.barkalov@itmm.unn.ru}

\end{document}
