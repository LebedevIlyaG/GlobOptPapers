%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Sample document for preparing papers to  "Avtomatika i Telemekhanika"
%%  charset=windows-1251
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
\usepackage{graphicx}

\begin{document}  %%%!!!

\year{2019}
\title{НАЗВАНИЕ СТАТЬИ ИЛИ ЗАМЕТКИ БЫВАЕТ С ФОРМУЛАМИ $a+b=c$}%
\thanks{Исследование выполнено за счет гранта Российского научного фонда (проект \mbox{№\,16-11-10150}).}

\authors{Р.Г.~СТРОНГИН, д-р~физ.-мат.~наук\\
В.П.~ГГЕРГЕЛЬ, д-р~техн.~наук\\
К.А.~БАРКАЛОВ, канд.~физ.-мат.~наук\\
(Нижегородский государственный университет им. Н.И. Лобачевского)}

\maketitle

\begin{abstract}
Краткая аннотация статьи или заметки. Иногда не бывает. Хх
хххххххххххх хххххх хххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх ххх ххххх хххххх х хххххххххххххх ххх ххххххх
хххх.
\end{abstract}


\section{Введение}

В статье рассматриваются задачи глобальной оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y):y\in D\right\}},\\
& D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Предполагается, что целевая функция может быть многоэкстремальной, задана неявно (функция вида <<черный ящик>>), а вычисление ее значений связано с решением задачи численного моделирования и является трудоемкой операцией.

Любая возможность достоверно оценить глобальный оптимум в многоэкстремальной задаче с функциями вида <<черный ящик>> принципиально основана на априорной информации, позволяющей связать возможные значения целевой функции с ее известными значениями в точках проведенных испытаний. Для многих прикладных задачах типичной является ситуация, когда ограниченное изменение вектора параметров $y$ вызывает ограниченное изменение значений $\varphi(y)$. Математической моделью, описывающей указанное предположение, является условие Липшица
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty.
\]
%Отметим, что значение константы $L$ обычно является априори неизвестным, что делает ее оценку одной из ключевых проблем при построении методов липшицевой оптимизации.
Предположение липшицевости целевой функции типично для многих подходов к разработке оптимизационных алгоритмов. Первые методы липшицевой оптимизации были предложены в начале 70-х годов \cite{Evtushenko71,Pijavski72,Shubert72}; с тех пор данное направление продолжает активно развиваться \cite{Jones93,Pinter96,Zilinskas08,Evtushenko07,Evtushenko09}.
Например, многие известные методы основаны на различных способах разбиения области поиска на систему подобластей и последующего выбора наиболее перспективной подобласти для размещения очередного испытания (вычисления целевой функции). Результаты в данном направлении представлены в работах \cite{Jones09,Zilinskas10,Evtushenko13,Kvasov13,Paulavicius16}.
 
В настоящее время для решения задач оптимизации с функциями вида <<черный ящик>> широко используются генетические и популяционные алгоритмы (см., например, \cite{Kureichik04,Karpenko08}), которые так или иначе основаны на идеях случайного поиска. В силу простоты реализации и использования они получили большую популярность, однако по качеству работы (численной оценкой которого может служить число корректно решенных задач из некоторого набора) они существенно уступают детерминированным алгоритмам \cite{Kvasov18,Sergeyev18}.

%В последние годы для решения задач с функциями вида ``черный ящик'' получили широкое распространение так называемые биоинспирированные алгоритмы, основанные на моделировании природных процессов и явлений. Типичными примерами методов данного класса являются генетические и популяционные алгоритмы (см., например, ). В основе указанных алгоритмов так или иначе лежит случайный поиск, т.к. они все содержат случайную компоненту (например, операции скрещивания и мутации в генетических или миграции в популяционных алгоритмах). В силу своей вероятностной природы методы данного класса обладают, вообще говоря, всюду плотной сходимостью, что невыгодно отличает их от детерминированных алгоритмов.

Одним из эффективных детерминированных методов решения задач многоэкстремальной оптимизации является \textit{информационно-статистический алгоритм глобального поиска}. Основы информационно-статистического подхода были заложены Ю.И. Неймарком \cite{Neymark66a,Neymark66b} и развиты Р.Г. Стронгиным \cite{Strongin78}. Впоследствии метод, изначально предложенный для решения безусловных задач, был успешно обобщен для решения задач с невыпуклыми ограничениями \cite{Strongin86,Strongin87} и многокритериальных задач \cite{Strongin93}.

Разработанные методы основаны на редукции исходной многомерной задачи к эквивалентной одномерной или системе одномерных подзадач и последующим решением одномерных задач эффективными методами оптимизации функций одной переменной. Предложено две такие схемы: редукция на основе кривых, заполняющих пространство (кривых Пеано, или разверток) \cite{Strongin91,Strongin00}, и схема рекурсивной вложенной оптимизации (многошаговая схема) \cite{Grishagin97,Grishagin01}. В статье \cite{Grishagin16} предложена адаптивная многошаговая схема, существенно повышающая эффективность оптимизации по сравнению с базовым прототипом. В данной работе предлагается обобщение адаптивной схемы редукции размерности, комбинирующее использование вложенной оптимизации и кривых Пеано. При таком подходе вложенные подзадачи в адаптивной схеме могут быть как одномерными, так и многомерными. В последнем случае для редукции размерности вложенных подзадач используются развертки.


%Развитие - параллельность. предложены параллельные варианты многих алгоритмов. Отметим, что в основном распараллеливают генетические алгоритмы. В статье - последние наработки в направлении алгоритма глобального поиска. 



\section{Базовый алгоритм глобального поиска}



\section{Многошаговая схема редукции размерности}

Многошаговая схема редукции размерности (схема вложненной оптимизации) основана на известном соотношении
\begin{equation}\label{nested}
\min_{y \in D}\varphi(y) = \min_{y_1\in\left[a_1,b_1\right]}\min_{y_2\in\left[a_2,b_2\right]}...\min_{y_N\in\left[a_N,b_N\right]}\varphi(y),
\end{equation}
которое позволяет свести решение исходной многомерной задачи (\ref{main_problem}) к решению семейства рекурсивно связанных одномерных позадач.

Для формального описания многошаговой схемы введем семейство функций, определяемых в соответствии с соотношениями 
\begin{equation}\label{nested_N}
\varphi_N(y_1,...,y_N) \equiv \varphi(y_1,...,y_N),
\end{equation}
\begin{equation}\label{nested_i}
\varphi_i(y_1,...,y_i) = \min_{ y_{i+1} \in\left[a_{i+1},b_{i+1}\right]} \varphi_{i+1}(y_1,...,y_i,y_{i+1}), 1\leq i\leq N-1.
\end{equation}

Тогда, в соответствии с (\ref{nested}), для решения многомерной задачи (\ref{main_problem}) достаточно решить одномерную задачу  
\begin{equation}\label{nested_1}
\varphi^* = \min_{y_1\in\left[a_1,b_1\right]}\varphi_1(y_1).
\end{equation}
Однако каждое вычисление значения функции $\varphi_1$ в некоторой фиксированной точке $y_1$ предполагает решение одномерной задачи оптимизации второго уровня 
\begin{equation}
\varphi_1(y_1) = \min_{y_2\in\left[a_2,b_2\right]}\varphi_2(y_1,y_2).
\end{equation}
Вычисление значений функции $\varphi_2$, в свою очередь, требует одномерной минимизации функции $\varphi_3$, и т.д. вплоть до
решения задачи
\begin{equation}
\varphi_{N-1}(y_1,...,y_{N-1}) = \min_{ y_{N} \in\left[a_{N},b_{N}\right]} \varphi_{N}(y_1,...,y_{N}),
\end{equation}
на последнем уровне рекурсии.

Схема вложенной оптмизации (\ref{nested_N})--(\ref{nested_1}) в предположении липшицевости одномерных функций $\overline{\varphi}_i(y_i)=\varphi_i(y_1,...,y_i)$ послужила основой для обобщения методов одномерной оптимизации [7,25,32,34,36 из JOGO] на случай многих переменных [3,12,25,26,36 из JOGO].

Дадим более подробное описание взаимосвязей между одномерными подзадачами в многошаговой схеме. Для удобства последующего изложения введем обозначение
\[
v_i = (y_1,...y_i), \; 1 \leq i\ \leq N.
\]
Тогда одномерные функции $\overline{\varphi}_i(y_i), \; 1 < i\ \leq N$, порожденные в рамках многошаговой схемы, могут быть представлены в виде
\[
\overline{\varphi_i}(y)_i = \varphi_i(v_{i-1},y_i), \; 1 < i\ \leq N,
\]
где вектор $v_{i-1}$ фиксирован. 

В соответствии с многошаговой схемой на некотором шаге поиска $i, 1 \leq i \leq k_1$  алгоритм начинает минимизировать одномерную функцию $\varphi_1(y_1)$. Для этого, используя накопленную поисковую информацию, алгоритм выбирает точку очередной итерации $\widehat{y}_1$ и вычисляет значение функции в ней. Но для того, чтобы получить значение $\varphi_1(\widehat{y}_1)$ в этой точке, должна быть решена задача минимизации функции $\varphi_2(\widehat{y}_1,y_2)$. То есть, алгоритм приостанавливает минимизацию функции $\varphi_1(y_1)$ и начинает минимизировать $\varphi_2(\widehat{y}_1,y_2)$. В процессе минимизации алгоритм генерирует последовательность точек $\{y_2^j, 1 \leq j \leq k_2\}$, для каждой из которых необходимо решить задачу минимизации функции $\varphi_3(\widehat{y}_1,\widehat{y}_2,y_3)$, и т.д. Данный процесс выполняется рекурсивно до последнего $N$-го уровня. Возникающие при этом подзадачи и взаимосвязи между ними отображены на рис. \ref{fig0}.

\begin{figure}
\begin{center}%[scale=0.3][width=0.75\textwidth]
  \includegraphics[width=0.9\textwidth]{fig1.png} 
  \caption{Взаимосвязи между подзадачами в многошаговой схеме}\label{fig0} 
\end{center}
\end{figure}


Наглядно видно, что структура взаимосвязей имеет форму дерева. Указанная структура может меняться в процессе решения многомерной задачи (\ref{main_problem}): вычисление значения функции $\varphi_i(\widehat{v}_{i-1},y_i), \; 1\leq i < N,$  на $i$-м уровне рекурсии требует решения всех подзадач в одном из поддеревьев $(i+1)$-го уровня. При этом функции $\varphi_N(\widehat{v}_{N-1},y_N)$ на $N$-м уровне являются листьями дерева задач, их значения вычисляются непосредственно.  

%недостатки многошаговой схемы

\section{Адаптивная многошаговая схема}

Рассмотрим обобщение схемы вложенной оптимизации (\ref{nested_N})--(\ref{nested_1}), позволяющее преодолеть недостатки указанные выше. Основным отличием является отказ от принципа подчитенности одномерных подзадач на основе функций $\overline{\varphi_i}(y_i), \; 1\leq i \leq N,$ порожденных в рамках многошаговой схемы. В новом подходе все возникающие подзадачи предлагается решать одновременно. Предварительное описание \textit{адаптивной многошаговой схемы} редукции размерности заключается в следующем.


%взять первый вариант из статьи в JoGO
\section{Результаты экспериментов}

Стандартный подход к сравнению алгоритмов глобальной оптимизации основан на решении указанными методами серии задач, выбранных случайно из некоторого класса.
Будем использовать два класса многоэкстремальных тестовых функций (GKLS \textit{Simple}, GKLS \textit{Hard}), описанные в \cite{Gaviano03}.

В работах \cite{Grishagin18,Sovrasov19} было показано, что алгоритм глобального поиска (АГП) как с использованием разверток, так и в сочетании с адаптивной схемой редукции размерности превосходит многие известные алгоритмы глобальной оптимизации, включая методы DIRECT и DIRECT\textit{l}. Поэтому в данном исследовании мы ограничимся сравнением вариантов АГП в сочетании с различными схемами редукции размерности.

Для сравнения эффективности работы алгоритмов будем использовать два критерия: среднее число испытаний и операционные характеристики.
Операционной характеристикой алгоритма называется функция $P(k)$, определяемая как доля задач из рассматриваемой серии, для решения которых потребовалось не более чем $k$ испытаний.
Задачу будем считать решенной, если алгоритм сгенерировал точку $y^k$ очередного испытания в окрестности решения $y^\ast$, т.е. $\left|y^k-y^\ast\right| < \delta \left\|b-a\right\|$, где $\delta = 0.01$, $a$ и $b$ --- границы области поиска $D$.

Первая серия экспериментов была проведена на двумерных задачах классов GKLS \textit{Simple} и GKLS \textit{Hard} (100 функций каждого класса). В табл.~\ref{tab1}  представлено среднее число испытаний, выполненных АГП с использованием разверток ($K_e$), многошаговой схемы ($K_n$) и адаптивной многошаговой схемы ($K_a$). На рис.~\ref{fig2}(a,b) приведены операционные характеристики алгоритмов, полученные на указанных классах задач. Непрерывная линия соответствует алгоритму с использованием разверток, короткий пунктир --- адаптивной многошаговой схемы, длинный пунктир -- многошаговой схемы. Результаты экспериментов показывают, что АГП с использованием адаптивной многошаговой схемы показывает примерно одинаковое быстродействие  по сравнению с развертками, и оба они значительно превосходят алгоритм, использующий многошаговую схему. Поэтому в дальнейших экспериментах мы ограничимся сравнением различных вариантов адаптивной схемы редукции размерности.

\begin{table}
\centering
\caption{Среднее число испытаний для двумерных задач}\label{tab1}
\begin{tabular}{lccc}
\hline\noalign{\smallskip}
 & GKLS \textit{Simple} &  GKLS \textit{Hard} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 $K_e$ &  252 & 674 \\
 $K_n$ &  697 & 1252 \\
 $K_a$ &  279 & 815 \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{figure}
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{2DSimple.pdf} \\ (a)}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{2DHard.pdf} \\ (b)}
\end{minipage}
\caption{Operating characteristics using 2d GKLS \textit{Simple} (a) and \textit{Hard} (b) classes}
\label{fig2}
\end{figure}

Вторая серия экспериментов была проведена на четырехмерных задачах классов GKLS \textit{Simple} и GKLS \textit{Hard} (100 функций каждого класса). В табл.~\ref{tab2}  представлено среднее число испытаний, выполненных АГП с использованием адаптивной многошаговой схемы ($K_a$) и блочной адаптивной схемы ($K_ba$) с формированием двух уровней подзадач одинаковой размерности $N_1=N_2=2$. Отметим, что при использовании исходного варианта адаптивной многошаговой схемы при решении задачи размерности $N=4$ формируется четыре уровня одномерных подзадач, что усложняет их обработку.

На рас.~\ref{fig3}(a,b) приведены операционные характеристики алгоритмов, полученные на классах GKLS \textit{Simple} и GKLS \textit{Hard}. Пунктирная линия соответствует алгоритму с использованием адаптивной, а непрерывная ---  блочной адаптивной схемы. 
Результаты экспериментов показывают, что использование блочной адаптивной схемы дает ощутимый выигрыш по числу испытаний (до 35\%) по сравнению с исходной схемой редукции размерности.

\begin{table}
\centering
\caption{Среднее число испытаний для четырехмерных задач}\label{tab2}
\begin{tabular}{lcc}
\hline\noalign{\smallskip}
 &    GKLS \textit{Simple} &  GKLS \textit{Hard} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 $K_a$  &  21 747 & 35 633 \\
 $K_{ba}$ &  15 942 & 33 206 \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{figure}
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{4DSimple.pdf} \\ (a)}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{4DHard.pdf} \\ (b)}
\end{minipage}
\caption{Операционные характеристики на классах задач GKLS \textit{Simple} (a) и \textit{Hard} (b)}
\label{fig3}
\end{figure}

\section{Заключение}

В данной работе предложена обобщенная адаптивная схема редукции размерности для задач глобальной оптимизации, комбинирующая использование кривых Пеано и схему вложенной (рекурсивной) оптимизации. Для решения редуцированных подзадач меньшей размерности используется алгоритм глобального поиска. Приведена вычислительная схема алгоритма, рассмотрены основные вопросы, связанные с использованием адаптивной схемы редукции размерности. Проведены вычислительные эксперименты на серии тестовых задач с целью сравнения эффективности различных схем редукции размерности. 
Результаты экспериментов показывают, что использование блочной адаптивной схемы редукции размерности может значительно сократить число испытаний, необходимое для решения задачи с заданной точностью. 

\begin{thebibliography}{10}


\bibitem{Evtushenko71}
{\it	Евтушенко Ю.Г.} Численный метод поиска глобального экстремума функций (перебор на неравномерной сетке) // Ж. вычисл. матем. и матем. физ. 1971. T. 11, №6, С. 1390--1403.

\bibitem{Pijavski72}
{\it	Пиявский С.А.} Один алгоритм отыскания абсолютного экстремума функций // Ж. вычисл. матем. и матем. физ. 1972. Т. 12, № 4. С. 888--896.

\bibitem{Shubert72}
{\it Shubert B.O.} A sequential method seeking the global maximum of a function // SIAM J. Numer. Anal. 1972. V. 9. P. 379–388.

\bibitem{Jones93} 
{\it Jones D.R., Perttunen C.D., Stuckman B.E.} Lipschitzian optimization without the Lipschitz constant // J. Optim. Theory Appl. 1993. V. 79. No. 1. P. 157--181.

\bibitem{Pinter96}
{\it Pinter J. D.} Global Optimization in Action (Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications). Dordrecht: Kluwer Academic Publishers, 1996.

\bibitem{Zilinskas08}
{\it {\v Z}ilinskas J.} Branch and bound with simplicial partitions for global optimization // Math. Model. Anal. 2008. V. 13. No. 1. P. 145--159.

\bibitem{Evtushenko07}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И. А.} Распараллеливание процесса поиска глобального экстремума // Автомат. и телемех. 2007. № 5. C. 46--58

\bibitem{Evtushenko09}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И. А.} Параллельный поиск глобального экстремума функций многих переменных // Ж. вычисл. матем. и матем. физ. 2009. Т. 49, №2. C. 255--269.


\bibitem{Jones09}%Проверить формат ссылки!!!
{\it Jones D. R.} The direct global optimization algorithm. In: Floudas C. A., Pardalos P. M. (eds.) The Encyclopedia of Optimization, Second Edition.  Springer. 2009. P. 725--735.

\bibitem{Zilinskas10}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J., Grothey A.}: Investigation of selection strategies in branch and bound algorithm with simplicial partitions and combination of Lipschitz bounds // Optim. Lett. 2010. V. 4(1). P. 173–--83

\bibitem{Evtushenko13}
{\it Evtushenko Y. G., Posypkin M. A.} A deterministic approach to global box-constrained optimization // Optim. Lett. 2013. V. 7(4). P. 819--829

\bibitem{Kvasov13}
{\it Квасов Д.Е., Сергеев Я.Д.} Методы липшицевой глобальной оптимизации в задачах управления // Автомат. и телемех. 2013. № 9. C. 3--19.

\bibitem{Paulavicius16}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J.} Advantages of simplicial partitioning for Lipschitz optimization problems with linear constraints // Optim. Lett. 2016. V. 10(2). P. 237--246.

\bibitem{Kureichik04}
{\it Гладков Л.А., Курейчик В.В., Курейчик В.М.} Генетические алгоритмы. Учебное пособие. М.: ФИЗМАТЛИТ, 2004.

\bibitem{Karpenko08}
{\it Карпенко А.П.} Современные алгоритмы поисковой оптимизации. Алгоритмы, вдохновленные природой: учебное пособие. М.: Издательство МГТУ им. Н. Э. Баумана, 2014.

\bibitem{Kvasov18}
{\it Kvasov D.E., Mukhametzhanov M.S.} Metaheuristic vs. deterministic global optimization algorithms: The univariate case // Appl. Math. Comput. 2018. V. 318. P. 245--259.

\bibitem{Sergeyev18}
{\it Sergeyev Y., Kvasov D., Mukhametzhanov M.} On the efficiency of nature-inspired metaheuristics in expensive global optimization with limited budget // Sci.
Rep. V. 8(1). Art. No. 435.

\bibitem{Neymark66a}
{\it Неймарк Ю.И., Стронгин Р.Г.} Поиск экстремума функций по принципу максимума информации // Автомат. и телемех. 1966. № 11. С. 113--118.

\bibitem{Neymark66b}
{\it Неймарк Ю.И., Стронгин Р.Г.} Информационный подход к задаче поиска экстремума функций // Изв. АН СССР, Техническая кибернетика. 1966. № 1. С. 17--26.

\bibitem{Strongin78}
{\it Стронгин Р.Г.} Численные методы в многоэкстремальных задачах (информационно-статистические алгоритмы). М.: Наука, 1978.

\bibitem{Strongin86}
{\it Стронгин Р.Г., Маркин Д.Л.} Минимизация многоэкстремальных функций при невыпуклых ограничениях // Кибернетика. 1986. №4. С.63--69.

\bibitem{Strongin87}
{\it Маркин Д.Л., Стронгин Р.Г.} Метод решения многоэкстремальных задач с невыпуклыми ограничениями, использующий априорную информацию об оценках оптимума // Ж. вычисл. матем. и матем. физ. 1987. Т.27, № 1. С.52--61.

\bibitem{Strongin93}
{\it Маркин Д.Л., Стронгин Р.Г.} О равномерной оценке множества слабоэффективных точек в многоэкстремальных многокритериальных задачах оптимизации // Ж. вычисл. матем. и матем. физ. 1993 Т. 33, № 2. С. 195--205.

\bibitem{Strongin91}
{\it Стронгин Р.Г.} Параллельная многоэкстремальная оптимизация с использованием множества разверток // Ж. вычисл. матем. и матем. физ. 1991. T. 31, № 8. С. 1173--1185.

\bibitem{Strongin00}
{\it Strongin R.G., Sergeev Ya.D.} Global Optimization with Non-Convex Constraints. Sequential and Parallel Algorithms. Dordrecht: Kluwer Academic Publishers, 2000.

\bibitem{Strongin13}
{\it Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А.} Параллельные вычисления в задачах глобальной оптимизации. М.: Издательство Московского университета, 2013.

\bibitem{Grishagin97}

{\it Grishagin V.A., Sergeyev Y.D., Strongin R.G.} Parallel characteristical algorithms for solving problems of global optimization // J. Glob. Optim. 1997. V. 10(2). P. 185--206.

\bibitem{Grishagin01}
{\it Sergeyev Y., Grishagin V.} Parallel asynchronous global search and the nested optimization scheme // J. Comput. Anal. Appl. 2001. V. 3(2). P. 123--145.

\bibitem{Grishagin16}
{\it Gergel V., Grishagin V., Gergel A.} Adaptive nested optimization scheme for multidimensional global search // J. Glob. Optim. 2016. V. 66(1). P. 35--51.

\bibitem{Gaviano03}
{\it Gaviano M., Lera D., Kvasov D.E., Sergeyev Ya.D.} Software for generation of classes of test functions with known local and global minima for global optimization // ACM Trans. Math. Software. 2003. V. 29. P. 469--480.

\bibitem{Grishagin18}
{\it Grishagin V., Israfiov R., Sergeyev Y.} Convergence conditions and numerical comparison of global optimization methods based on dimensionality reduction schemes // Appl. Math. Comput. 2018. V. 318. P. 270--280.

\bibitem{Sovrasov19}
{\it Sovrasov V.} Comparison of several stochastic and deterministic derivative-free global optimization algorithms // Lecture Notes in Computer Science. 2019. V. 11548. P. 70--81. 

%\bibitem{Sergeyev00} Kvasov, D.E., Pizzuti, C., Sergeyev, Y.D.: Local tuning and partition strategies for diagonal GO methods. Numer. Math. 94(1), 93--106 (2003)
%\bibitem{Sergeyev06} Sergeyev, Ya.D., Kvasov, D.E.: Global search based on efficient diagonal partitions and a set of Lipschitz constants. SIAM J. Optim. 16(3), 910--937 (2006)

%\bibitem{third} {\it Третий Т.Т.} Публикация в трудах конференции // Тр. Ин-та такого-то РАН. 2000. Т. 1. № 2. С. 3--4.
%\bibitem{forth} {\it Четвертый Ч.Ч.} Публикация по теме в серийном издании или сборнике / Сб. научн. тр. к 20-летию Института. Новосибирск, Изд-во <<Пирогов>>, 2000. Т. 1. № 2. С. 3--4.
%\bibitem{fifth} {\it Fifth F.} Some journal publication in English // Appl. Math. Comput. J., Elsevier Publ. 1925. V. 501. No. 1. P. 1234--5678.
%\bibitem{sixth} {\it Sixth J.Th.} A book in English. Boston: Springer, 1991.

\end{thebibliography}

\AdditionalInformation{Стронгин Р.Г.}{Нижегородский государственный университет им. Н.И. Лобачевского, президент, Нижний Новгород}{strongin@unn.ru}

\AdditionalInformation{Гергель В.П.}{Нижегородский государственный университет им. Н.И. Лобачевского, директор института информационных технологий, математики и механики, Нижний Новгород}{gergel@unn.ru}

\AdditionalInformation{Баркалов К.А.}{Нижегородский государственный университет им. Н.И. Лобачевского, доцент кафедры математического обеспечения и суперкомпьютерных технологий института информационных технологий, математики и механики, Нижний Новгород}{konstantin.barkalov@itmm.unn.ru}

\end{document}
