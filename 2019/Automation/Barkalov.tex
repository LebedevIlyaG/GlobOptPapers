%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Sample document for preparing papers to  "Avtomatika i Telemekhanika"
%%  charset=windows-1251
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
%\usepackage{hyperref}
\usepackage{graphicx}


\begin{document}  %%%!!!

\year{2019}
\title{АДАПТИВНАЯ ГЛОБАЛЬНАЯ ОПТИМИЗАЦИЯ НА ОСНОВЕ БЛОЧНО-РЕКУРСИВНОЙ СХЕМЫ РЕДУКЦИИ РАЗМЕРНОСТИ}%
\thanks{Исследование выполнено за счет гранта Российского научного фонда (проект \mbox{№\,16-11-10150}).}

\authors{
Р.Г.~СТРОНГИН, д-р~физ.-мат.~наук\\
В.П.~ГГЕРГЕЛЬ, д-р~техн.~наук\\
К.А.~БАРКАЛОВ, канд.~физ.-мат.~наук\\
(Нижегородский государственный университет им. Н.И. Лобачевского)}

\maketitle

\begin{abstract}
В статье рассматриваются задачи многомерной многоэкстремальной оптимизации и численные методы их решения. Об оптимизируемой функции делается лишь общее предположение, что она удовлетворяет условию Липшица с априори неизвестной константой (задачи такого типа часто встречаются в приложениях). 
Рассмотрено два способа редукции размерности в задачах многомерной оптимизации: использованные кривых Пеано (разверток) и рекурсивная многошаговая схема.
Предложена обобщенная схема, комбинирующая эти два подхода. В новой схеме решение многомерной задачи сводится к решению семейства задач меньшей размерности, в которых, в свою очередь, используются развертки. Реализован адаптивный алгоритм, в котором все возникающие подзадачи решаются одновременно. 
Проведены численные эксперименты на нескольких сотнях тестовых задач, подтверждающие эффективность предложенной схемы редукции размерности.
\end{abstract}


\section{Введение}

В статье рассматриваются задачи глобальной оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y):y\in D\right\}},\\
& D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Предполагается, что целевая функция может быть многоэкстремальной, задана неявно (функция вида <<черный ящик>>), а вычисление ее значений связано с решением задачи численного моделирования и является трудоемкой операцией.

Любая возможность достоверно оценить глобальный оптимум в многоэкстремальной задаче с функциями вида <<черный ящик>> принципиально основана на априорной информации, позволяющей связать возможные значения целевой функции с ее известными значениями в точках проведенных испытаний. Для многих прикладных задачах типичной является ситуация, когда ограниченное изменение вектора параметров $y$ вызывает ограниченное изменение значений $\varphi(y)$. Математической моделью, описывающей указанное предположение, является условие Липшица
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty.
\]
%Отметим, что значение константы $L$ обычно является априори неизвестным, что делает ее оценку одной из ключевых проблем при построении методов липшицевой оптимизации.
Предположение липшицевости целевой функции типично для многих подходов к разработке оптимизационных алгоритмов. Первые методы липшицевой оптимизации были предложены в начале 70-х годов \cite{Evtushenko71,Pijavski72,Shubert72}; с тех пор данное направление продолжает активно развиваться \cite{Jones93,Pinter96,Zilinskas08,Evtushenko07,Evtushenko09}.
Например, многие известные методы основаны на различных способах разбиения области поиска на систему подобластей и последующего выбора наиболее перспективной подобласти для размещения очередного испытания (вычисления целевой функции). Результаты, полученные в данном направлении, представлены в работах \cite{Jones09,Zilinskas10,Evtushenko13,Kvasov13,Paulavicius16}.
 
В настоящее время для решения задач оптимизации с функциями вида <<черный ящик>> широко используются генетические и популяционные алгоритмы (см., например, \cite{Kureichik04,Karpenko08}), которые так или иначе основаны на идеях случайного поиска. В силу простоты реализации и использования они получили большую популярность, однако по качеству работы (численной оценкой которого может служить число корректно решенных задач из некоторого набора) они существенно уступают детерминированным алгоритмам \cite{Kvasov18,Sergeyev18}.

%В последние годы для решения задач с функциями вида ``черный ящик'' получили широкое распространение так называемые биоинспирированные алгоритмы, основанные на моделировании природных процессов и явлений. Типичными примерами методов данного класса являются генетические и популяционные алгоритмы (см., например, ). В основе указанных алгоритмов так или иначе лежит случайный поиск, т.к. они все содержат случайную компоненту (например, операции скрещивания и мутации в генетических или миграции в популяционных алгоритмах). В силу своей вероятностной природы методы данного класса обладают, вообще говоря, всюду плотной сходимостью, что невыгодно отличает их от детерминированных алгоритмов.

Одним из эффективных детерминированных методов решения задач многоэкстремальной оптимизации является \textit{информационно-статистический алгоритм глобального поиска}. Основы информационно-статистического подхода были заложены Ю.И. Неймарком \cite{Neymark66a,Neymark66b} и развиты Р.Г. Стронгиным \cite{Strongin70,Strongin78}. Впоследствии метод, изначально предложенный для решения безусловных задач, был успешно обобщен для решения задач с невыпуклыми ограничениями \cite{Strongin86,Strongin87} и многокритериальных задач \cite{Strongin93}. Для различных вариантов алгоритма были предложены способы распараллеливания, учитывающие особенности архитектуры современных вычислительных систем \cite{Strongin13}.

Разработанные методы основаны на редукции исходной многомерной задачи к эквивалентной одномерной или системе одномерных подзадач и последующим решением одномерных задач эффективными методами оптимизации функций одной переменной. Предложено две такие схемы: редукция на основе кривых, заполняющих пространство (кривых Пеано, или \textit{разверток}) \cite{Strongin91,Strongin00}, и схема рекурсивной вложенной оптимизации (\textit{многошаговая схема}) \cite{Grishagin97,Grishagin01}. В статье \cite{Grishagin16} предложена адаптивная многошаговая схема, существенно повышающая эффективность оптимизации по сравнению с базовым прототипом. В данной работе предлагается обобщение адаптивной схемы редукции размерности, комбинирующее использование вложенной оптимизации и кривых Пеано. При таком подходе вложенные подзадачи в адаптивной схеме могут быть как одномерными, так и многомерными; в последнем случае для редукции размерности вложенных подзадач используются развертки.


%Развитие - параллельность. предложены параллельные варианты многих алгоритмов. Отметим, что в основном распараллеливают генетические алгоритмы. В статье - последние наработки в направлении алгоритма глобального поиска. 



\section{Базовый алгоритм глобального поиска}
\label{SectionCore}

В качестве базовой задачи рассмотрим одномерную задачу многоэкстремальной оптимизации 
\begin{equation}\label{uni_problem}
\varphi^\ast = \varphi(x^\ast)=\min{\left\{\varphi(x):x\in \left[0,1\right] 
\right\}}
\end{equation}
с целевой функцией, удовлетворяющей условию Липшица.

Приведем описание алгоритма глобального поиска (АГП) для решения базовой задачи в соответствии с \cite{Strongin00}.
В процессе своей работы АГП порождает последовательность точек $x^i$, в которых вычисляются значения целевой функции $z^i=\varphi(x^i)$. 
Будем называть процесс вычисления значения целевой функции \textit{испытанием}.

В соответствии с алгоритмом, первые два испытания проводятся в граничных точках интервала $[0,1]$, т.е. $x^0=0,\;x^1=1$. 
В этих точка вычисляются значения целевой функции $z^0=\varphi(x^0),\;z^1=\varphi(x^1)$, и устанавливается значение счетчика $k=1$. 
Точка очередного испытания $x^{k+1}, k\geq 1,$ выбирается в соответствии со следующими правилами.

\textbf{Шаг 1.} Перенумеровать нижним индексом (начиная с 0) точки $x^i,\:0\leq i\leq k$, проведенных испытаний в порядке возрастания координаты, т.е.,
\[
0=x_0<x_1<\ldots <x_{k}=1.
\] 
Сопоставить точкам $x_i, 0\leq i\leq k$, вычисленные в них значения целевой функции $z_i=\varphi(x_i), 0\leq i\leq k$.

\textbf{Шаг 2.} Вычислить максимальное абсолютное значение относительной первой разности
\begin{equation}\label{mu}
\mu=\max_{1\leq i\leq k}\frac{\left|z_i-z_{i-1}\right|}{\Delta_i}
\end{equation}
где $\Delta_i = x_i-x_{i-1}$. Если вычисленное в соответствии с данной формулой значение равно 0, то положить $\mu = 1$.

\textbf{Шаг 3.} Для всех интервалов $(x_{i-1},x_i),1\leq i\leq k$,  вычислить значение
\begin{equation}\label{R}
R(i)=r\mu\Delta_i+\frac{(z_i-z_{i-1})^2}{r\mu\Delta_i}-2(z_i+z_{i-1}),
\end{equation} 
называемое \textit{характеристикой} интервала; величина $r>1$ является параметром алгоритма. 

\textbf{Шаг 4.} Найти интервал $(x_{t-1},x_t)$ с максимальной характеристикой
\begin{equation}\label{MaxR}
R(t)=\max_{1\leq i\leq {k}}R(i).
\end{equation}
Если максимальная характеристика соответствует нескольким интервалам, то в качестве $t$ выбрать минимальное число, удовлетворяющее (\ref{MaxR}).

\textbf{Шаг 5.} Провести новое испытание в точке
\begin{equation}\label{xk1}
x^{k+1}=\frac{1}{2}(x_{t-1}+x_t) - \frac{z_t-z_{t-1}}{2r\mu}.
\end{equation}

Алгоритм прекращает свою работу при выполнении условия $\Delta_t<\epsilon$; здесь $t$ из (\ref{MaxR}), а $\epsilon>0$ --- заданная точность. 
В качестве оценки решения задачи выбираются значения 
\[
z_k^\ast=\min_{0\leq i \leq k}\varphi(x^i), \ x_k^\ast=\arg \min_{0\leq i \leq
 k}\varphi(x^i).
\] 
Теоретические условия, определяющие сходимость алгоритма, представлены в \cite{Strongin00}.

\section{Редукция размерности}
\subsection{Редукция размерности с помощью кривых, заполняющих пространство}
\label{SectionPeano}

Рассмотрим схему редукции размерности, основанную на использовании кривых, заполняющих пространство (кривых Пеано).
Известно, что подобного типа кривые позволяют однозначно отобразить одномерный отрезок $[0,1]$ на многомерную область, т.е.
\begin{equation}
\left\{y(x):0\leq x \leq 1 \right\} = \left\{y\in R^N: -2^{-1}\leq y_i \leq 2^{-1}, 1 \leq i \leq N\right\}.
\end{equation}

Отметим, что теоретическая кривая $y(x)$ определяется как предельный объект. Поэтому при практической реализации может быть построено лишь некоторое приближение к истинной кривой. Методы построения подобных аппроксимаций (называемых \textit{развертками}) рассмотрены в \cite{Strongin78,Strongin00}. При этом точность приближения развертки к истинной кривой $y(x)$ зависит от \textit{плотности} развертки $m$ (являющейся параметром ее построения) и составляет величину порядка $2^{-m}$ по каждой координате.

Использование подобного рода отображений позволяет свести решение многомерной задачи (\ref{main_problem}) к решению эквивалентной ей одномерной
\begin{equation}
\varphi(y^\ast)=\varphi(y(x^\ast))=\min{\left\{\varphi(y(x)): x\in[0,1]
\right\}}.
\end{equation}

Важным свойством при этом является сохранение ограниченности относительных разностей функции 
(см. \cite{Strongin00}). Если функция $\varphi(y)$ в области $D$ удовлетворяла условию Липшица, тогда редуцированная функция  
$\varphi(y(x)), x\in [0,1]$ будет удовлетворять равномерному условию Гельдера 
\begin{equation}\label{Holder}
\left|\varphi(y(x_1))-\varphi(y(x_2))\right|\leq H\left|x_1-x_2\right|^{1/N},
\end{equation}
где константа Гельдера $H$ связана с константой Липшица  $L$ соотношением
\begin{equation}\label{HL}
H=2L\sqrt{N+3}.
\end{equation}

Условия (\ref{Holder}), (\ref{HL}) позволяет легко обобщить <<одномерный>> алгоритм из раздела \ref{SectionCore} для решения многомерных задач. 
Для этого достаточно заменить длины интервалов $\Delta_i$ в формулах (\ref{mu}),(\ref{R}) на длины 
\begin{equation}
\Delta_i=\left(x_i-x_{i-1}\right)^{1/N},
\end{equation}
а также заменить формулу (\ref{xk1}) для вычисления точки нового испытания на формулу
\begin{equation}
x^{k+1} = \frac{x_t+x_{t-1}}{2} - \mathrm{sign}(z_t-z_{t-1})\frac{1}{2r}
\left[\frac{\left|z_t-z_{t-1}\right|}{\mu}\right]^N.
\end{equation}


\subsection{Многошаговая схема редукции размерности}

Многошаговая схема редукции размерности (схема вложенной оптимизации) основана на известном соотношении \cite{Strongin78,Strongin13} 
\begin{equation}\label{nested}
\min_{y \in D}\varphi(y) = \min_{y_1\in\left[a_1,b_1\right]}\min_{y_2\in\left[a_2,b_2\right]}...\min_{y_N\in\left[a_N,b_N\right]}\varphi(y),
\end{equation}
которое позволяет свести решение исходной многомерной задачи (\ref{main_problem}) к решению семейства рекурсивно связанных одномерных подзадач.

Для формального описания многошаговой схемы введем семейство функций, определяемых в соответствии с соотношениями 
\begin{equation}\label{nested_N}
\varphi_N(y_1,...,y_N) \equiv \varphi(y_1,...,y_N),
\end{equation}
\begin{equation}\label{nested_i}
\varphi_i(y_1,...,y_i) = \min_{ y_{i+1} \in\left[a_{i+1},b_{i+1}\right]} \varphi_{i+1}(y_1,...,y_i,y_{i+1}), 1\leq i\leq N-1.
\end{equation}

Тогда, в соответствии с (\ref{nested}), для решения многомерной задачи (\ref{main_problem}) достаточно решить одномерную задачу  
\begin{equation}\label{nested_1}
\varphi^* = \min_{y_1\in\left[a_1,b_1\right]}\varphi_1(y_1).
\end{equation}
Однако каждое вычисление значения функции $\varphi_1$ в некоторой фиксированной точке $y_1$ предполагает решение одномерной задачи оптимизации второго уровня 
\begin{equation}
\varphi_1(y_1) = \min_{y_2\in\left[a_2,b_2\right]}\varphi_2(y_1,y_2).
\end{equation}
Вычисление значений функции $\varphi_2$, в свою очередь, требует одномерной минимизации функции $\varphi_3$, и т.д. вплоть до
решения задачи
\begin{equation}
\varphi_{N-1}(y_1,...,y_{N-1}) = \min_{ y_{N} \in\left[a_{N},b_{N}\right]} \varphi_{N}(y_1,...,y_{N}),
\end{equation}
на последнем уровне рекурсии.

%Схема вложенной оптмизации (\ref{nested_N})--(\ref{nested_1}) в предположении липшицевости одномерных функций $\overline{\varphi}_i(y_i)=\varphi_i(y_1,...,y_i)$ послужила основой для обобщения методов одномерной оптимизации [7,25,32,34,36 из JOGO] на случай многих переменных [3,12,25,26,36 из JOGO].

%Дадим более подробное описание взаимосвязей между одномерными подзадачами в многошаговой схеме. %Для удобства последующего изложения введем обозначение
%\[ v_i = (y_1,...y_i), \; 1 \leq i\ \leq N. \]
%Тогда одномерные функции $\overline{\varphi}_i(y_i), \; 1 < i\ \leq N$, порожденные в рамках многошаговой схемы, могут быть представлены в виде
%\[ \overline{\varphi_i}(y)_i = \varphi_i(v_{i-1},y_i), \; 1 < i\ \leq N, \]
%где вектор $v_{i-1}$ фиксирован. 

%В соответствии с многошаговой схемой на некотором шаге поиска $i, 1 \leq i \leq k_1$  алгоритм начинает минимизировать одномерную функцию $\varphi_1(y_1)$. Для этого, используя накопленную поисковую информацию, алгоритм выбирает точку очередной итерации $\widehat{y}_1$ и вычисляет значение функции в ней. Но для того, чтобы получить значение $\varphi_1(\widehat{y}_1)$ в этой точке, должна быть решена задача минимизации функции $\varphi_2(\widehat{y}_1,y_2)$. То есть, алгоритм приостанавливает минимизацию функции $\varphi_1(y_1)$ и начинает минимизировать $\varphi_2(\widehat{y}_1,y_2)$. В процессе минимизации алгоритм генерирует последовательность точек $\{y_2^j, 1 \leq j \leq k_2\}$, для каждой из которых необходимо решить задачу минимизации функции $\varphi_3(\widehat{y}_1,\widehat{y}_2,y_3)$, и т.д. Данный процесс выполняется рекурсивно до последнего $N$-го уровня. 

%А нужен ли рисунок?
Возникающие при этом подзадачи и взаимосвязи между ними отображены на рис. \ref{fig0}.
Наглядно видно, что структура взаимосвязей имеет форму дерева,
%Указанная структура может меняться в процессе решения многомерной задачи (\ref{main_problem}): вычисление значения функции $\varphi_i(\widehat{v}_{i-1},y_i), \; 1\leq i < N,$  на $i$-м уровне рекурсии требует решения всех подзадач в одном из поддеревьев $(i+1)$-го уровня. 
причем функции $\varphi_N(y_1,...,y_N)$ на $N$-м уровне являются листьями дерева задач, их значения вычисляются непосредственно.

\begin{figure}
\begin{center}%[scale=0.3][width=0.75\textwidth]
  \includegraphics[width=0.7\textwidth]{fig1.pdf} 
  \caption{Взаимосвязи между подзадачами в многошаговой схеме}\label{fig0} 
\end{center}
\end{figure}


Для описанной выше многошаговой схемы было предложено обобщение --- \textit{блочная многошаговая схема}, которое комбинирует использование разверток и вложенной оптимизации \cite{Barkalov2014}.

Рассмотрим $y$ как вектор, состоящий из векторов (блочных переменных)
\begin{equation}
y=(y_1,y_2,...,y_N)=(u_1,u_2,...,u_M),
\end{equation}
где $i$-я блочная переменная, т.е. вектор $u_i$, состоит из взятых последовательно компонент вектора $y$, т.е. $u_1=(y_1,y_2,...,y_{N_1})$, $u_2=(y_{N_1+1},y_{N_1+2}
,...,y_{N_1+N_2})$,..., $u_M=(y_{N-N_M+1},y_{N-N_M+2},...,y_{N})$, а $N_1+
N_2+...+N_M=N$.

Используя введенные обозначения, основное соотношение многошаговой схемы (\ref{nested}) 
может быть переписано в форме 
\begin{equation}\label{block_nested}
\min_{y \in D}\varphi(y) = \min_{u_1\in D_1}\min_{u_2\in D_2}...\min_{u_M \in D_M}\varphi(y),
\end{equation}
где подобласти $D_i, 1 \leq i \leq M$, являются проекциями исходной области поиска 
$D$ на подпространства, соответствующие переменным $u_i, 1 \leq i \leq M$.

Способ решения задачи (\ref{main_problem}) на основе соотношения (\ref{block_nested}) будет в целом повторять  (\ref{nested_N})--(\ref{nested_1}). Необходимо лишь заменить исходные переменные $y_i, 1\leq i \leq N$,  на блочные переменные $u_i, 1\leq i \leq M$. 

При этом вложенные подзадачи
\begin{equation}\label{block_nested_i}
\varphi^i(u_1,...,u_i) = \min_{u^{i+1}\in D_{i+1}} \varphi_{i+1}(u_1,...,u_i,u
_{i+1}), 1\leq i\leq M-1.
\end{equation}
будут многомерными, что является основным отличием от исходной многошаговой схемы. Для решения многомерных подзадач может быть использована схема редукции размерности на основе кривых Пеано. 

\subsection{Блочная адаптивная схема редукции размерности}
\Russian
Решение возникающего множества подзадач вида (\ref{nested_i}) (для многошаговой схемы) или вида (\ref{block_nested_i}) (для блочной многошаговой схемы) может быть организовано различными способами. 
Очевидный способ (детально проработанный в \cite{Grishagin97,Grishagin2001,Grishagin2015} для многошаговой схемы и в \cite{Barkalov2014,Barkalov2016} для блочной многошаговой схемы) основан на решении подзадач в соответствии с рекурсивным порядком их порождения. Однако здесь возникает потеря значительной части информации о целевой функции. 

Иным  подходом является адаптивная схема, в которой все подзадачи решаются одновременно, что позволяет более полно учитывать информацию о многомерной задаче и за счет этого ускорять процесс ее решения.
Для случая одномерных подзадач данный подход был теоретически обоснован и апробирован в \cite{Grishagin16,Grishagin2016_1,Grishagin18}. Настоящая работа предлагает обобщение адаптивной схемы для случая многомерных подзадач. Дадим краткое описание ее основных элементов.

Пусть вложенные подзадачи вида (\ref{block_nested_i}) решаются с помощью алгоритма глобального поиска, описанного в разделе \ref{SectionPeano}. Тогда каждой подзадаче (\ref{block_nested_i}) можно присвоить некоторое числовое значение, называемое характеристикой этой задачи. В качестве такой характеристики можно взять значение $R(t)$ из (\ref{MaxR}), т.е. максимальную характеристику интервалов, сформированных в данной задаче. В соответствии с правилом вычисления характеристик (\ref{R}), чем выше значение характеристики, тем более перспективной является подзадача для продолжения поиска в ней глобального минимума исходной задачи (\ref{main_problem}). Поэтому на каждой итерации выбирается подзадача с максимальной характеристикой для проведения в ней очередного испытания. Это испытание либо вычисляет значение целевой функции $\varphi(y)$ (если выбранная подзадача принадлежала уровню $j=M$), либо порождает новые подзадачи согласно (\ref{block_nested_i}) при $j\leq M-1$. В последнем случае новые порожденные задачи добавляются к текущему множеству задач, вычисляются их характеристики и процесс повторяется. Завершение процесса оптимизации происходит, когда в корневой задаче выполняется условие остановки алгоритма, решающего эту задачу.


\section{Результаты экспериментов}

Стандартный подход к сравнению алгоритмов глобальной оптимизации основан на решении указанными методами серии задач, выбранных случайно из некоторого класса.
Будем использовать два класса многоэкстремальных тестовых функций (GKLS \textit{Simple}, GKLS \textit{Hard}), описанные в \cite{Gaviano03}.

В работах \cite{Grishagin18,Sovrasov19} было экспериментально установлено, что алгоритм глобального поиска (АГП) как с использованием разверток, так и в сочетании с адаптивной схемой редукции размерности превосходит многие известные алгоритмы глобальной оптимизации, включая методы DIRECT и DIRECT\textit{l}. Поэтому в данном исследовании мы ограничимся сравнением вариантов АГП в сочетании с различными схемами редукции размерности.

Для сравнения эффективности работы алгоритмов будем использовать два критерия: среднее число испытаний и операционные характеристики.
Операционной характеристикой алгоритма называется функция $P(k)$, определяемая как доля задач из рассматриваемой серии, для решения которых потребовалось не более чем $k$ испытаний.
Задачу будем считать решенной, если алгоритм сгенерировал точку $y^k$ очередного испытания в окрестности решения $y^\ast$, т.е. $\left\|y^k-y^\ast\right\| < \delta \left\|b-a\right\|$, где $\delta = 0.01$, $a$ и $b$ --- границы области поиска $D$.

\begin{table}
\centering
\caption{Среднее число испытаний на классах GKLS \textit{Simple} и \textit{Hard}, $N=2$}\label{tab1}
\begin{tabular}{lccc}
\hline\noalign{\smallskip}
 & GKLS \textit{Simple} &  GKLS \textit{Hard} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 $K_e$ &  252 & 674 \\
 $K_n$ &  697 & 1252 \\
 $K_a$ &  279 & 815 \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{2DSimple.pdf} \\ (a)}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{2DHard.pdf} \\ (b)}
\end{minipage}
\caption{Операционные характеристики на классах GKLS \textit{Simple} (a) и \textit{Hard} (b), $N=2$}
\label{fig2}
\end{figure}

Первая серия экспериментов была проведена на двумерных задачах классов GKLS \textit{Simple} и GKLS \textit{Hard} (100 функций каждого класса). В табл.~\ref{tab1}  представлено среднее число испытаний, выполненных АГП с использованием разверток ($K_e$), многошаговой схемы ($K_n$) и адаптивной многошаговой схемы ($K_a$). На рис.~\ref{fig2}(a,b) приведены операционные характеристики алгоритмов, полученные на указанных классах задач. Непрерывная линия соответствует алгоритму с использованием разверток, короткий пунктир --- адаптивной многошаговой схеме, длинный пунктир --- многошаговой схеме. Результаты экспериментов показывают, что АГП с использованием адаптивной многошаговой схемы демонстрирует примерно одинаковое быстродействие  по сравнению с развертками, и оба они значительно превосходят алгоритм, использующий многошаговую схему. Поэтому в дальнейших экспериментах мы ограничимся сравнением различных вариантов адаптивной схемы редукции размерности.


Вторая серия экспериментов была проведена на четырехмерных задачах классов GKLS \textit{Simple} и GKLS \textit{Hard} (100 функций каждого класса). В табл.~\ref{tab2}  представлено среднее число испытаний, выполненных АГП с использованием адаптивной многошаговой схемы ($K_a$) и блочной адаптивной схемы ($K_{ba}$) с формированием двух уровней подзадач одинаковой размерности $N_1=N_2=2$. Отметим, что при использовании исходного варианта адаптивной многошаговой схемы при решении задачи размерности $N=4$ формируется четыре уровня одномерных подзадач, что усложняет их обработку.

На рис.~\ref{fig3}(a,b) приведены операционные характеристики алгоритмов, полученные на классах GKLS \textit{Simple} и GKLS \textit{Hard}. Пунктирная линия соответствует алгоритму с использованием адаптивной, а непрерывная ---  блочной адаптивной схемы. 
Результаты экспериментов показывают, что использование блочной адаптивной схемы дает ощутимый выигрыш по числу испытаний (до 35\%) по сравнению с исходной схемой редукции размерности.

\begin{table}
\centering
\caption{Среднее число испытаний на классах GKLS \textit{Simple} и \textit{Hard}, $N=4$}\label{tab2}
\begin{tabular}{lcc}
\hline\noalign{\smallskip}
 &    GKLS \textit{Simple} &  GKLS \textit{Hard} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 $K_a$  &  21 747 & 35 633 \\
 $K_{ba}$ &  15 942 & 33 206 \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{4DSimple.pdf} \\ (a)}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\center{\includegraphics[width=1.0\linewidth]{4DHard.pdf} \\ (b)}
\end{minipage}
\caption{Операционные характеристики на классах GKLS \textit{Simple} (a) и \textit{Hard} (b), $N=4$}
\label{fig3}
\end{figure}


Последняя серия экспериментов была проведена на шестимерных задачах класса GKLS \textit{Simple} (100 функций). Сравнивалась работа АГП с использованием разверток и блочной адаптивной схемы с формированием двух уровней подзадач одинаковой размерности $N_1=N_2=3$.
Среднее число испытаний, выполненных АГП с использованием разверток, составило 102 987, тогда как с использованием блочной адаптивной схемы  --- 75 390. 
На рис.~\ref{fig4} приведены операционные характеристики алгоритмов. Пунктирная линия соответствует алгоритму с использованием разверток, а непрерывная --- блочной адаптивной схемы. 

\begin{figure}
\center
\includegraphics[width=0.5\linewidth]{6DSimple.pdf}
\caption{Операционные характеристики на классе GKLS \textit{Simple}, $N=6$}
\label{fig4}
\end{figure}



\section{Заключение}

В данной работе предложена обобщенная адаптивная схема редукции размерности для задач глобальной оптимизации, комбинирующая использование кривых Пеано и схему вложенной (рекурсивной) оптимизации. Для решения редуцированных подзадач используется алгоритм глобального поиска. Приведена вычислительная схема алгоритма, рассмотрены основные вопросы, связанные с использованием адаптивной схемы редукции размерности. Проведены вычислительные эксперименты на серии тестовых задач с целью сравнения эффективности различных схем редукции размерности. 
Результаты экспериментов показывают, что использование блочной адаптивной схемы редукции размерности может значительно сократить число испытаний, необходимое для решения задачи с заданной точностью. 

\begin{thebibliography}{10}


\bibitem{Evtushenko71}
{\it	Евтушенко Ю.Г.} Численный метод поиска глобального экстремума функций (перебор на неравномерной сетке) // Ж. вычисл. матем. и матем. физ. 1971. T. 11, №6, С. 1390--1403.

\bibitem{Pijavski72}
{\it	Пиявский С.А.} Один алгоритм отыскания абсолютного экстремума функций // Ж. вычисл. матем. и матем. физ. 1972. Т. 12, № 4. С. 888--896.

\bibitem{Shubert72}
{\it Shubert B.O.} A sequential method seeking the global maximum of a function // SIAM J. Numer. Anal. 1972. V. 9. P. 379–388.

\bibitem{Jones93} 
{\it Jones D.R., Perttunen C.D., Stuckman B.E.} Lipschitzian optimization without the Lipschitz constant // J. Optim. Theory Appl. 1993. V. 79. No. 1. P. 157--181.

\bibitem{Pinter96}
{\it Pinter J. D.} Global Optimization in Action (Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications). Dordrecht: Kluwer Academic Publishers, 1996.

\bibitem{Zilinskas08}
{\it {\v Z}ilinskas J.} Branch and bound with simplicial partitions for global optimization // Math. Model. Anal. 2008. V. 13. No. 1. P. 145--159.

\bibitem{Evtushenko07}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И. А.} Распараллеливание процесса поиска глобального экстремума // Автомат. и телемех. 2007. № 5. C. 46--58

\bibitem{Evtushenko09}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И. А.} Параллельный поиск глобального экстремума функций многих переменных // Ж. вычисл. матем. и матем. физ. 2009. Т. 49, №2. C. 255--269.


\bibitem{Jones09}%Проверить формат ссылки!!!
{\it Jones D. R.} The DIRECT global optimization algorithm. In: Floudas C. A., Pardalos P. M. (eds.) The Encyclopedia of Optimization, Second Edition.  Springer. 2009. P. 725--735.

\bibitem{Zilinskas10}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J., Grothey A.}: Investigation of selection strategies in branch and bound algorithm with simplicial partitions and combination of Lipschitz bounds // Optim. Lett. 2010. V. 4(1). P. 173–--83

\bibitem{Evtushenko13}
{\it Evtushenko Y. G., Posypkin M. A.} A deterministic approach to global box-constrained optimization // Optim. Lett. 2013. V. 7(4). P. 819--829

\bibitem{Kvasov13}
{\it Квасов Д.Е., Сергеев Я.Д.} Методы липшицевой глобальной оптимизации в задачах управления // Автомат. и телемех. 2013. № 9. C. 3--19.

\bibitem{Paulavicius16}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J.} Advantages of simplicial partitioning for Lipschitz optimization problems with linear constraints // Optim. Lett. 2016. V. 10(2). P. 237--246.

\bibitem{Kureichik04}
{\it Гладков Л.А., Курейчик В.В., Курейчик В.М.} Генетические алгоритмы. Учебное пособие. М.: ФИЗМАТЛИТ, 2004.

\bibitem{Karpenko08}
{\it Карпенко А.П.} Современные алгоритмы поисковой оптимизации. Алгоритмы, вдохновленные природой: учебное пособие. М.: Издательство МГТУ им. Н. Э. Баумана, 2014.

\bibitem{Kvasov18}
{\it Kvasov D.E., Mukhametzhanov M.S.} Metaheuristic vs. deterministic global optimization algorithms: The univariate case // Appl. Math. Comput. 2018. V. 318. P. 245--259.

\bibitem{Sergeyev18}
{\it Sergeyev Y., Kvasov D., Mukhametzhanov M.} On the efficiency of nature-inspired metaheuristics in expensive global optimization with limited budget // Sci.
Rep. V. 8(1). Art. No. 435.

\bibitem{Neymark66a}
{\it Неймарк Ю.И., Стронгин Р.Г.} Поиск экстремума функций по принципу максимума информации // Автомат. и телемех. 1966. № 11. С. 113--118.

\bibitem{Neymark66b}
{\it Неймарк Ю.И., Стронгин Р.Г.} Информационный подход к задаче поиска экстремума функций // Изв. АН СССР, Техническая кибернетика. 1966. № 1. С. 17--26.

\bibitem{Strongin70}
{\it Стронгин Р.Г.} Многоэкстремальная минимизация // Автомат. и телемех. 1970. № 7. С. 63--67.

\bibitem{Strongin78}
{\it Стронгин Р.Г.} Численные методы в многоэкстремальных задачах (информационно-статистические алгоритмы). М.: Наука, 1978.

\bibitem{Strongin86}
{\it Стронгин Р.Г., Маркин Д.Л.} Минимизация многоэкстремальных функций при невыпуклых ограничениях // Кибернетика. 1986. №4. С.63--69.

\bibitem{Strongin87}
{\it Маркин Д.Л., Стронгин Р.Г.} Метод решения многоэкстремальных задач с невыпуклыми ограничениями, использующий априорную информацию об оценках оптимума // Ж. вычисл. матем. и матем. физ. 1987. Т.27, № 1. С.52--61.

\bibitem{Strongin93}
{\it Маркин Д.Л., Стронгин Р.Г.} О равномерной оценке множества слабоэффективных точек в многоэкстремальных многокритериальных задачах оптимизации // Ж. вычисл. матем. и матем. физ. 1993 Т. 33, № 2. С. 195--205.

\bibitem{Strongin13}
{\it Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А.} Параллельные вычисления в задачах глобальной оптимизации. М.: Издательство Московского университета, 2013.

\bibitem{Strongin91}
{\it Стронгин Р.Г.} Параллельная многоэкстремальная оптимизация с использованием множества разверток // Ж. вычисл. матем. и матем. физ. 1991. T. 31, № 8. С. 1173--1185.

\bibitem{Strongin00}
{\it Strongin R.G., Sergeev Ya.D.} Global Optimization with Non-Convex Constraints. Sequential and Parallel Algorithms. Dordrecht: Kluwer Academic Publishers, 2000.

\bibitem{Grishagin97}
{\it Grishagin V.A., Sergeyev Y.D., Strongin R.G.} Parallel characteristical algorithms for solving problems of global optimization // J. Glob. Optim. 1997. V. 10(2). P. 185--206.

\bibitem{Grishagin01}
{\it Sergeyev Y., Grishagin V.} Parallel asynchronous global search and the nested optimization scheme // J. Comput. Anal. Appl. 2001. V. 3(2). P. 123--145.

\bibitem{Grishagin16}
{\it Gergel V., Grishagin V., Gergel A.} Adaptive nested optimization scheme for multidimensional global search // J. Glob. Optim. 2016. V. 66(1). P. 35--51.

\bibitem{Barkalov2014}
{\it Barkalov K., Gergel V.} Multilevel scheme of dimensionality reduction for parallel
global search algorithms // In: OPT-i 2014 -- 1st International Conference on Engineering and Applied Sciences Optimization, Proceedings. 2014. P. 2111--2124.

\bibitem{Grishagin2001}
{\it Sergeyev Y., Grishagin V.} Parallel asynchronous global search and the nested optimization scheme // J. Comput. Anal. Appl. 2001. V. 3(2). P. 123--145.

\bibitem{Grishagin2015}
{\it Gergel V., Grishagin V., Israfilov R.} Local tuning in nested scheme of global optimization // Procedia Computer Science. 2015. V. 51(1). P. 865--874

\bibitem{Barkalov2016}
{\it Barkalov K., Lebedev I.} Solving multidimensional global optimization problems using graphics accelerators // CCIS. 2016. V. 687. P. 224--235.

\bibitem{Grishagin2016_1}
{\it Grishagin V., Israfilov R., Sergeyev Y.}: Comparative efficiency of dimensionality reduction schemes in global optimization // AIP Conference Proceedings. 2016. V. 1776, art. no. 060011.

\bibitem{Grishagin18}
{\it Grishagin V., Israfiov R., Sergeyev Y.} Convergence conditions and numerical comparison of global optimization methods based on dimensionality reduction schemes // Appl. Math. Comput. 2018. V. 318. P. 270--280.

\bibitem{Gaviano03}
{\it Gaviano M., Lera D., Kvasov D.E., Sergeyev Ya.D.} Software for generation of classes of test functions with known local and global minima for global optimization // ACM Trans. Math. Software. 2003. V. 29. P. 469--480.

\bibitem{Sovrasov19}
{\it Sovrasov V.} Comparison of several stochastic and deterministic derivative-free global optimization algorithms // LNCS. 2019. V. 11548. P. 70--81. 


%\bibitem{Sergeyev00} Kvasov, D.E., Pizzuti, C., Sergeyev, Y.D.: Local tuning and partition strategies for diagonal GO methods. Numer. Math. 94(1), 93--106 (2003)
%\bibitem{Sergeyev06} Sergeyev, Ya.D., Kvasov, D.E.: Global search based on efficient diagonal partitions and a set of Lipschitz constants. SIAM J. Optim. 16(3), 910--937 (2006)

%\bibitem{third} {\it Третий Т.Т.} Публикация в трудах конференции // Тр. Ин-та такого-то РАН. 2000. Т. 1. № 2. С. 3--4.
%\bibitem{forth} {\it Четвертый Ч.Ч.} Публикация по теме в серийном издании или сборнике / Сб. научн. тр. к 20-летию Института. Новосибирск, Изд-во <<Пирогов>>, 2000. Т. 1. № 2. С. 3--4.
%\bibitem{fifth} {\it Fifth F.} Some journal publication in English // Appl. Math. Comput. J., Elsevier Publ. 1925. V. 501. No. 1. P. 1234--5678.
%\bibitem{sixth} {\it Sixth J.Th.} A book in English. Boston: Springer, 1991.

\end{thebibliography}

\AdditionalInformation{Стронгин Р.Г.}{Нижегородский государственный университет им. Н.И. Лобачевского, президент, Нижний Новгород}{strongin@unn.ru}

\AdditionalInformation{Гергель В.П.}{Нижегородский государственный университет им. Н.И. Лобачевского, директор института информационных технологий, математики и механики, Нижний Новгород}{gergel@unn.ru}

\AdditionalInformation{Баркалов К.А.}{Нижегородский государственный университет им. Н.И. Лобачевского, доцент кафедры математического обеспечения и суперкомпьютерных технологий института информационных технологий, математики и механики, Нижний Новгород}{konstantin.barkalov@itmm.unn.ru}

\end{document}
