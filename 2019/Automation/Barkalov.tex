%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Sample document for preparing papers to  "Avtomatika i Telemekhanika"
%%  charset=windows-1251
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
%\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{cite}
\usepackage[labelsep=period]{caption}
%						 format=hang,
%            font=onehalfspacing,
%            margin=\the\parindent,
%            figurename=Рисунок,
            

\usepackage{setspace}
\onehalfspacing % полуторный интервал для всего текста

\begin{document}  %%%!!!

\year{2020}
\title{АДАПТИВНАЯ ГЛОБАЛЬНАЯ ОПТИМИЗАЦИЯ НА ОСНОВЕ БЛОЧНО-РЕКУРСИВНОЙ СХЕМЫ РЕДУКЦИИ РАЗМЕРНОСТИ }%
\thanks{Исследование выполнено за счет гранта Российского научного фонда (проект \mbox{№\,16-11-10150}).}

\authors{
Р.Г.~СТРОНГИН, д-р~физ.-мат.~наук (strongin@unn.ru),\\
В.П.~ГЕРГЕЛЬ, д-р~техн.~наук (gergel@unn.ru),\\
К.А.~БАРКАЛОВ, канд.~физ.-мат.~наук (barkalov@vmk.unn.ru)\\
(Нижегородский государственный университет им. Н.И. Лобачевского)}

\KeyWords{глобальная оптимизация, многоэкстремальные функции, редукция размерности, кривые Пеано, рекурсивная оптимизация. }



\maketitle

\begin{abstract}
Рассматриваются задачи многомерной многоэкстремальной оптимизации и численные методы их решения. Об оптимизируемой функции делается лишь общее предположение, что она удовлетворяет условию Липшица с априори неизвестной константой (задачи такого типа часто встречаются в приложениях). 
Рассмотрено два способа редукции размерности в задачах многомерной оптимизации: использование кривых Пеано (разверток) и рекурсивная многошаговая схема.
Предложена обобщенная схема, комбинирующая эти два подхода. В новой схеме решение многомерной задачи сводится к решению семейства задач меньшей размерности, в которых в свою очередь используются развертки. Реализован адаптивный алгоритм, в котором все возникающие подзадачи решаются одновременно. 
Проведены численные эксперименты на нескольких сотнях тестовых задач, подтверждающие эффективность предложенной схемы редукции размерности.
\end{abstract}


\section{Введение}

В статье рассматриваются задачи глобальной оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y):y\in D\right\}},\\
& D=\left\{y\in \text{R}^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Предполагается, что целевая функция может быть многоэкстремальной, задана неявно (функция вида ``черный ящик''), а вычисление ее значений связано с решением задачи численного моделирования и является трудоемкой операцией.

Любая возможность достоверно оценить глобальный оптимум в многоэкстремальной задаче с функциями вида ``черный ящик'' принципиально основана на априорной информации, позволяющей связать возможные значения целевой функции с ее известными значениями в точках проведенных испытаний. Для многих прикладных задач типичной является ситуация, когда ограниченное изменение вектора параметров $y$ вызывает ограниченное изменение значений $\varphi(y)$. Математической моделью, описывающей указанное предположение, является условие Липшица
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty.
\]
%Отметим, что значение константы $L$ обычно является априори неизвестным, что делает ее оценку одной из ключевых проблем при построении методов липшицевой оптимизации.$\overline{\underline{\text{XX}}}$
Предположение липшицевости целевой функции типично для многих подходов к разработке оптимизационных алгоритмов. Первые методы липшицевой оптимизации были предложены в начале 70-х гг. XX в. \cite{Evtushenko71,Pijavski72,Shubert72}; с тех пор данное направление продолжает активно развиваться \cite{Jones93,Pinter96,Zilinskas08,Evtushenko07,Evtushenko09}.
Например, многие известные методы основаны на различных способах разбиения области поиска на систему подобластей и последующего выбора наиболее перспективной подобласти для размещения очередного испытания (вычисления целевой функции). Результаты, полученные в данном направлении, представлены в публикациях \cite{Jones09,Zilinskas10,Evtushenko13,Kvasov13,Paulavicius16}.
 
В настоящее время для решения задач оптимизации с функциями вида ``черный ящик'' широко используются генетические и популяционные алгоритмы (см., например, \cite{Kureichik04,Karpenko08}), которые так или иначе основаны на идеях случайного поиска. В силу простоты реализации и использования они получили большую популярность, однако по качеству работы (численной оценкой которого может служить число корректно решенных задач из некоторого набора) они существенно уступают детерминированным алгоритмам \cite{Kvasov18,Sergeyev18}.

%В последние годы для решения задач с функциями вида ``черный ящик'' получили широкое распространение так называемые биоинспирированные алгоритмы, основанные на моделировании природных процессов и явлений. Типичными примерами методов данного класса являются генетические и популяционные алгоритмы (см., например, ). В основе указанных алгоритмов так или иначе лежит случайный поиск, т.к. они все содержат случайную компоненту (например, операции скрещивания и мутации в генетических или миграции в популяционных алгоритмах). В силу своей вероятностной природы методы данного класса обладают, вообще говоря, всюду плотной сходимостью, что невыгодно отличает их от детерминированных алгоритмов.

Одним из эффективных детерминированных методов решения задач многоэкстремальной оптимизации является \textit{информационно-статистический алгоритм глобального поиска}. Основы информационно-статистического подхода были заложены Ю.И. Неймарком \cite{Neymark66a,Neymark66b} и развиты Р.Г. Стронгиным \cite{Strongin70,Strongin78}. Впоследствии метод, изначально предложенный для решения безусловных задач, был успешно обобщен для решения задач с невыпуклыми ограничениями \cite{Strongin86,Strongin87} и многокритериальных задач \cite{Strongin93}. Для различных вариантов алгоритма были предложены способы распараллеливания, учитывающие особенности архитектуры современных вычислительных систем \cite{Strongin13}.

Разработанные методы основаны на редукции исходной многомерной задачи к эквивалентной одномерной или к системе одномерных подзадач с последующим решением одномерных задач эффективными методами оптимизации функций одной переменной. Предложено две такие схемы: редукция на основе кривых, заполняющих пространство (кривых Пеано, или \textit{разверток}) \cite{Strongin91,Strongin00}, и схема рекурсивной вложенной оптимизации (\textit{многошаговая схема}) \cite{Grishagin97,Grishagin01}. В \cite{Grishagin16} предложена адаптивная многошаговая схема, существенно повышающая эффективность оптимизации по сравнению с базовым прототипом. В данной статье предлагается обобщение адаптивной схемы редукции размерности, комбинирующее использование вложенной оптимизации и кривых Пеано. При таком подходе вложенные подзадачи в адаптивной схеме могут быть как одномерными, так и многомерными; в последнем случае для редукции размерности вложенных подзадач используются развертки.


%Развитие - параллельность. предложены параллельные варианты многих алгоритмов. Отметим, что в основном распараллеливают генетические алгоритмы. В статье - последние наработки в направлении алгоритма глобального поиска. 



\section{Базовый алгоритм глобального поиска}
\label{SectionCore}

В качестве базовой задачи рассмотрим одномерную задачу многоэкстремальной оптимизации 
\begin{equation}\label{uni_problem}
\varphi^\ast = \varphi(x^\ast)=\min{\left\{\varphi(x):x\in \left[0,1\right] 
\right\}}
\end{equation}
с целевой функцией, удовлетворяющей условию Липшица.

Приведем описание алгоритма глобального поиска (АГП) для решения базовой задачи в соответствии с \cite{Strongin00}.
В процессе своей работы АГП порождает последовательность точек $x^i$, в которых вычисляются значения целевой функции $z^i=\varphi(x^i)$. 
Будем называть процесс вычисления значения целевой функции \textit{испытанием}.

В соответствии с алгоритмом первые два испытания проводятся в граничных точках отрезка $[0,1]$, т.е. $x^0=0,\;x^1=1$. 
В этих точках вычисляются значения целевой функции $z^0=\varphi(x^0),\;z^1=\varphi(x^1)$ и устанавливается значение счетчика $k=1$. 
Точка очередного испытания $x^{k+1}, k\geq 1,$ выбирается в соответствии со следующими правилами.

\so{\sl Шаг 1}. Перенумеровать нижним индексом (начиная с 0) точки $x^i,\:0\leq i\leq k$, проведенных испытаний в порядке возрастания координаты, т.е.
\[
0=x_0<x_1<\ldots <x_{k}=1.
\] 
Сопоставить точкам $x_i, \; 0\leq i\leq k$, вычисленные в них значения целевой функции $z_i=\varphi(x_i), \; 0\leq i\leq k$.

\so{\sl Шаг 2}. Вычислить максимальное абсолютное значение относительной первой разности
\begin{equation}\label{mu}
\mu=\max_{1\leq i\leq k}\frac{\left|z_i-z_{i-1}\right|}{\Delta_i},
\end{equation}
где $\Delta_i = x_i-x_{i-1}$. Если вычисленное в соответствии с данной формулой значение равно нулю, то положить $\mu = 1$.

\so{\sl Шаг 3}. Для всех интервалов $(x_{i-1},x_i),1\leq i\leq k$,  вычислить значение
\begin{equation}\label{R}
R(i)=r\mu\Delta_i+\frac{(z_i-z_{i-1})^2}{r\mu\Delta_i}-2(z_i+z_{i-1}),
\end{equation} 
называемое \textit{характеристикой} интервала; величина $r>1$ является параметром алгоритма. 

\so{\sl Шаг 4}. Найти интервал $(x_{t-1},x_t)$ с максимальной характеристикой
\begin{equation}\label{MaxR}
R(t)=\max_{1\leq i\leq {k}}R(i).
\end{equation}
Если максимальная характеристика соответствует нескольким интервалам, то в качестве $t$ выбрать минимальное число, удовлетворяющее (\ref{MaxR}).

\so{\sl Шаг 5}. Провести новое испытание в точке
\begin{equation}\label{xk1}
x^{k+1}=\frac{1}{2}(x_{t-1}+x_t) - \frac{z_t-z_{t-1}}{2r\mu}.
\end{equation}

Алгоритм прекращает свою работу при выполнении условия $\Delta_t<\epsilon$; здесь $t$ из (\ref{MaxR}), а $\epsilon>0$ --- заданная точность. 
В качестве оценки решения задачи выбираются значения 
\[
z_k^\ast=\min_{0\leq i \leq k}\varphi(x^i), \ x_k^\ast=\arg \min_{0\leq i \leq
 k}\varphi(x^i).
\] 
Теоретические условия, определяющие сходимость алгоритма, представлены в \cite{Strongin00}.

\section{Редукция размерности}
\subsection{Редукция размерности с помощью кривых, заполняющих пространство}
\label{SectionPeano}

Рассмотрим схему редукции размерности, основанную на использовании кривых, заполняющих пространство (кривых Пеано).
Известно, что подобного типа кривые позволяют однозначно отобразить одномерный отрезок $[0,1]$ на многомерную область, т.е.
\begin{equation}
\left\{y(x):0\leq x \leq 1 \right\} = \left\{y\in \text{R}^N: -2^{-1}\leq y_i \leq 2^{-1}, 1 \leq i \leq N\right\}.
\end{equation}

Отметим, что теоретическая кривая $y(x)$ определяется как предельный объект. Поэтому при практической реализации может быть построено лишь некоторое приближение к истинной кривой. Методы построения подобных аппроксимаций (называемых \textit{развертками}) рассмотрены в \cite{Strongin78,Strongin00}. При этом точность приближения развертки к истинной кривой $y(x)$ зависит от \textit{плотности} развертки $m$ (являющейся параметром ее построения) и составляет величину порядка $2^{-m}$ по каждой координате.

Использование подобного рода отображений позволяет свести решение многомерной задачи (\ref{main_problem}) к решению эквивалентной ей одномерной
\begin{equation}
\varphi(y^\ast)=\varphi(y(x^\ast))=\min{\left\{\varphi(y(x)): x\in[0,1]
\right\}}.
\end{equation}

Важным свойством при этом является сохранение ограниченности относительных разностей функции 
(см. \cite{Strongin00}). Если функция $\varphi(y)$ в области $D$ удовлетворяла условию Липшица, тогда редуцированная функция  
$\varphi(y(x)), x\in [0,1]$, будет удовлетворять равномерному условию Гельдера 
\begin{equation}\label{Holder}
\left|\varphi(y(x_1))-\varphi(y(x_2))\right|\leq H\left|x_1-x_2\right|^{1/N},
\end{equation}
где константа Гельдера $H$ связана с константой Липшица  $L$ соотношением
\begin{equation}\label{HL}
H=2L\sqrt{N+3}.
\end{equation}

Условия (\ref{Holder}), (\ref{HL}) позволяют легко обобщить ``одномерный'' алгоритм из раздела \ref{SectionCore} для решения многомерных задач. 
Для этого достаточно заменить длины интервалов $\Delta_i$ в формулах (\ref{mu}), (\ref{R}) на длины 
\begin{equation}
\Delta_i=\left(x_i-x_{i-1}\right)^{1/N},
\end{equation}
а также заменить формулу (\ref{xk1}) для вычисления точки нового испытания на формулу
\begin{equation}
x^{k+1} = \frac{x_t+x_{t-1}}{2} - \mathrm{sign}(z_t-z_{t-1})\frac{1}{2r}
\left[\frac{\left|z_t-z_{t-1}\right|}{\mu}\right]^N.
\end{equation}


\subsection{Многошаговая схема редукции размерности}

Многошаговая схема редукции размерности (схема вложенной оптимизации) основана на известном соотношении \cite{Strongin78,Strongin13} 
\begin{equation}\label{nested}
\min_{y \in D}\varphi(y) = \min_{y_1\in\left[a_1,b_1\right]}\min_{y_2\in\left[a_2,b_2\right]}...\min_{y_N\in\left[a_N,b_N\right]}\varphi(y),
\end{equation}
которое позволяет свести решение исходной многомерной задачи (\ref{main_problem}) к решению семейства рекурсивно связанных одномерных подзадач.

Для формального описания многошаговой схемы введем семейство функций, определяемых в соответствии с соотношениями 
\begin{equation}\label{nested_N}
\varphi_N(y_1,...,y_N) \equiv \varphi(y_1,...,y_N),
\end{equation}
\begin{equation}\label{nested_i}
\varphi_i(y_1,...,y_i) = \min_{ y_{i+1} \in\left[a_{i+1},b_{i+1}\right]} \varphi_{i+1}(y_1,...,y_i,y_{i+1}), 1\leq i\leq N-1.
\end{equation}

Тогда, в соответствии с (\ref{nested}), для решения многомерной задачи (\ref{main_problem}) достаточно решить одномерную задачу  
\begin{equation}\label{nested_1}
\varphi^* = \min_{y_1\in\left[a_1,b_1\right]}\varphi_1(y_1).
\end{equation}
Однако каждое вычисление значения функции $\varphi_1$ в некоторой фиксированной точке $y_1$ предполагает решение одномерной задачи оптимизации второго уровня 
\begin{equation}
\varphi_1(y_1) = \min_{y_2\in\left[a_2,b_2\right]}\varphi_2(y_1,y_2).
\end{equation}
Вычисление значений функции $\varphi_2$ в свою очередь требует одномерной минимизации функции $\varphi_3$ и т.д. вплоть до
решения задачи
\begin{equation}
\varphi_{N-1}(y_1,...,y_{N-1}) = \min_{ y_{N} \in\left[a_{N},b_{N}\right]} \varphi_{N}(y_1,...,y_{N})
\end{equation}
на последнем уровне рекурсии.

%Схема вложенной оптмизации (\ref{nested_N})--(\ref{nested_1}) в предположении липшицевости одномерных функций $\overline{\varphi}_i(y_i)=\varphi_i(y_1,...,y_i)$ послужила основой для обобщения методов одномерной оптимизации [7,25,32,34,36 из JOGO] на случай многих переменных [3,12,25,26,36 из JOGO].

%Дадим более подробное описание взаимосвязей между одномерными подзадачами в многошаговой схеме. %Для удобства последующего изложения введем обозначение
%\[ v_i = (y_1,...y_i), \; 1 \leq i\ \leq N. \]
%Тогда одномерные функции $\overline{\varphi}_i(y_i), \; 1 < i\ \leq N$, порожденные в рамках многошаговой схемы, могут быть представлены в виде
%\[ \overline{\varphi_i}(y)_i = \varphi_i(v_{i-1},y_i), \; 1 < i\ \leq N, \]
%где вектор $v_{i-1}$ фиксирован. 

%В соответствии с многошаговой схемой на некотором шаге поиска $i, 1 \leq i \leq k_1$  алгоритм начинает минимизировать одномерную функцию $\varphi_1(y_1)$. Для этого, используя накопленную поисковую информацию, алгоритм выбирает точку очередной итерации $\widehat{y}_1$ и вычисляет значение функции в ней. Но для того, чтобы получить значение $\varphi_1(\widehat{y}_1)$ в этой точке, должна быть решена задача минимизации функции $\varphi_2(\widehat{y}_1,y_2)$. То есть, алгоритм приостанавливает минимизацию функции $\varphi_1(y_1)$ и начинает минимизировать $\varphi_2(\widehat{y}_1,y_2)$. В процессе минимизации алгоритм генерирует последовательность точек $\{y_2^j, 1 \leq j \leq k_2\}$, для каждой из которых необходимо решить задачу минимизации функции $\varphi_3(\widehat{y}_1,\widehat{y}_2,y_3)$, и т.д. Данный процесс выполняется рекурсивно до последнего $N$-го уровня. 

%А нужен ли рисунок?
Возникающие при этом подзадачи и взаимосвязи между ними отображены на рис. \ref{fig0}. \marginpar{\framebox{Рис. 1}}
Наглядно видно, что структура взаимосвязей имеет форму дерева,
%Указанная структура может меняться в процессе решения многомерной задачи (\ref{main_problem}): вычисление значения функции $\varphi_i(\widehat{v}_{i-1},y_i), \; 1\leq i < N,$  на $i$-м уровне рекурсии требует решения всех подзадач в одном из поддеревьев $(i+1)$-го уровня. 
причем функции $\varphi_N(y_1,...,y_N)$ на $N$-м уровне являются листьями дерева задач, их значения вычисляются непосредственно.

Для описанной выше многошаговой схемы было предложено обобщение --- \textit{блочная многошаговая схема}, которое комбинирует использование разверток и вложенной оптимизации \cite{Barkalov2014}.

Рассмотрим $y$ как вектор, состоящий из векторов (блочных переменных)
\begin{equation}
y=(y_1,y_2,...,y_N)=(u_1,u_2,...,u_M),
\end{equation}
где $i$-я блочная переменная, т.е. вектор $u_i$, состоит из взятых последовательно компонент вектора $y$, т.е. $u_1=(y_1,y_2,...,y_{N_1})$, $u_2=(y_{N_1+1},y_{N_1+2}
,...,y_{N_1+N_2})$, ..., $u_M=(y_{N-N_M+1}, y_{N-N_M+2},..., y_{N})$, а $N_1+N_2+...+N_M=N$.

Используя введенные обозначения, основное соотношение многошаговой схемы (\ref{nested}) 
может быть переписано в форме 
\begin{equation}\label{block_nested}
\min_{y \in D}\varphi(y) = \min_{u_1\in D_1}\min_{u_2\in D_2}...\min_{u_M \in D_M}\varphi(y),
\end{equation}
где подобласти $D_i, 1 \leq i \leq M$, являются проекциями исходной области поиска 
$D$ на подпространства, соответствующие переменным $u_i, 1 \leq i \leq M$.

Способ решения задачи (\ref{main_problem}) на основе соотношения (\ref{block_nested}) будет в целом повторять  (\ref{nested_N})--(\ref{nested_1}). Необходимо лишь заменить исходные переменные $y_i, 1\leq i \leq N$,  на блочные переменные $u_i, 1\leq i \leq M$. 

При этом вложенные подзадачи
\begin{equation}\label{block_nested_i}
\varphi_i(u_1,...,u_i) = \min_{u_{i+1}\in D_{i+1}} \varphi_{i+1}(u_1,...,u_i,u_{i+1}), \; 1\leq i\leq M-1,
\end{equation}
будут многомерными, что является основным отличием от исходной многошаговой схемы. Для решения многомерных подзадач может быть использована схема редукции размерности на основе кривых Пеано. 

\subsection{Блочная адаптивная схема редукции размерности}
\Russian
Решение возникающего множества подзадач вида (\ref{nested_i}) (для многошаговой схемы) или вида (\ref{block_nested_i}) (для блочной многошаговой схемы) может быть организовано различными способами. 
Очевидный способ (детально проработанный в \cite{Grishagin97,Grishagin2001,Grishagin2015} для многошаговой схемы и в \cite{Barkalov2014,Barkalov2016} для блочной многошаговой схемы) основан на решении подзадач в соответствии с рекурсивным порядком их порождения. Однако здесь возникает потеря значительной части информации о целевой функции. 

Иным  подходом является адаптивная схема, в которой все подзадачи решаются одновременно, что позволяет более полно учитывать информацию о многомерной задаче и за счет этого ускорять процесс ее решения.
Для случая одномерных подзадач данный подход был теоретически обоснован и апробирован в \cite{Grishagin16,Grishagin2016_1,Grishagin18}. Настоящая статья предлагает обобщение адаптивной схемы для случая многомерных подзадач.

Перед изложением новой схемы организации вычислений еще раз отметим, что в рамках исходной многошаговой схемы (или же ее блочного варианта) порождаемые подзадачи решаются строго последовательно; получаемая в результате иерархическая схема порождения и решения подзадач (в виде дерева) представлена на рис.~\ref{fig0}. Построение этого дерева происходит динамически в процессе решения исходной задачи (\ref{main_problem}). При этом вычисление одного значения функции $\varphi_i(y_1,y_2,...,y_i)$ на $i$-м уровне требует полного решения всех задач одного из поддеревьев уровня $i+1$.

Адаптивная многошаговая схема редукции размерности изменяет порядок решения подзадач: они будут решаться не по одной (в соответствии с их иерархией в дереве задач), а одновременно, т.е. будет существовать некоторое множество подзадач, находящихся в процессе решения. В рамках новой схемы:
\begin{itemize}
	\item 
для вычисления значения функции $i$-го уровня из (\ref{block_nested_i}) порождается новая задача уровня $i+1$, в которой проводится только одно испытание, после чего новая порожденная задача включается в множество уже имеющихся задач, подлежащих решению;
	\item 
	итерация глобального поиска состоит в выборе одной (наиболее перспективной) задачи из множества имеющихся задач, в которой проводится одно испытание; точка проведения нового испытания определяется в соответствии с алгоритмом из подраздела \ref{SectionPeano};
	\item
в качестве минимальных значений функций из (\ref{block_nested_i}) используются их текущие оценки, полученные на основе накопленной поисковой информации.
\end{itemize}

Краткое описание основных шагов блочной адаптивной схемы редукции размерности состоит в следующем.
Пусть вложенные подзадачи вида (\ref{block_nested_i}) решаются с помощью алгоритма глобального поиска, описанного в подразделе \ref{SectionPeano}. Тогда каждой подзадаче (\ref{block_nested_i}) можно присвоить некоторое числовое значение, называемое характеристикой этой задачи. В качестве такой характеристики можно взять значение $R(t)$ из (\ref{MaxR}), т.е. максимальную характеристику интервалов, сформированных в данной задаче. В соответствии с правилом вычисления характеристик (\ref{R}) чем выше значение характеристики, тем более перспективной является подзадача для продолжения поиска в ней глобального минимума исходной задачи (\ref{main_problem}). Поэтому на каждой итерации выбирается подзадача с максимальной характеристикой для проведения в ней очередного испытания. Это испытание либо приводит к вычислению значения целевой функции $\varphi(y)$ (если выбранная подзадача принадлежала уровню $j=M$), либо порождает новые подзадачи согласно (\ref{block_nested_i}) при $j\leq M-1$. В последнем случае новые порожденные задачи добавляются к текущему множеству задач, вычисляются их характеристики и процесс повторяется. Завершение процесса оптимизации происходит, когда в корневой задаче выполняется условие остановки алгоритма, решающего эту задачу.

Отметим новые элементы предлагаемой схемы редукции размерности:
\begin{itemize}
	\item 
В адаптивной многошаговой схеме все порождаемые задачи решаются совместно -- на каждой итерации выбирается одна (наиболее перспективная) подзадача из множества подзадач, в которой проводится одно испытание. Такой подход, с одной стороны, требует одновременного хранения поисковой информации во всех решаемых подзадачах. С другой стороны, он позволяет (при необходимости) увеличивать необходимую точность решения задачи (\ref{main_problem}) без проведения повторного решения всех подзадач (\ref{block_nested_i}). Например, начальная стадия оптимизации может быть выполнена с достаточно грубой точностью, что позволит быстро получить некоторую оценку решения; далее (после анализа результатов) точность может быть повышена и процесс глобального поиска может быть продолжен;
	\item
	В исходной многошаговой схеме редукции размерности решение каждой возникающей подзадачи проводится ``до конца'', т.е. порождения новой подзадачи не произойдет, пока не будет решена текущая подзадача. Но точка $\overline{u_i}$ ее решения, вообще говоря, не будет совпадать с $i$-й компонентой вектора решения $y^* = (u_1^\ast,...,u_i^\ast,...,u_M^\ast)$ исходной многомерной задачи. В адаптивной схеме подобные бесперспективные подзадачи могут быть отбракованы уже на начальном этапе их решения;
\item
Наличие правила выбора подзадачи при выполнении очередной итерации в рамках адаптивной схемы позволяет получать различные ее варианты, соответствующие разным правилам выбора подзадачи. Например, как исходная многошаговая схема редукции размерности, так и схема редукции размерности с помощью единственной кривой Пеано могут быть получены как частные случаи нового подхода;
\item
Существование множества одновременно решаемых подзадач позволяет организовать их параллельное решение и эффективно задействовать ресурсы современных вычислительных систем. Некоторые результаты в данном направлении представлены в \cite{Gergel19}.
\end{itemize}

\section{Результаты экспериментов}

Стандартный подход к сравнению алгоритмов глобальной оптимизации основан на решении указанными методами серии задач, выбранных случайно из некоторого класса.
Будем использовать два класса многоэкстремальных тестовых функций (GKLS \textit{Simple}, GKLS \textit{Hard}), описанных в \cite{Gaviano03}.

В \cite{Grishagin18,Sovrasov19} было экспериментально установлено, что алгоритм глобального поиска (АГП) как с использованием разверток, так и в сочетании с адаптивной схемой редукции размерности превосходит многие известные алгоритмы глобальной оптимизации, включая методы DIRECT и DIRECT\textit{l}. Поэтому в данном исследовании ограничимся сравнением вариантов АГП в сочетании с различными схемами редукции размерности.

Для сравнения эффективности работы алгоритмов будем использовать два критерия: среднее число испытаний и операционные характеристики.
Операционной характеристикой алгоритма называется функция $P(k)$, определяемая как доля задач из рассматриваемой серии, для решения которых потребовалось не более чем $k$ испытаний.
Задачу будем считать решенной, если алгоритм сгенерировал точку $y^k$ очередного испытания в окрестности решения $y^\ast$, т.е. $\left\|y^k-y^\ast\right\| < \delta \left\|b-a\right\|$, где $\delta = 10^{-2}$, $a$ и $b$ --- границы области поиска $D$.

\begin{table}
\centering
\captionsetup{labelfont=bf}
\caption{Среднее число испытаний на классах GKLS \textit{Simple} и \textit{Hard}, $N=2$}\label{tab1}
\begin{tabular}{| lcc |}
\hline 
%\smallskip
 & GKLS \textit{Simple} &  GKLS \textit{Hard} \\
%\noalign{\smallskip}
\hline
%\noalign{\smallskip}
 $K_e$ &  252 & 674 \\
 $K_n$ &  697 & 1252 \\
 $K_a$ &  279 & 815 \\
%\noalign{\smallskip}
\hline
\end{tabular}
\end{table}


Первая серия экспериментов была проведена на двумерных задачах классов GKLS \textit{Simple} и GKLS \textit{Hard} (100 функций каждого класса). В табл.~\ref{tab1}  представлено среднее число испытаний, выполненных АГП с использованием разверток ($K_e$), многошаговой схемы ($K_n$) и адаптивной многошаговой схемы ($K_a$). На рис.~\ref{fig2} \marginpar{\framebox{Рис. 2}}
приведены операционные характеристики алгоритмов, полученные на указанных классах задач. Сплошная линия соответствует алгоритму с использованием разверток, точечная линия --- адаптивной многошаговой схеме, штриховая линия --- многошаговой схеме. Результаты экспериментов показывают, что АГП с использованием адаптивной многошаговой схемы демонстрирует примерно одинаковое быстродействие  по сравнению с развертками и оба они значительно превосходят алгоритм, использующий многошаговую схему. Поэтому в дальнейших экспериментах ограничимся сравнением различных вариантов адаптивной схемы редукции размерности.

\begin{table}
\captionsetup{labelfont=bf}
\centering
\caption{Среднее число испытаний на классах GKLS \textit{Simple} и \textit{Hard}, $N=4$}\label{tab2}
\begin{tabular}{| l c c |}
\hline
%\noalign{\smallskip}
 &    GKLS \textit{Simple} &  GKLS \textit{Hard} \\
%\noalign{\smallskip}
\hline
%\noalign{\smallskip}
 $K_a$  &  21 747 & 35 633 \\
 $K_{ba}$ &  15 942 & 33 206 \\
%\noalign{\smallskip}
\hline
\end{tabular}
\end{table}


Вторая серия экспериментов была проведена на четырехмерных задачах классов GKLS \textit{Simple} и GKLS \textit{Hard} (100 функций каждого класса). В табл.~\ref{tab2}  представлено среднее число испытаний, выполненных АГП с использованием адаптивной многошаговой схемы ($K_a$) и блочной адаптивной схемы ($K_{ba}$) с формированием двух уровней подзадач одинаковой размерности $N_1=N_2=2$. Отметим, что при использовании исходного варианта адаптивной многошаговой схемы при решении задачи размерности $N=4$ формируется четыре уровня одномерных подзадач, что усложняет их обработку.

На рис.~\ref{fig3} \marginpar{\framebox{Рис. 3}}
приведены операционные характеристики алгоритмов, полученные на классах GKLS \textit{Simple} и GKLS \textit{Hard}. Точечная линия соответствует алгоритму с использованием адаптивной, а сплошная ---  блочной адаптивной схемы. 
Результаты экспериментов показывают, что использование блочной адаптивной схемы дает ощутимый выигрыш по числу испытаний (до 35 \%) по сравнению с исходной схемой редукции размерности.

Последняя серия экспериментов была проведена на шестимерных задачах класса GKLS \textit{Simple} (100 функций). Сравнивалась работа АГП с использованием разверток и блочной адаптивной схемы с формированием двух уровней подзадач одинаковой размерности $N_1=N_2=3$.
Среднее число испытаний, выполненных АГП с использованием разверток, составило 102 987, тогда как с использованием блочной адаптивной схемы  --- 75 390. 
На рис.~\ref{fig4} \marginpar{\framebox{Рис. 4}}
приведены операционные характеристики алгоритмов. Точечная линия соответствует алгоритму с использованием разверток, а сплошная --- блочной адаптивной схемы. 

\section{Заключение}

В данной статье предложена обобщенная адаптивная схема редукции размерности для задач глобальной оптимизации, комбинирующая использование кривых Пеано и схему вложенной (рекурсивной) оптимизации. Для решения редуцированных подзадач используется алгоритм глобального поиска. Приведена вычислительная схема алгоритма, рассмотрены основные вопросы, связанные с использованием адаптивной схемы редукции размерности. Проведены вычислительные эксперименты на серии тестовых задач с целью сравнения эффективности различных схем редукции размерности. 
Результаты экспериментов показывают, что использование блочной адаптивной схемы редукции размерности может значительно сократить число испытаний, необходимое для решения задачи с заданной точностью. 

\begin{thebibliography}{10}


\bibitem{Evtushenko71}
{\it	Евтушенко Ю.Г.} Численный метод поиска глобального экстремума функций (перебор на неравномерной сетке) // Журн. вычисл. матем. и матем. физ. 1971. T.~11. №~6. С.~1390--1403.

\bibitem{Pijavski72}
{\it	Пиявский С.А.} Один алгоритм отыскания абсолютного экстремума функций // Журн. вычисл. матем. и матем. физ. 1972. Т. 12. № 4. С. 888--896.

\bibitem{Shubert72}
{\it Shubert B.O.} A Sequential Method Seeking the Global Maximum of a Function // SIAM J. Numer. Anal. 1972. V. 9. P. 379–388.

\bibitem{Jones93} 
{\it Jones D.R., Perttunen C.D., Stuckman B.E.} Lipschitzian Optimization without the Lipschitz Constant // J. Optim. Theory Appl. 1993. V. 79. No. 1. P. 157--181.

\bibitem{Pinter96}
{\it Pinter J.D.} Global Optimization in Action (Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications). Dordrecht: Kluwer Acad. Publishers, 1996.

\bibitem{Zilinskas08}
{\it {\v Z}ilinskas J.} Branch and Bound with Simplicial Partitions for Global Optimization // Math. Model. Anal. 2008. V. 13. No. 1. P.~145--159.

\bibitem{Evtushenko07}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И.А.} Распараллеливание процесса поиска глобального экстремума // АиТ. 2007. №~5. C.~46--58.

{\it Evtushenko Yu.G., Malkova V.U., Stanevichyus A.A.} Parallelization of the Global Extremum Searching Process // Autom. Remote Control. 2007. V. 68. No. 5. P.~787--798.

\bibitem{Evtushenko09}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И.А.} Параллельный поиск глобального экстремума функций многих переменных // Журн. вычисл. матем. и матем. физ. 2009. Т.~49. №~2. C.~255--269.

\bibitem{Jones09}%Проверить формат ссылки!!!
{\it Jones D.R.} The DIRECT global optimization algorithm / Floudas C.A., Pardalos P.M. (eds.) The Encyclopedia of Optimization. Second Edition.  Springer, 2009. P.~725--735.

\bibitem{Zilinskas10}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J., Grothey A.} Investigation of Selection Strategies in Branch and Bound Algorithm with Simplicial Partitions and Combination of Lipschitz Bounds // Optim. Lett. 2010. V. 4 (1). P. 173--83.

\bibitem{Evtushenko13}
{\it Evtushenko Y.G., Posypkin M.A.} A Deterministic Approach to Global Box-Constrained Optimization // Optim. Lett. 2013. V. 7 (4). P. 819--829.

\bibitem{Kvasov13}
{\it Квасов Д.Е., Сергеев Я.Д.} Методы липшицевой глобальной оптимизации в задачах управления // АиТ. 2013. № 9. C. 3--19.

{\it Kvasov D.E., Sergeyev Ya.D.} Lipschitz Global Optimization Methods in Control Problems // Autom. Remote Control. 2013. V. 74. No. 9. P. 1435--1448.

\bibitem{Paulavicius16}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J.} Advantages of Simplicial Partitioning for Lipschitz Optimization Problems with Linear Constraints // Optim. Lett. 2016. V.~10~(2). P.~237--246.

\bibitem{Kureichik04}
{\it Гладков Л.А., Курейчик В.В., Курейчик В.М.} Генетические алгоритмы. Уч. пос. М.: Физматлит, 2004.

\bibitem{Karpenko08}
{\it Карпенко А.П.} Современные алгоритмы поисковой оптимизации. Алгоритмы, вдохновленные природой. Уч. пос. М.: Изд-во МГТУ им. Н.Э. Баумана, 2014.

\bibitem{Kvasov18}
{\it Kvasov D.E., Mukhametzhanov M.S.} Metaheuristic vs. Deterministic Global Optimization Algorithms. The Univariate Case // Appl. Math. Comput. 2018. V.~318. P.~245--259.

\bibitem{Sergeyev18}
{\it Sergeyev Y., Kvasov D., Mukhametzhanov M.} On the Efficiency of Nature-Inspired Metaheuristics in Expensive Global Optimization with Limited Budget // Sci.
Rep. V.~8~(1). Art. No. 435.

\bibitem{Neymark66a}
{\it Неймарк Ю.И., Стронгин Р.Г.} Поиск экстремума функций по принципу максимума информации // АиТ. 1966. №~11. С.~113--118.

{\it Neimark Yu.I., Strongin R.G.} Function Extremum Search with the Use of Information Maximum Principle // Autom. Remote Control. 1966. V. 27. No. 1. P.~101--105.

\bibitem{Neymark66b}
{\it Неймарк Ю.И., Стронгин Р.Г.} Информационный подход к задаче поиска экстремума функций // Изв. АН СССР, Технич. кибернетика. 1966. №~1. С.~17--26.

\bibitem{Strongin70}
{\it Стронгин Р.Г.} Многоэкстремальная минимизация // АиТ. 1970. №~7. С.~63--67.

{\it Strongin R.G.} Multiextremal Minimization // Autom. Remote Control. 1970. V. 7. P.~1085--1088.

\bibitem{Strongin78}
{\it Стронгин Р.Г.} Численные методы в многоэкстремальных задачах (информационно-статистические алгоритмы). М.: Наука, 1978.

\bibitem{Strongin86}
{\it Стронгин Р.Г., Маркин Д.Л.} Минимизация многоэкстремальных функций при невыпуклых ограничениях // Кибернетика. 1986. №~4. С.~63--69.

\bibitem{Strongin87}
{\it Маркин Д.Л., Стронгин Р.Г.} Метод решения многоэкстремальных задач с невыпуклыми ограничениями, использующий априорную информацию об оценках оптимума // Журн. вычисл. матем. и матем. физ. 1987. Т.~27. №~1. С.~52--61.

\bibitem{Strongin93}
{\it Маркин Д.Л., Стронгин Р.Г.} О равномерной оценке множества слабоэффективных точек в многоэкстремальных многокритериальных задачах оптимизации // Журн. вычисл. матем. и матем. физ. 1993. Т.~33. №~2. С.~195--205.

\bibitem{Strongin13}
{\it Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А.} Параллельные вычисления в задачах глобальной оптимизации. М.: Изд-во МГУ, 2013.

\bibitem{Strongin91}
{\it Стронгин Р.Г.} Параллельная многоэкстремальная оптимизация с использованием множества разверток // Журн. вычисл. матем. и матем. физ. 1991. T.~31. №~8. С.~1173--1185.

\bibitem{Strongin00}
{\it Strongin R.G., Sergeev Ya.D.} Global Optimization with Non-Convex Constraints. Sequential and Parallel Algorithms. Dordrecht: Kluwer Acad. Publishers, 2000.

\bibitem{Grishagin97}
{\it Grishagin V.A., Sergeyev Y.D., Strongin R.G.} Parallel Characteristical Algorithms for Solving Problems of Global Optimization // J. Glob. Optim. 1997. V.~10~(2). P. 185--206.

\bibitem{Grishagin01}
{\it Sergeyev Y., Grishagin V.} Parallel Asynchronous Global Search and the Nested Optimization Scheme // J. Comput. Anal. Appl. 2001. V.~3~(2). P.~123--145.

\bibitem{Grishagin16}
{\it Gergel V., Grishagin V., Gergel A.} Adaptive Nested Optimization Scheme for Multidimensional Global Search // J. Glob. Optim. 2016. V.~66~(1). P.~35--51.

\bibitem{Barkalov2014}
{\it Barkalov K., Gergel V.} Multilevel Scheme of Dimensionality Reduction for Parallel Global Search Algorithms / OPT-i 2014 --- 1st Int. Conf. on Engineering and Applied Sciences Optimization. Proc. 2014. P. 2111--2124.

\bibitem{Grishagin2001}
{\it Sergeyev Y., Grishagin V.} Parallel Asynchronous Global Search and the Nested Optimization Scheme // J. Comput. Anal. Appl. 2001. V. 3~(2). P. 123--145.

\bibitem{Grishagin2015}
{\it Gergel V., Grishagin V., Israfilov R.} Local Tuning in Nested Scheme of Global Optimization // Procedia Computer Sci. 2015. V.~51~(1). P.~865--874.

\bibitem{Barkalov2016}
{\it Barkalov K., Lebedev I.} Solving Multidimensional Global Optimization Problems using Graphics Accelerators // CCIS. 2016. V. 687. P. 224--235.

\bibitem{Grishagin2016_1}
{\it Grishagin V., Israfilov R., Sergeyev Y.} Comparative Efficiency of Dimensionality Reduction Schemes in Global Optimization // AIP Conf. Proc. 2016. V. 1776. Art. No. 060011.

\bibitem{Grishagin18}
{\it Grishagin V., Israfiov R., Sergeyev Y.} Convergence Conditions and Numerical Comparison of Global Optimization Methods Based on Dimensionality Reduction Schemes // Appl. Math. Comput. 2018. V. 318. P. 270--280.

\bibitem{Gergel19}
{\it Gergel V., Grishagin V., Israfilov R.} Parallel Dimensionality Reduction for Multiextremal Optimization Problems // LNCS. 2019. V. 11657. P. 166--178. 

\bibitem{Gaviano03}
{\it Gaviano M., Lera D., Kvasov D.E., Sergeyev Ya.D.} Software for Generation of Classes of Test Functions with Known Local and Global Minima for Global Optimization // ACM Trans. Math. Software. 2003. V. 29. P. 469--480.

\bibitem{Sovrasov19}
{\it Sovrasov V.} Comparison of Several Stochastic and Deterministic Derivative-Free Global Optimization Algorithms // LNCS. 2019. V. 11548. P. 70--81. 

\end{thebibliography}


\clearpage

%\begin{center}
%\bfseries Рисунки к статье  
%\end{center}
\phantom{f}

\vfill

\begin{figure}[h]
\begin{center}%[scale=0.3][width=0.75\textwidth]
  \includegraphics[width=0.7\textwidth]{fig1.pdf} 
  %\caption*{Взаимосвязи между подзадачами в многошаговой схеме.}\label{fig0} 
  \caption{}\label{fig0} 
\end{center}
\end{figure}

\vfill


\clearpage

\phantom{f}
\vfill

\begin{figure}[h]
\begin{minipage}{0.5\linewidth}
 \center{а}  \\ \center{\includegraphics[width=1.0\linewidth]{2DSimple.pdf}}
\end{minipage}
\begin{minipage}{0.5\linewidth}
 \center{б} \\ \center{\includegraphics[width=1.0\linewidth]{2DHard.pdf}}
\end{minipage}
%\caption{Операционные характеристики на классах GKLS \textit{Simple} (а) и \textit{Hard} (б), $N=2$.}
\caption{} \label{fig2}
\end{figure}

\vfill

\clearpage

\phantom{f}
\vfill

\begin{figure}[h]
\begin{minipage}{0.5\linewidth}
\center{а} \\ \center{\includegraphics[width=1.0\linewidth]{4DSimple.pdf}}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\center{б} \\ \center{\includegraphics[width=1.0\linewidth]{4DHard.pdf}}
\end{minipage}
%\caption{Операционные характеристики на классах GKLS \textit{Simple} (а) и \textit{Hard} (б), $N=4$.}
\caption{} \label{fig3}
\end{figure}

\vfill

\clearpage

\begin{figure}
\center
\includegraphics[width=0.5\linewidth]{6DSimple.pdf}
%\caption{Операционные характеристики на классе GKLS \textit{Simple}, $N=6$.}
\caption{} \label{fig4}
\end{figure}

\clearpage

\begin{center}
\bfseries Список подрисуночных подписей  
\end{center}

\begin{itemize}%[font=\normalfont]
  \item [Рис. 1.] Взаимосвязи между подзадачами в многошаговой схеме.
  \item [Рис. 2.] Операционные характеристики на классах GKLS \textit{Simple} (а) и \textit{Hard} (б), $N=2$.
  \item [Рис. 3.] Операционные характеристики на классах GKLS \textit{Simple} (а) и \textit{Hard} (б), $N=4$.
	\item [Рис. 4.] Операционные характеристики на классе GKLS \textit{Simple}, $N=6$.
\end{itemize}



\end{document}
