\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
%\usepackage[T1]{fontenc}
\modulolinenumbers[5]

%\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Global optimization method with dual Lipschitz constant estimates for problems with non-convex constraints}

%% Group authors per affiliation:
\author{Roman Strongin}
\author{Konstantin Barkalov}
\author{Semen Bevzuk}
%\address[mymainaddress]{Lobachevsky State University of Nizhni Novgorod, Nizhni Novgorod, Russia}
%\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
%\cortext[mycorrespondingauthor]{Corresponding author}
%\ead{support@elsevier.com}
\address{Lobachevsky State University of Nizhni Novgorod, Nizhni Novgorod, Russia}

\begin{abstract}
This paper considers the constrained global optimization problems, in which the functions are of the “black box” type and satisfy the Lipschitz condition. The algorithms for solving the problems of this class require the use of adequate estimates of the a priori unknown Lipschitz constants for the problem functions. A novel approach presented in this paper is based on a simultaneous use of two estimates of the Lipschitz constant: an overestimated and an underestimated one. The upper estimate provides the global convergence whereas the lower one reduces the number of trials necessary to find the global optimizer with the required accuracy. In this case,  the considered algorithm for solving the constrained problems doesn’t use the ideas of the penalty function method; each constraint of the problem is accounted for separately. The convergence conditions of the proposed algorithm are formulated in the corresponding theorem. The results of the numerical experiments on a series of multiextremal problems with non-convex constraints demonstrating the efficiency of the proposed scheme of dual Lipschitz constant estimates are presented.
\end{abstract}

\begin{keyword}
Global optimization, Multiextremal problems, Non-convex constraints, Lipschitz constant estimates 
\end{keyword}

\end{frontmatter}

\linenumbers


\section{Introduction}
	In this paper, we consider constrained global optimization problems and the numerical methods for solving problems of this type. Such problems are often encountered in applications (see, for example, the review \cite{Pinter2006}). As a rule, problem functions are defined by a program code, i.e. they are “black-box” functions, for which the computing of the values can be a computationally costly operation since in applied problems it will require  numerical modeling. Any  possibility to estimate reliably the global optimum in the multiextremal problems with “black-box” functions is based fundamentally on the auxiliary assumptions relating the possible values of the problem functions to their known values at the points of the trials already performed.
	
	The assumption that the problem functions satisfy the Lipschitz condition is typical for many applied problems (in this case, one can speak about the Lipschitzian optimization problems) because the ratio of the functions’ variations to corresponding ones of the variables usually cannot exceed some threshold determined by the limited energy of variations in the system. This threshold can be described using the Lipschitz constant.
	
	The methods for solving the Lipschitzian optimization problems constitute a very important area in the development of the global optimization methods and are the objects of investigations of many researchers. The first algorithms of this class were proposed as early as in the 1970s \cite{Evtushenko1971, Piyavskii1972, Shubert1972, Strongin1970}; since that time, this area has been continuing to develop extensively (see, for example \cite{Evtushenko2009, Evtushenko2013, Strongin2000, Sergeyev2013, Jones2009}). Note that these global optimization algorithms are, as a rule, oriented towards solving either  unconstrained optimization problems 
	\cite{Zilinskas, Sergeyev, Pinter, Jones}
or problems with constraints of a simple structure \cite{Vaz2009, Paulavicius2016}. The solving of the problems with complex non-convex constraints is usually performed with the use of the penalty functions method \cite{Stripinis2019, Pillo2012, Pillo2016}, which has a number of drawbacks.  In particular, this method cannot be applied to the problems with the partially defined functions (i.e. when any constraint is violated, the values of all the other functions of the problem remain undefined).
	
	This situation is typical of many optimal design problems, when, in case of violation of some conditions of the modeled object's functioning, other characteristics of the object appear to be undefined. For example, the modeled object is an electronic device, and its response time is minimized. However, the response time is undefined if at a given combination of parameters the device fails to operate. Such a partial computability of the functions in the constrained optimization problems complicates essentially (in some cases, makes impossible) the application of the penalty functions method. 
	
	Within the framework of our research, an original approach to the minimization of the multiextremal functions with non-convex constraints called the index method of the accounting for the constraints 
	\cite{Strongin} 
was used. This approach is based on separate accounting for each constraint of the problem and does not involve the use of the penalty functions. According to the rules of the index method, every iteration called trial at the corresponding point of the search domain includes a sequential checking of the feasibility of the problem constraints at this point. When the first violation of any constraint is found, the trial is terminated (the values of the remaining problem functions are not computed at this point) and the transition to the next iteration point is initiated. This allows solving the problems with partially defined functions. The experimental comparison of the index method of accounting for the constraints and the penalty function method demonstrated the advantages of the index method in terms of the number of iterations and the number of computations of the problem functions \cite{Barkalov2017_1, Barkalov2017_2}.
	
	Note that the values of the Lipschitz constant for the problem functions are usually not known a priori, which  makes their estimation one of the key problems in the construction of the Lipschitzian optimization methods. The methods utilizing the values of the Lipschitz constant predefined a priori (see, for example, \cite{Piyavskii1972, Shubert1972, Wood1991, Meewella1988, Mladineo1986}) are important in the theoretical aspect, but it is difficult to apply such methods in applied problems (in which the information of the Lipschitz constant values is absent).
		
		The value of the unknown Lipschitz constant estimate affects the convergence rate of the algorithm essentially. Therefore, the issue of its correct estimate is very important. An underestimate of the true value of the constant may lead to the loss of the algorithm convergence to the global solution. At the same time, too high a value of the constant estimate not corresponding to real behavior of the function results in slow convergence of the algorithm to the global minimum point. 
		
		Several typical methods for adaptive estimation of the Lipschitz constant according to the results of the performed search trial are known:
\begin{itemize}[\labelitemii]
  \item global estimate of the constant $L$ in the whole search domain $D$ \cite{Horst1996, Pinter1996, Strongin2000}.
  \item local estimates of the constants $L_i$ in different subdomains $D_i$ of the search domain $D$ \cite{Kvasov2003, Sergeyev2010, Sergeyev2016}.
	\item choice of the estimates of the constant $L$ from some set of possible values \cite{Gablonsky2001, Jones1993, Jones2009, Sergeyev2006}.
\end{itemize}

	Each of the approaches listed above has its own advantages and drawbacks. For example, the use of the global estimate only in the entire search domain may slow down the convergence of the algorithm to the global minimum point. On the other hand, the use of the local estimates accelerating the convergence requires an adequate adjustment of the algorithm parameters in order to ensure the global convergence. 

	In this paper, we consider an algorithm, in which it is proposed to use two global estimates of the Lipschitz constant, one of them being much greater than the other one. The larger estimate ensures global convergence and the smaller one reduces the total number of trials needed to find the global optimizer. The choice of one of the two estimates to be used in the rules of the algorithm is performed in the adaptive manner, depending on the behavior of the function. This work continues the research reported in the paper 
	\cite{NUMTA2019},
in which the preliminary results have been obtained for the unconstrained problems.
	
	The main part of the paper has the following structure. In Section 2, the index method of accounting for the constraints and the dimensionality reduction scheme for the constrained global optimization problems are described. In Section 3, a description of the computational rules of the index global search algorithm is given and its theoretical properties are formulated. In Section 4, the index algorithm with dual Lipschitz constant estimates is formulated and its convergence is proved. Section 5 presents the results of the numerical experiments carried out on several series of the multiextremal problems with non-convex constraints. Section 6 concludes the paper.



\section{Index Method of Accounting for the Constraints and Dimensionality Reduction}
	We consider the global optimization problems of the form
\begin{equation}\label{problem}
	\min{\left\{\varphi(y):y \in D, \; g_j(y)\leq 0, \; 1 \leq j \leq m\right\}},
\end{equation}
\begin{equation}\label{D}
	D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}.
\end{equation}

	It is supposed that the objective function $\varphi(y)$ (henceforth denoted by $g_{m+1}(y)$) and the left-hand sides $g_{j}(y), 1 \leq j \leq m$, of the constraints satisfy the Lipschitz condition
\begin{equation}\label{lipschitz_condition}
	\left|g_j(y')-g_j (y'')\right| \leq L_j \|y'-y''\|, \; y', y''\in D, \; 1\leq j \leq m+1.
\end{equation}
with \textit{a priori} unknown constant $L_j, 1\leq j \leq m+1$, and may be multiextremal.

	The analytical form of the functions $g_{j}(y)$ may be unknown, i.e. these functions may be defined algorithmically (“black-box” functions). In this case, it is supposed that even a single search trial may be a computationally costly operation since in applied problems it involves the need to perform numerical modeling. 
	
	Along with the exact solution $y^\ast$ of the problem (\ref{problem}), we will consider also the $\varepsilon$-reserved solution $y_{\epsilon}$, for which 
\begin{equation}\label{epsilon_reserved_solution}
	\varphi(y_{\epsilon})=\min{\left\{\varphi(y):y \in D, \; g_j(y)\leq -{\epsilon}_j, \; 1 \leq j \leq m\right\}},
\end{equation}
and the set of solutions $Y_{\epsilon}$, which are not worse than the $\varepsilon$-reserved solution in the objective function value, i.e.
\begin{equation}\label{Y_epsilon}
	Y_{\epsilon}=\left\{ y \in D, \; g_j(y) \leq 0, \; 1 \leq j \leq m, \; \varphi(y) \leq \varphi(y_{\epsilon}) \right\}.
\end{equation}
Fig. \ref{fig:eps_reserved_solution} illustrates the concepts introduced.

\begin{figure}[h]
	\center{\includegraphics[width=0.7\linewidth]{img/epsilon_reserved_solution}}
	\caption{$\varepsilon$-reserved solution}
	\label{fig:eps_reserved_solution}
\end{figure}

	The existence of the $\varepsilon$-reserved solutions of problem (\ref{problem}) can be interpreted as some analog of the regularity condition in classical nonlinear programming problems. The applied role of this condition should be also noted. Even if the exact solution $y^\ast$ is found, its practical implementation is possible as some approximation of $y^\ast$ only. Therefore, it is important that there are feasible points from $Q_{m+1}$ close enough to $y^\ast$ (in coordinates and values). The existence of the $\varepsilon$-reserved solution guarantees the existence of such points. In this case, the domain $Y_{\epsilon}$ may play the role of a set of suitable approximations.

	We will consider the initial problem (\ref{problem}) assuming that the problem functions $g_{j}(y), 1 \leq j \leq m+1$ may be defined only partly. It means that they are defined and calculable only in the sets 
\begin{equation}\label{D_sets}
	D_1 = D, \; D_{j+1} = \left\{ y \in D_j: \; g_j(y) \leq 0 \right\}, \; 1 \leq j \leq m.
\end{equation}
This situation is typical for the optimal design problems when some conditions for the functioning of the modeled object are not met and its other characteristics appear to be undefined. In this approach, the first violation of any constraint indicating that the point ybeing considered is no longer feasible terminates further computations at the point $y$.

	Many well known approaches to solving the multidimensional global optimization problems are based on either explicit or implicit reduction of multidimensional problems to one-dimensional ones (see, for example, the description of methods using the diagonal partitions \cite{Sergeyev2006} or simplicial partitions \cite{Zilinskas2008}). In this paper, we will use the approach based on the dimensionality reduction with the use of the space-filling curve $y(x)$ (the Peano curve). Continuous unambiguous mapping $y(x)$ projecting the interval [0,1] onto an $N$-dimensional hypercube $D$ allows reducing a multidimensional constrained minimization problem in the domain $D$ to a one-dimensional constrained optimization problem in the interval [0,1]
\begin{equation}\label{one_dimensional_problem}
	\varphi(y(x^\ast))=\min \left\{\varphi(y(x)): x \in [0,1], \; g_j(y(x))\leq 0, \; 1 \leq j \leq m\right\},
\end{equation}
The issues of the numerical construction of the Peano space filling curve-type mappings and the corresponding theory were described in detail in \cite{Strongin2000, Sergeyev2013}. Note that the theoretical Peano curve $y(x)$ is a limit object and in practice is replaced by its approximation (called \textit{evolvent}) with predefined precision $2^{-m}$ where the number $m>1$ is a parameter of the construction (the density of the evolvent). In this case, the construction density of the evolvent is determined by the accuracy of the possible practical implementation of the problem solution found. For  illustration, Fig. \ref{fig:evolvent} (a), (b) shows the projections of the interval [0,1] are shown for the mapping $y(x)$ with $N=2$ and $m=4,5$, correspondingly.
\begin{figure}[h]
	\begin{minipage}[h]{0.48\linewidth}
		\center{\includegraphics[width=0.8\linewidth]{img/evolvent_N=2_m=4} \\ a)}
	\end{minipage}
	\hfill
	\begin{minipage}[h]{0.48\linewidth}
		\center{\includegraphics[width=0.8\linewidth]{img/evolvent_N=2_m=5} \\ b)}
	\end{minipage}
	\caption{Evolvent for $N=2$ and $m=4$ (a), $m=5$ (b)}
	\label{fig:evolvent}
\end{figure}

	The dimensionality reduction scheme considered here matches a multidimensional problem with Lipschitzian minimized function and Lipschitzian constraints to a one-dimensional problem, in which the corresponding functions satisfy the uniform H\"{o}lder condition (see \cite{Strongin2000, Sergeyev2013}), i.e.
$$
	\left|g_j(y(x'))-g_j(y(x''))\right| \leq K_j \left|x'-x'' \right|^{1/N}, \; x', x''\in [0,1], \; 1\leq j \leq m+1,
$$
where $N$ is the dimensionality of the initial multidimensional problem and the coefficients $K_j$ are related to the Lipschitz constants $L_j$ of the initial problem by the relations $K_j \leq 2L_j \sqrt{(N+3}$.

	According to (\ref{D_sets}) and (\ref{one_dimensional_problem}), the reduced one-dimensional functions $g_j (y(x)), 1 \leq j \leq m+1$ will also be defined and calculable in the corresponding intervals 
\begin{equation}\label{Q_intervals}
	Q_1=[0,1], \; Q_{j+1}=\left\{x \in Q_j : g_j(y(x)) \leq 0 \right\}, \; 1 \leq j \leq m.
\end{equation}
These relations allow introducing a classification of the points $x \in [0,1]$ according to the number $\nu = \nu(x)$ of the constraints calculable at the given point. We will call this number $\nu = \nu(x)$ the \textit{index} of the point $x$. The index can be determined from the conditions 
$$
	g_j(y(x)) \leq 0, \; 1 \leq j \leq \nu-1, \; g_{\nu}(y(x))>0,
$$
where the last inequality is insufficient if $\nu = m+1$. 

	Thus, a search trial at some point $x^k \in [0,1]$ performed at the $k^{th}$ iteration of the algorithm will consist of the following sequence of operations:
\begin{enumerate} 
  \item Determine the image $y^k=y(x^k )$ with the use of the evolvent $y(x)$. 
  \item Compute the values $g_1 (y^k ), \ldots ,g_{\nu} (y^k )$ where $\nu = \nu(x)$ is the index of the point.
	\item Fix the result of the trial as three values 
\end{enumerate}
	
\begin{equation}\label{trial_result}
	x^k, \; z^k = g_{\nu}\left( y(x^k) \right), \; \nu = \nu(x^k),
\end{equation}

	According to this scheme, the trial is terminated at the point $y^k=y(x^k)$ when the first violated constraint  is found.
	
	In the case when the point $y^k$ is a feasible one, i.e. when $y(x^k) \in Q_(m+1)$, the trial involves checking all constraints and computing the objective function. Here, the value of the index is accepted to be equal to the quantity $\nu = m+1$.
	
	The main idea of the index scheme of accounting for the constraints consists in the reduction of the constrained problem (\ref{one_dimensional_problem}) (the functions of which satisfy the H\"{o}lder condition with corresponding coefficients $K_j$) to an unconstrained problem of the form
\begin{equation}\label{reduction_problem}
	\psi(x^*)=\min \left\{\psi(x): x \in [0,1] \right\},
\end{equation}
\begin{eqnarray}
	\psi(x)=\frac{g_{\nu}(y(x))}{K_{\nu}} - 
	\left\{
   \begin{array}{lr}
     0, & \nu \leq m,\\
     \frac{\varphi^\ast}{K_{m+1}}, & \nu = m + 1.
   \end{array}
	\right.
\end{eqnarray}
where $\nu = \nu(x)$ and $\varphi^\ast$ is the objective function value at the point of the problem solution (\ref{one_dimensional_problem}).

	The value $\psi(x)$ at $\nu(x) \leq m$ matches the value of the left-hand part of the first constraint violated at this point $g_{\nu}(y(x))$ divided by the constant $K_{\nu}$. In the case when $\nu(x) = m+1$ 
$$
	\psi(x)=\frac{\varphi(y(x))-\varphi^\ast}{K_{m+1}}
$$
and, consequently, the set of the points, in which the function $\psi(x)$ turns to zero matches the set of the global minimum points of the problem (\ref{one_dimensional_problem}). It should be noted that in any subdomain $Q_j$ the function $\psi(x)$ has the H\"{o}lder coefficient equal to 1.
	
	These considerations constitute the basis of the index global search algorithm, according to which the unknown constants $K_{\nu}$ and the unknown minimum value $\varphi^\ast$ are substituted in the course of computations by their current approximations.



\section{Index-Based Global Search Algorithm}

\[
	0=x_0<x_1<\dots <x_k<x_{k+1}=1,
\]

\[
	I_\nu =\left\{i:1 \leq i \leq k, \; \nu=\nu(x_i) \right\}, \; 1 \leq \nu \leq m+1,
\]

\[
	M=\max\left\{\nu=\nu(x_i), \; 1 \leq i \leq k \right \}.
\]

\begin{equation}\label{current_lower_bounds}
	\mu_{\nu} = \max\left\{ \frac{\left|z_i-z_j\right|}{ (x_i - x_j)^{1/N} }, \; i,j \in I_\nu, \; i>j \right\},
\end{equation}

\begin{equation}\label{z_estimates}
	z_\nu^\ast = \left\{
   \begin{array}{lr}
     -\epsilon_\nu, & \nu < M,\\
     \min\{ z_i: i\in I_\nu \}, & \nu = M,
   \end{array}
	\right.
\end{equation}

\begin{equation}\label{R_1}
	R(i)=\Delta_i+\frac{(z_i-z_{i-1})^2}{r_\nu^2 \mu_\nu^2\Delta_i}-2\frac{z_i+z_{i-1}-2z_\nu^\ast}{r_\nu \mu_\nu}, \;  \nu=\nu(x_i)=\nu(x_{i-1}),
\end{equation}
\begin{equation}\label{R_2}
	R(i)=2\Delta_i-4\frac{z_i-z_\nu^\ast}{r_\nu \mu_\nu}, \; \nu=\nu(x_i)>\nu(x_{i-1}),
\end{equation}
\begin{equation}\label{R_3}
R(i)=2\Delta_i-4\frac{z_{i-1}-z_\nu^\ast}{r_\nu \mu_\nu}, \; \nu=\nu(x_{i-1})>\nu(x_i),
\end{equation}

\begin{equation}\label{MaxR}
	R(t)=\max{\left\{R(i): 1 \leq i \leq k+1\right\}}.
\end{equation}

\[
	x^{k+1} = \frac{x_t + x_{t-1}}{2}, \; \nu(x_{t-1}) \neq \nu(x_t).
\]

\[
	x^{k+1} = \frac{x_t+x_{t-1}}{2} + \frac{\mathrm{sign}(z_t-z_{t-1})}{2r_\nu}\left[\frac{\left|z_t-z_{t-1}\right|}{\mu_\nu}\right]^N, \; \nu=\nu(x_{t-1})=\nu(x_t).
\]

\[
	g_j \left( y(x) \right) = G_j \left( y(x) \right), \; x \in Q_j, \; 1 \leq j \leq m+1,
\]

\begin{equation}\label{theorem_inequalities}
	r_{\nu}\mu_{\nu} > 2^{3-\frac{1}{N}}L_{\nu}\sqrt{N+3}, \; 1 \leq \nu \leq m+1, 
\end{equation}

\[
	\varphi(\bar y) = \inf\left\{\varphi(y^q):g_j(y^q) \leq 0, 1 \leq j \leq m, q = 1,2,\ldots \right\} \leq \varphi(y_{\epsilon}).
\]

\begin{equation}\label{epsilon_nu}
	\epsilon_{\nu} = \mu_{\nu}\delta, \; 1 \leq \nu \leq m, 
\end{equation}


\section{Index Algorithm with Dual Lipschitz Constant Estimates}

\begin{equation}\label{estimates_Lipschitz_constants}
	\frac{r_{\nu}\mu_{\nu}}{2^{3-\frac{1}{N}}L_{\nu}\sqrt{N+3}}, \; 1 \leq \nu \leq m+1, 
\end{equation}

\begin{equation}\label{R_glob_1}
	R(i)=R_{glob}(i)=R_{loc}(i)=2\Delta_i>0. 
\end{equation}

\[
	z_i+z_{i-1}-2z_M^* = |z_i-z_{i-1}|\leq \mu_M\Delta_i
\]

\begin{eqnarray}
	R(i) & = & \Delta_i + \frac{(z_i-z_{i-1})^2}{r_M^2\mu_M^2\Delta_i} - 2\frac{z_i+z_{i-1}-2z_M^*}{r_M\mu_M} \geq \Delta_i + \frac{(z_i-z_{i-1})^2}{r_M^2\mu_M^2\Delta_i} - 2\frac{\mu_M\Delta_i}{r_M\mu_M} \nonumber \\
	& \geq & \Delta_i + \frac{(z_i-z_{i-1})^2}{r_M^2\mu_M^2\Delta_i} - 2\frac{\Delta_i}{r_M} = \Delta_i + \Delta_i\frac{(z_i-z_{i-1})^2/\Delta_i^2}{r_M^2\mu_M^2\Delta_i^2} - 2\frac{\Delta_i}{r_M}
\nonumber \\
	& \geq & \Delta_i + \frac{\Delta_i}{r_M^2} - 2\frac{\Delta_i}{r_M} = \Delta_i \left( 1-\frac{1}{r_M} \right)^2>0  
	\nonumber
\end{eqnarray}

\begin{equation}\label{R_glob_greater_R_loc}
	R_{glob}(i)=\Delta_i\left(1-\frac{1}{r_M^{glob}} \right)^2 > \Delta_i\left(1-\frac{1}{r_M^{loc}} \right)^2 = R_{loc}(i)
\end{equation}

\begin{equation}\label{R_glob_equal_R_loc}
	R_{glob}(i) = \rho R_{loc}(i)
\end{equation}

\begin{equation}\label{R_max}
	R(i) = \max{\left\{ R_{glob}(i), \rho R_{loc}(i) \right\}}
\end{equation}

\begin{equation}\label{R_max_t}
	R(t) = \max{\left\{ R(i): 1 \leq i \leq k+1\right\}}
\end{equation}

\[
	x^{k+1} = \frac{x_t + x_{t-1}}{2}, \; \nu(x_{t-1}) \neq \nu(x_t).
\]

\[
	x^{k+1} = \frac{x_t+x_{t-1}}{2} + \frac{\mathrm{sign}(z_t-z_{t-1})}{2r_\nu}\left[\frac{\left|z_t-z_{t-1}\right|}{\mu_\nu}\right]^N, \; \nu=\nu(x_{t-1})=\nu(x_t).
\]

\begin{eqnarray}
	\varphi(y_1, y_2)=-1.5y_1^2\exp{\left\{1-y_1^2-20.25(y_1-y_2)^2\right\}}- \nonumber \\
	-\left(0.5(y_1-1)(y_2-1)\right)^4\exp{\left\{2-\left(0.5(y_1-1)\right)^4-(y_2-1)^4\right\}}
	\nonumber
\end{eqnarray}

\begin{eqnarray}
	g_1(y_1, y_2) &=& 0.01 \left( (y_1-2.2)^2+(y_2-1.2)^2-2.25 \right) \leq 0, \nonumber \\
	g_2(y_1, y_2) &=& 100 \left(1-(y_1-2)^2/1.44+(0.5y_2)^2 \right) \leq 0, \nonumber \\
	g_3(y_1, y_2) &=& 10 \left( y_2 - 1.5 - 1.5 \sin{\left( 6.283(y_1-1.75) \right)}\right) \leq 0
	\nonumber
\end{eqnarray}

\begin{equation}\label{phi_bar_y}
	\varphi(\bar y) = \inf\left\{\varphi(y^k):g_j(y^k) \leq 0, 1 \leq j \leq m, k = 1,2,\ldots \right\} \leq \varphi(y_{\epsilon}).
\end{equation}

\[
	R_{glob}(t) < \rho R_{loc}(t)
\]

\[
	z_j=g_{\nu}\left( y(x_j) \right) \leq g_{\nu}\left( y(\bar x) \right) + 2L_{\nu}\sqrt{N+3}(x_j-\bar x)^{1/N}, \; \nu=\nu(x_j)
\]

\[
	z_{j-1}=g_{\nu}\left( y(x_{j-1}) \right) \leq g_{\nu}\left( y(\bar x) \right) + 2L_{\nu}\sqrt{N+3}(\bar x - x_{j-1})^{1/N}, \; \nu=\nu(x_{j-1})
\]

\[
	g_{\nu}\left( y(\bar x) \right) \leq -\epsilon_{\nu}, \; 1\leq\nu\leq m.
\]

\[
	g_{m+1}\left( y(\bar x) \right) \leq z_{m+1}^*
\]

\begin{eqnarray}
	R(j) &=& \Delta_i + \frac{(z_j-z_{j-1})^2}{r_{\nu}^2\mu_{\nu}^2\Delta_i} - 2\frac{z_j+z_{j-1}-2z_{\nu}^*}{r_{\nu}\mu_{\nu}}  \nonumber \\
	&\geq& \Delta_i + 4\frac{z_{\nu}^*-\left( g_{\nu}\left( y(\bar x) \right)+L_{\nu}\sqrt{N+3}\left( (x_j-\bar x)^{\frac{1}{N}}+(\bar x - x_{j-1})^{\frac{1}{N}} \right)\right)}{r_{\nu}\mu_{\nu}} \nonumber \\
	&=& \Delta_i-4\frac{L_{\nu}\sqrt{N+3}\left( (x_j-\bar x)^{\frac{1}{N}}+(\bar x - x_{j-1})^{\frac{1}{N}} \right)}{r_{\nu}\mu_{\nu}}+4\frac{z_{\nu}^*-g_{\nu}\left( y(\bar x) \right)}{r_{\nu}\mu_{\nu}} \nonumber \\
	&=& \Delta_i-4\frac{L_{\nu}\sqrt{N+3}\left( \alpha^{\frac{1}{N}}+(1-\alpha)^{\frac{1}{N}} \right)}{r_{\nu}\mu_{\nu}}+4\frac{z_{\nu}^*-g_{\nu}\left( y(\bar x) \right)}{r_{\nu}\mu_{\nu}} \nonumber \\
	&\geq& \Delta_i\left(1-4\frac{L_{\nu}\sqrt{N+3}\max_{0\leq\alpha\leq1} {\left( \alpha^{\frac{1}{N}}+(1-\alpha)^{\frac{1}{N}} \right)}}{r_{\nu}\mu_{\nu}} \right)+4\frac{z_{\nu}^*-g_{\nu}\left( y(\bar x) \right)}{r_{\nu}\mu_{\nu}} \nonumber \\
	&=& \Delta_i\left(1-4\frac{2^{3-1/N}L_{\nu}\sqrt{N+3}}{r_{\nu}\mu_{\nu}} \right)+4\frac{z_{\nu}^*-g_{\nu}\left( y(\bar x) \right)}{r_{\nu}\mu_{\nu}} \geq 0
	\nonumber
\end{eqnarray}

\begin{eqnarray}
	R(j) &=& 2\Delta_i - 4\frac{z_j-z_{\nu}^*}{r_{\nu}\mu_{\nu}} \geq 2\Delta_i+4\frac{z_{\nu}^*-\left( g_{\nu}\left(y(\bar x)\right)+2L_{\nu}\sqrt{N+3}(x_j-\bar x)^{\frac{1}{N}} \right)}{r_{\nu}\mu_{\nu}} \nonumber \\
	&=& 2\Delta_i - 8\frac{L_{\nu}\sqrt{N+3}(x_j-\bar x)^{\frac{1}{N}}}{r_{\nu}\mu_{\nu}}+4\frac{z_{\nu}^*-g_{\nu}\left( y(\bar x) \right)}{r_{\nu}\mu_{\nu}} \nonumber \\
	&\geq& 2\Delta_i\left( 1-4\frac{L_{\nu}\sqrt{N+3}}{r_{\nu}\mu_{\nu}} \right) + 4\frac{z_{\nu}^*-g_{\nu}\left( y(\bar x) \right)}{r_{\nu}\mu_{\nu}} > 0
	\nonumber
\end{eqnarray}

\begin{equation}
	R_{glob}(j) > \rho R_{loc}(t)
\end{equation}

\section{TEST}

There are various bibliography styles available. You can select the style of your choice in the preamble of this document. These styles are Elsevier styles based on standard styles like Harvard and Vancouver. Please use Bib\TeX\ to generate your bibliography and include DOIs whenever available.

Here are two sample references: 

\cite{Evtushenko1971, Piyavskii1972, Shubert1972, Strongin1970, Evtushenko2009, Evtushenko2013, Strongin2000, Sergeyev2013, Pinter1996, Jones2009, Wood1991, Meewella1988, Mladineo1986, Vaz2009, Stripinis2019, Paulavicius2016, Pillo2012, Pillo2016, Barkalov2017_1, Barkalov2017_2, Sergeyev2006, Zilinskas2008, Sovrasov2019, Kvasov2003, Sergeyev2010,Sergeyev2016, Horst1996, Gablonsky2001, Jones1993, Gaviano2003, Barkalov2018, Paulavicius2014, Sergeyev2015, Strongin2018, Gergel2017_2, Gergel2019}.


\bibliography{bibliography}

\end{document}