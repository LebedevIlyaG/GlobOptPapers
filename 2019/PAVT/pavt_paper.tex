%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\usepackage{graphicx}
\usepackage{marvosym}
\usepackage{amssymb}
\usepackage{cite}
%%%%%ДОБАВИЛ ДЛЯ РУССКОГО ТЕКСТА
\usepackage[utf8x]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{cmap}
%%%%%
% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\def\UrlFont{\rmfamily}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
%
\title{Comparison of several sequential and parallel derivative-free global optimization algorithms}
%
\titlerunning{Comparison of several optimization methods}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Vladislav Sovrasov\inst{1}\letter \and Semen Bevzuk\inst{1}}
%
\authorrunning{Vladislav Sovrasov et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Vladislav Sovrasov and Semen Bevzuk}
%
\institute{Lobachevsky State University of Nizhni Novgorod, Russia \\
  \email{sovrasov.vlad@gmail.com, semen.bevzuk@gmail.com}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
This work considers several stochastic and deterministic derivative-free global optimization algorithms. In the first part of the paper popular sequential open-source solvers are compared against the Globalizer solver, which is developed at the Lobachevsky State University. The Globalizer is designed to solve problems with black-box objectives satisfying the Lipschitz condition and shows competitive performance with other similar solvers. The comparison is done on several sets of challenging multi-extremal benchmark functions. The second part of this work is devoted to comparison between the Globalizer and MIDACO solvers on systems with shared and distributed memory. MIDACO is a state-of-the-art global solver included to the TOMLAB optimization environment for MATLAB. Results of the benchmark show advantages of the Globalizer on small-dimensional, but sufficiently multi-extremal benchmark functions.
% We would like to encourage you to list your keywords within
% the abstract section using the \keywords{...} command.
\keywords{deterministic global optimization $\cdot$ stochastic global optimization
  $\cdot$ parallel numerical methods $\cdot$ derivative-free algorithms $\cdot$ black-box optimization}
\end{abstract}
%
\section{Introduction}
Нелинейная глобальная оптимизация невыпуклых функций традиционно считается одной из самых трудных
задач математического программирования. Отыскание глобального минимума функции от нескольких переменных
зачастую оказывается сложнее, чем локальная оптимизация в тысячемерном пространстве. Для последней может оказаться достаточно
применения простейшего метода градиентного спуска, в то время как чтобы \textit{гаранитрованно} отыскать глобальный оптимум методам
оптимизации приходится накапливать информацию о поведении целевой функции во всей области поиска \cite{Jones2009,Paulavicius2011,Evtushenko2013,strSergGO}. В последнее время стали популярны
различные стохастические алгоритмы глобально оптимизации, прежде всего эволюционные \cite{Storn1997, SCHLUTER2009, KennedyEberhart1995}. Они имеют довольно простую структуру, позволяют решать задачи большой размерности, но обеспечивают глобальную сходимость только в вероятностном смысле.

В данной работе рассмотрены open-source реализации девяти различных методов глобальной оптимизации.
Все алгоритмы были протестированы на наборе из 900 существенно многоэкстремальных функций, который был сгенерирован с
помощью специализированных генераторов задач \cite{Gaviano2003, grishaginClass}. Помимо сравнения последовательных алгоритмов, на подмножестве из 200 тестовых задач произведено сравнение солвера MIDACO \cite{Schlueter2012} и программной системы Globalizer \cite{globalizerSystem} в условиях работы на суперкомпьютере Лобачевский.

\section{Related Work}

\section{Statement of Multidimensional Global Optimization Problem}
In this paper, the core class of optimization problems, which can be solved using
Globalizer, is formulated. This class involves the multidimensional global
optimization problems without constraints, which can be defined in the following way:
\begin{equation}
\label{eq:task}
\begin{array}{cr}\\
  \varphi(y^*)=\min\{\varphi(y):y\in D\}, \\
  D=\{y\in \mathbb{R}^N:a_i\leq y_i\leq{b_i}, 1\leq{i}\leq{N}\}
\end{array}
\end{equation}
with the given boundary vectors  $a$ and  $b$. It is supposed, that the objective function
\(\varphi(y)\) satisfies the Lipschitz condition
\begin{equation}
\label{eq:lip}
|\varphi(y_1)-\varphi(y_2)|\leq L\Vert y_1-y_2\Vert,y_1,y_2\in D,
\end{equation}
where \(L>0\) is the Lipschitz constant, and \(||\cdot||\) denotes the norm in \(\mathbb{R}^N\)
space.
\par
Usually, the objective function \(\varphi(y)\) is defined as a computational procedure,
according to which the value \(\varphi(y)\) can be calculated for any vector \(y\in D\)
(let us further call such a calculation \textit{a trial}). It is supposed that this procedure
is time-consuming.
\section{Dimension Reduction with Evolvents}
Within the framework of the information-statistical global optimization theory,
the Peano space-filling curves (or evolvents) \(y(x)\) mapping the interval \([0,1]\)
onto an \(N\)-dimensional hypercube \(D\) unambiguously are used for the dimensionality
reduction \cite{sergeyevStronginLera2013}, \cite{strongin1978},
\cite{stronginGergelBarkalovParGO}, \cite{strSergGO}.
\par
As a result of the reduction, the initial multidimensional global optimization
problem (\ref{eq:task}) is reduced to the following one-dimensional problem:
\begin{equation}
\label{eq:oneDimTask}
\varphi(y(x^*))=\min\{\varphi(y(x)):x\in [0,1]\}.
\end{equation}
\par
It is important to note that this dimensionality reduction scheme transforms the % minimized
Lipschitzian function from (\ref{eq:task}) to the corresponding one-dimensional
function \(\varphi(y(x))\), which satisfies the uniform H{\"o}lder condition, i. e.
\begin{equation}
\label{eq:holder}
|\varphi(y(x_1))-\varphi(y(x_2))|\leq H{|x_1-x_2|}^{\frac{1}{N}}, x_1,x_2\in[0,1],
\end{equation}
where the constant $H$ is defined by the relation \(H=2L\sqrt{N+3}\), \(L\) is the Lipschitz
constant from (\ref{eq:lip}), and \(N\) is the dimensionality of the optimization problem
(\ref{eq:task}).
\par
The algorithms for the numerical construction of the Peano curve approximations are
given in \cite{strSergGO}.

\par
The computational scheme obtained as a result of the dimensionality reduction consists of the
following:
\begin{itemize}
  \item The optimization algorithm performs the minimization of the reduced one-dimensional
  function \(\varphi(y(x))\) from (\ref{eq:oneDimTask}),
  \item After determining the next trial point \(x\), a multidimensional image \(y\) is calculated by
using the mapping \(y(x)\),
  \item The value of the initial multidimensional function \(\varphi(y)\) is calculated at the point
\(y\in D\),
  \item The calculated value \(z=\varphi(y)\) is used further as the value of the reduced one-dimensional function \(\varphi(y(x))\) at the point \(x\).
\end{itemize}
\subsection{Rotated Evolvents}
To overcome complexity of the $S$-evolvent and to preserve the information on the nearness of the points in
the $N$-dimensional space, one more scheme of building of the multiple mappings was proposed.
The building of a set of Peano curves not by the shift along the main diagonal of the hypercube
but by rotation of the evolvents around the coordinate origin is a distinctive feature of the
proposed scheme \cite{Gergel2009}.
In Fig.~\ref{6_fig_9} two evolvents being the approximations to Peano curves for the case
$N=2$ are presented as an illustration.
Taking into account the initial mapping, one can conclude that current implementation of the
method allows to build up to $N(N-1)+1$ evolvents for mapping the $N$-dimensional domain
onto the corresponding one-dimensional intervals. Moreover, the additional constraint  $g_0(y)
\leq 0$ with $g_0(y)$ from (\ref{6_g0}), which arises in shifted evolvents, is absent. This
method for building a set of mappings can be ``scaled'' easily to obtain more evolvents (up to
$2^N$) if necessary.
\section{Sequential and Parallel Algorithm of Global Search}
The optimization methods applied in Globalizer to solve the reduced problem
(\ref{eq:oneDimTask}) are based on the MAGS method, which can be presented as follows ---
see \cite{strongin1978}, \cite{strSergGO}.
\par
The initial iteration of the algorithm is performed at an arbitrary point \mbox{\(x^1\in(0,1)\)}.
Then, let us suppose that \(k\), \(k\ge 1\), optimization iterations have been completed already.
The selection of the trial point \(x^{k+1}\) for the next iteration is performed according to the
following rules.

\textit{Rule 1}. Renumber the points of the preceding trials by the lower indices in order of
increasing value of coordinates
$0=x_0<x_1<...<x_{k+1}=1$.

\textit{Rule 2}. Compute the characteristics \(R(i)\) for each interval \((x_{i-1},x_i),1\leq i\leq
k+1\).

\textit{Rule 4}. Determine the interval with the maximum characteristic $R(t)=\max_{1\leq i
\leq k+1}R(i)$.

\textit{Rule 5}. Execute a new trial at the point \(x^{k+1}\) located within the interval with the
maximum characteristic from the previous step
  $x^{k+1}=d(x_t)$.

The stopping condition, which terminated the trials, is defined by the inequality
$\rho_t<\varepsilon$
for the interval with the maximum characteristic from Step 4 and \(\varepsilon >0\) is the
predefined
accuracy of the optimization problem solution. If the stopping condition is not satisfied,
the index \(k\) is incremented by 1, and the new global optimization iteration is executed.

The convergence conditions and exact formulas for descision rules $R(i)$ and $d(x)$ of the
described algorithm are given, for example, in \cite{strSergGO}.

\subsection{Parallel Algorithm Exploiting a Set of Evolvents}
\label{sec:parallel_evolvents}
Using the multiple mapping allows solving initial problem (\ref{eq:task}) by parallel solving the
problems
\[
\min\{\varphi(y^s(x)):x\in [0,1]\}, 1\leqslant s\leqslant S
\]
on a set of intervals $[0,1]$ by the index method. Each one-dimensional problem is solved on a
separate processor. The trial results at the point \(x^k\) obtained for the problem being solved by
particular processor are interpreted as the results of the trials in the rest problems (in the
corresponding points \(x^{k_1},\dots,x^{k_S})\). In this approach, a trial at the point \(x^k \in
[0,1]\) executed in the framework of the \(s\)-th problem, consists in the following sequence of
operations:
\par
1. Determine the image \(y^k=y^s (x^k)\) for the evolvent \(y^s (x)\).
\par
2. Inform the rest of processors about the start of the trial execution at the point \( y^k\) (the
blocking of the point \(y^k\) ).
\par
3. Determine the preimages \(x{}^{k_s}  \in [0,1], 1\leqslant s\leqslant S\), of the point \(y^k\) and interpret the
trial executed at the point \(y^k \in D \) as the execution of the trials in the \(S\) points
\(x{}^{k_1} ,\dots,x{}^{k_s} \)
\par
4. Inform the rest of processors about the trial results at the point \(y^k\).
\par
The decision rules for the mentioned parallel algorithm, in general, are the same as the rules of the
sequential algorithm (except the method of the trial execution). Each processor has its own copy
of the software realizing the computations of the problem functions and the decision rule of the
index algorithm. For the organization of the interactions among the processors, the queues are
created on each processor, where the processors store the information on the executed iterations
in the form of the tuples: the processor number \(s\), the trial point \(x{}^{k_s}\).
\par
The mentioned parallelization scheme is implemented in the Globalizer system with the use of MPI technology. Main
features of implementation consist in the following. A separate MPI-process is created for each
of \(S\) one-dimensional problems being solved, usually, one process per one processor
employed. Each process can use $p$ threads to parallel execute $p$ trials, usually one thread per an accessible core.

\section{Review of Other Methods}
\subsection{Sequential Methods}
\begin{itemize}
  \item Algorithm of global search (AGSl) (\url{https://github.com/sovrasov/ags_nlp_solver})
  \item Multi Level Single Linkage (MLSL) (\url{https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mlsl-multi-level-single-linkage})
  \item DIRECT (\url{https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#direct-and-direct-l})
  \item Locally-based DIRECT (DIRECT$l$) (\url{https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#direct-and-direct-l})
  \item Dual Simulated Annealing (\url{https://github.com/sgubianpm/sdaopt})
  \item Differential Evolution (\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution})
  \item Controlled Random Search (\url{https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#controlled-random-search-crs-with-local-mutation})
  \item StoGO (\url{https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#stogo})
\end{itemize}
\subsection{MIDACO Parallel Solver}
\section{Tools for Comparison of Global Optimization Algorithms}
\section{Results of Comparison}
\section{Conclusions}
\section*{Acknowledgements}
The study was supported by the Russian Science Foundation, project No 16-11-
10150

% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}

\end{document}
