%% Based on a TeXnicCenter-Template by Gyorgy SZEIDL.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%------------------------------------------------------------
%
\documentclass{article}%
%
\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{cancel}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel} % Русские и английские переносы

%-------------------------------------------

\begin{document}

\section*{Answers to the reviewers' comments
%for the paper ``Parallel global optimization algorithm with uniform convergence for solving a set of constrained global optimization problems''
}


We would like to express our deepest gratitute to all the reviewers for their useful comments and suggestions.
Please find below our answers and clarifications of the issues.

%\phantom{f}

\subsection*{Answers to Reviewer 1}

\subsubsection*{Major comments}

\begin{enumerate}
\item
First of all, this is the Nizhny Novgorod version of the English demonstrated by the authors. ... I recommend that the authors should seek professional help from experienced translators of mathematical texts.

\textbf{Answer}.
The paper was reviewed and corrected by an experienced translator. Some parts of the text were reformulated or rewritten to improve readability and correct grammatical mistakes. 

\item The principal objective of the paper, which is to solve the number (set) of nonconvex problems, is not presented conveniently in the manuscript. There is no mathematical formulation of the problem, say, in the form of an optimization problem, meanwhile the authors tell us about “the optimal way” of solving such a problem, without substantiating this view.

\textbf{Answer}.
Clarification of the objective of this work was added at the end of Section 2.

\item As well-known now, the solution of (1-3)-dimensional math programming problems, even nonconvex ones, has no considerable difficulties (say, with the help of B\&B methods) and, hence, it is not an attractive value for estimation of the power and effectiveness of algorithms.

\textbf{Answer}.
The target function and constraints in the considered tasks are assumed to be multiextreme and set as a "black box". In addition, we assumed that each trial (i.e. sequential calculation of constraints and target function values) is a labor-intensive operation and may require significant computational resources. Such setting is often encountered in solving applied problems (e.g., problems of nonlinear approximation; solving systems of nonlinear equations and inequalities; selecting parameters of mathematical models from experimental data, etc.). Therefore, even the problems of Lipschitz global optimization with only one variable still remain of interest to researchers [1,2], and problems of higher dimensionality (2-3) are considered standard for tests and comparison of algorithms of Lipschitz  global optimization [3].

We added a comment on the complexity of the considered class of problems (see ...), as well as numerical experiments on solving dimensionality problems 4 (see Section 4.1 and Table 1).

1. Calvin, J.M., Chen, Y., {\v Z}ilinskas, A. An Adaptive Univariate Global Optimization Algorithm and Its Convergence Rate for Twice Continuously Differentiable Functions. J. Optim. Theory Appl. 155, 628–636 (2012)

2. Sergeyev, Y.D., Mukhametzhanov, M.S., Kvasov, D.E. et al. Derivative-Free Local Tuning and Local Improvement Techniques Embedded in the Univariate Global Optimization. J. Optim. Theory Appl. 171, 186--208 (2016)

3. Paulavi{\v c}ius, R., {\v Z}ilinskas, J. Simplicial Global Optimization. Springer Briefs in Optimization. Springer, New York (2014)


\item There is no comparison of the computational results with popular approaches in the Global Optimization, such as Random Searches, Genetics Algorithms (for example), and other bioinitiated methods.
Additionally, we recommend that the authors pay attention to the DCA methods (by P.D.Tao and L.Th.An) which are able of producing a stationary (KKT-) vector and even a global solution (sometimes) to DC problems.

\textbf{Answer}.
The results of comparison of core Global Search Algorithm with a number of nature-inspired methods when solving several hundred test tasks are given in [1]. The results show that GSA provides better quality of problem solving (more problems are solved in fewer search trials). A corresponding comment has been added to the manuscript (see page 11).

Once again, we would like to point out that the article considers problems with "black box" functions (i.e., an analytical representation of the functions is not available). Therefore, comparing them with methods based on analytical representation of the task functions (for example, based on the ideas of DC-optimization, see [2] and references given therein) would not be appropriate. An relevant comment has been added to the manuscript (see page 2).


1. Sovrasov, V.  Comparison of Several Stochastic and Deterministic Derivative-Free Global Optimization Algorithms.  Lecture Notes in Computer Science. 11548, 70--81 (2019)

2. Pham Dinh T., Le Thi H.A. Recent Advances in DC Programming and DCA. Lecture Notes in Computer Science. 8342, 1--37 (2014)

\end{enumerate}


\subsubsection*{Minor comments}

\begin{enumerate}
\item p. 3. Apparently, $z_k\neq g_s(x_k ) =:z_{ks}$.
\textbf{Answer}.
This formula was rewritten.

\item  “To account for the latter” change for “On account of”.
\textbf{Answer}.
The phrase was replaced.

\item  p. 4. To put more clear the definition (DF) of $Q_i$ (poor grammar usage).
\textbf{Answer}.
The definition was added to the Section 3. See equation (5).

\item  Delete “At that” (???) globally.
\textbf{Answer}.
Deleted.

\item “At the point” put instead of “in the point”.

\textbf{Answer}.
Fixed.

\item  Specify the DF of the trial (what is it?) at the point $x$.

\textbf{Answer}.
The definition of the trial was clarified. See the description after the equation (6).

\item  Where is a DF of $\nu(x)$?

\textbf{Answer}.
See Section 3, equation (6).

\item  Is it relevant to use at Step 1 “juxtapose”? What does it mean?

\textbf{Answer}.
Here “juxtapose” means combine points and the corresponding results of trials into pairs.
The sentence was clarified.

\item  It seems that in Theorem 1 it is sufficient to assume that $D\neq\emptyset$ (I-st theorem of Weierstrass).

\textbf{Answer}.
The original formulation of the theorem was made for the interval [0,1], this assumption is missing indeed.

\item  Delete “Beginning with some step”, put simply “IF …”

\textbf{Answer}.
Deleted.

\item  Change “admissible” for “feasible”.

\textbf{Answer}.
Done.

\item  The equalities (12) are wrong.

\textbf{Answer}.
The $\inf$ operator is missing here. It was added. See equation (14).

\item  Delete “where $y_*$ is from (3)”.

\textbf{Answer}.
Deleted.

\item  Completely reformulate Theorem 2.

\textbf{Answer}.
The theorem was reformulated using new aliases for algorithms.

\item  Before Table 1 the numbers (?) $p, S_i, S_t$ are not correctly defined.

\textbf{Answer}.
The definitions were added to the Section 4.1.

\item  Delete two last propositions before Table 1.

\textbf{Answer}.

\item  Completely rewrite the conclusion. As it is, it is not written in English.

\textbf{Answer}.
Conclusion was rewritten.

\item  The English and the terminology must be similar to [6] and [16].

\textbf{Answer}.
Definitions and terminology in the Section 3 were corrected.

\end{enumerate}



\subsection*{Answers to Reviewer 2}

\textbf{Comment}. Is it possible to find different solutions by the suggested technique, please, give some comments.

\textbf{Answer}.
The presented method converges to all the global optimal solutions at once, but numerically it's hard
to obtain them all with a guarentee. That happens due to lack of an appropriate stopping criterion.


\subsection*{Answers to Reviewer 3}

\textbf{Comment}. It's not clear why solving a series of global optimization problems in parallel is an important problem. From my experience such problem statement sounds artificial.

\textbf{Answer}.

A series of global optimization problems can arise from an mixed-integer problem or
after scalarization [1] of an multi-objective problem [2]. In both cases, all the problems from the series have to be solved with approximately similar quality, because all of
them are equally important. To speedup this process under constrained computational resources,
we can apply parallel computations along with a resource balancing approach to guarantee equal
quality of solutions in all the problems by the end of the optimization process.

1. Barkalov, K. and I. Lebedev. “Parallel Global Optimization for Non-convex Mixed-Integer Problems.” RuSCDays (2019).

2. Jahn J. Scalarization in Multi Objective Optimization. In: Serafini P. (eds) Mathematics of Multi Objective Optimization. International Centre for Mechanical Sciences (Courses and Lectures), vol 289. Springer, Vienna (1985).

\end{document}
