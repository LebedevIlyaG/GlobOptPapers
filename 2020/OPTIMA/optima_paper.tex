% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}

%custom packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{subfig}
\usepackage{breqn}
\usepackage[labelfont=bf,labelsep=period]{caption}

\usepackage[utf8]{inputenc}
\inputencoding{utf8}
\usepackage[english]{babel}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Parallel global optimization algorithm with uniform convergence
for solving a set of constrained global optimization problems\thanks{This study was supported
by the Russian Science Foundation, project No.\,16-11-10150.}}
%
\titlerunning{Parallel global optimization algorithm with uniform convergence}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Vladislav Sovrasov\and
Konstantin Barkalov
}
%
\authorrunning{V. Sovrasov et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia
\email{sovrasov.vladislav@itmm.unn.ru},
\email{konstantin.barkalov@itmm.unn.ru}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
This paper proposes an extension of global optimization algorithm for solving a set of problems
with non-convex constraints. The uniform convergence inherent to the algorithm allows for the optimal
distribution of the computational resources, since the number of errors in numerical solutions
decreases at approximately equal rate for all problems processed by the algorithm. The algorithm assigns
a priority level to each problem and, with every iteration, performs the computations of the objective
functions for several problems in parallel. After the algorithm is terminated at any given time,
it obtains solutions of similar quality for all the problems of the set. Such sets appear when
the global optimization problem has a discrete parameter or, for example, when a multicriteria optimization
problem is solved by scalarization techniques. The algorithm employs Peano-type space-filling curves
for the reduction of the multidimensional optimization problems to the one-dimensional ones.
The efficiency of the implemented algorithm was tested using the sets of artificially generated
global optimization problems, as well as a series of problems obtained as a result of scalarization
of a multicriteria optimization problem. Additional numerical experiments also confirmed
the uniform convergence of the proposed method.

\keywords{Global optimization \and Non-convex constraints \and Uniform convergence \and Derivative-free optimization
\and Parallel computing.}
\end{abstract}
%
%
%

\section{Introduction}
\label{sec-intro}
Global optimization problems with non-convex constraints are considered as one of the most difficult
optimization problems.
Finding of the global minimum of a function with several variables often appears to be
more difficult than a local optimization in a thousand-dimensional space.
The simplest gradient descent method could be sufficient for the latter, however,
to \textit{guarantee} that the global optimum is found, the optimization
methods have to accumulate the information on the behavior of the objective function
in the entire search domain  \cite{Jones2009,Paulavicius2011,Evtushenko2013,Strongin2000}.
In some cases, for instance, for DC or QP problems, more effective methods and even
analytical optimality conditions could be applied  \cite{PhamDinh2014, qpBook},
but these methods are barely applicable for black box functions (i.e., an analytical
representation of the functions is not available).

Solving a series of such problems with limited computational resources is even more
difficult: besides the search of the global extremum, the computational resources
have to be distributed in such a way that the location of the global extremum would be
estimated with approximately equal quality for all problems being solved at the same time.
Usually, a series of  \(q\) problems is solved either sequentially or in parallel by sets of  \(p\ll
q\) problems where \(p\) is the number of parallel computational devices.
Such an approach creates a situation when at every given moment there exist problems for which
the global optimum is not obtained yet, while the optimum for the problems from the beginning of
the set may be estimated even with excess precision.

This paper considers a parallel global optimization method developed at the Lobachevsky
State University of Nizhny Novgorod for simultaneous solving of a set of problems \cite{BarkalovStrongin2018}
and in particular, its generalization to cover the non-convex constraints. The index scheme \cite{Strongin2000,Pugliese}
which is applied to account for the constraints, allows operating with partially defined objective functions and constraints, and its efficiency is on par with similar approaches \cite{BarkalovLebedev2017}.
The convergence of the algorithm to the global optimizers in all problems was proved theoretically,
the efficiency of the implemented algorithm was demonstrated on sets of problems generated
by a special mechanism. This mechanism generates sets of problems with predefined dimensionality
and predefined number of non-convex constraints  \cite{GergelBarkalov2019}.
Besides the artificially generated problems, the method was also tested on sets of
multicriteria optimization prob¬lems with nonlinear constraints solved by
criteria convolution method  \cite{Ehrgott2005}.

The paper is structured as follows. Section \ref{sec:problem} provides the statement of the problem.
Section \ref{sec:method} gives a description of the parallel optimization method.
Subsection \ref{sec:conv_method} describes the sufficient conditions of convergence
of the considered method.
The results of the numerical experiments confirming the efficiency of considered method are
presented in Section \ref{sec:exps}.
Conclusion gives a short summary of the results of the study and proposes the
directions for further improvement of the software implementation of the considered method.


\section{Statement of the global optimization problem}
\label{sec:problem}
This paper considers the following problem:
to find the global minimum of an  \(N\)-dimensional function \(\varphi(y)\) in
a hyperinterval $D$
\begin{equation}
\label{eq:task}
\varphi(y^*)=\min\{\varphi(y):y\in D\},
\end{equation}
\begin{displaymath}
D=\{y\in \mathbf{R}^N:a_i\leqslant x_i\leqslant{b_i}, 1\leqslant{i}\leqslant{N}\}.
\end{displaymath}
In order to produce an estimate of the global minimum based on a finite number of
computations of the function values, the rate of variation of \(\varphi(y)\) in \(D\) has to
be limited.
As a rule, the Lipschitz condition
\begin{displaymath}
\label{lip}
|\varphi(y_1)-\varphi(y_2)|\leqslant L\Vert y_1-y_2\Vert,y_1,y_2\in D,0<L<\infty,
\end{displaymath}
is accepted as such a limitation.

There are various methods for solving the considered multidimensional problem directly
\cite{SergeyevKvasov2017, Jones2009} as well as efficient methods for solving the univariate problems \cite{Norkin1992, Strongin2000}.
This paper considers a one-dimensional method, which is applied jointly with
the dimensionality reduction scheme.
The use of space¬filling curve (or \textit{evolvent}) $y(x)$, where
\begin{equation}
\label{cube}
\lbrace y\in \mathbf{R}^N:-2^{-1}\leqslant y_i\leqslant 2^{-1},1\leqslant i\leqslant
N\rbrace=\{y(x):0\leqslant x\leqslant 1\},
\end{equation}
 is a well-known scheme of dimensionality reduction
 of the initial problem for the global optimization algorithms
\cite{Sergeyev2013}.
A mapping of the type (\ref{cube}) allows reducing a multidimensional problem to a univariate one at the expense of worsening its properties. In particular, the
one-dimensional function \(\varphi(y(x))\) is not a Lipschitz function but a Holder one:
\begin{displaymath}
\label{holder}
|\varphi(y(x_1))-\varphi(y(x_2))|\leqslant H{|x_1-x_2|}^{\frac{1}{N}}, \; x_1,x_2\in[0,1],
\end{displaymath}
where the H\"{o}lder constant \(H\) is related to the Lipschitz constant \(L\) as
\begin{displaymath}
  H=2L\sqrt{N+3}.
\end{displaymath}

The feasible domain can also be defined by the constraints that essentially complicate the problem.
The problem statement in this case will take the following form:
\begin{equation}
  \label{eq:constrained_problem}
  \varphi(y^*)=\min\{\varphi(y):y\in G\},\:G=D \cap \{y: g_j(y)\leqslant 0, 1\leqslant j\leqslant m\}.
\end{equation}

Let us set \(g_{m+1}(y)=\varphi(y)\). Hereafter, we shall assume all functions
\(g_k(y),1\leqslant k \leqslant m+1\),
to satisfy the Lipschitz condition on the hyperinterval \(D\).

Further, let us consider solving a series of \(q\) problems of the kind
(\ref{eq:constrained_problem}):
\begin{equation}
  \label{eq:many_problems}
  \min\left\{\varphi_1(y), y\in G_1 \right\}, \min\left\{\varphi_2(y), y\in G_2\right\},...,
\min\left\{\varphi_q(y), y\in G_q\right\}.
\end{equation}

Similar to \cite{BarkalovStrongin2018}, this work aims to formulate a method that will ensure a uniform convergence of the
solutions of all problems in the series. Here and below, a uniform convergence implies a proportional decrease
of the distance between the best iteration point and the
global optimizer in all the problems in the series:
\begin{equation}
  \exists \varepsilon > 0: \forall s>1, \forall i,j\in\{1,\dots,q\}\;
    \frac{\Vert \tilde{y_i}(s)^* - y_i*\Vert_\infty}{\Vert \tilde{y_j}(s)^* - y^*_j\Vert_	\infty} \leqslant \varepsilon,
\end{equation}
where \(s\) is the number of steps of the optimization method, \(\tilde{y_i}(s)^*\) is the best iteration point
in the problem \(i\) from the set (\ref{eq:many_problems}) at the step \(s\).

\section{Description of the global optimization method}
\label{sec:method}

Taking into account the dimensionality reduction scheme (\ref{cube}), let us assume that the method
requires finding the global minimum of the function \(\varphi(x),
x\in[0,1]\), which satisfies the H\"{o}lder condition with the constraints \(g_j(x)\), which in turn satisfy
this condition in the interval \([0,1]\).

The index algorithm of global search (IAGS) for solving one-dimensional problems
%(\ref{eq:constrained_problem})
considered here implies construction of a sequence of points
\(x_k\), at which the values of index function \(z_k\) are calculated.
The index scheme \cite{Strongin2000} was used to account for the latter.
Let us assume that each function \(g_i(x), 1 \leqslant i \leqslant m + 1\), is defined and computable
only in the corresponding subrange \(Q_i \in [a, b]\), where \(\varphi(x)\) is denoted as \(g_{m+1}(x)\) and
\begin{equation}
  \label{eq:q}
  Q_1=[0,1],\: Q_{i+1}=\left\{x\in Q_i:g_{i}(x)\leqslant 0\right\},\:1 \leqslant i \leqslant m, \: Q_{m+2}=\emptyset
\end{equation}
Considering the definition (\ref{eq:q}) the initial problem can be rewritten as
\begin{displaymath}
  \varphi(x^*)=\min\{g_{m+1}(x):x\in Q_{m+1}\}.
\end{displaymath}

The index scheme assigns a number called index \(\nu(x)\) to each point of the search sequence \(x_k\):
\begin{equation}
  \nu(x) = i:x\in Q_i, x\not\in Q_{i+1},\:1\leqslant i\leqslant m + 1.
\end{equation}

In order to obtain the index of the point \(x\) we have to perform a trial defined via the following steps:
\begin{enumerate}
  \item Sequentially  compute functions \(g_i(x),1 \leqslant i \leqslant m\) until for some \(i^*\) \(g_{i^*}(x)>0\) or until \(i=m+1\).
  Set \(\nu(x)=i^*\) or \(\nu(x)=m+1\) if \(g_i(x) \leqslant 0, 1 \leqslant i \leqslant m\).
  Therefore, \(\nu(x)\) is the number of the first violated constraint at the point \(x\) or \(m+1\) if
  all the constraints are satisfied at \(x\).
  \item Return pair \(\nu(x), z=g_{\nu(x)}(x)\) as a result of the trial.
\end{enumerate}

This approach to trials allows reducing the initial problem with functional
constraints to an unconstrained problem of minimization of a discontinuous function:
\begin{displaymath}
  \begin{array}{lr}
    \psi (x^{*})=\min_{x\in [0,1]}\psi (x), \\
    \psi (x)={\begin{cases}g_{\nu }(x)/H_{\nu },&\nu <M,\\(g_{M}(x)-g_{M}^{*})/H_{M},&\nu
=M.\end{cases}}
  \end{array}
\end{displaymath}
Here \(M=\max_{}^{}\left\{\nu (x):x\in [0,1]\right\}\) and \(g_{M}^{*}=\min
_{}^{}\left\{g_{M}(x):x\in Q_{M}\right\}\).
Because of the definition of the number \(M\), the problem of finding \(g_{M}^{*}\) always
has a solution. If \(M=m+1\), then \(g_{M}^{*}=\varphi(x^{*})\).
The function \(\psi (x)\) satisfies the H\"{o}lder condition on the set
\(Q_1\) with the constant 1, and \(\psi (x)\) can have jump discontinuities at the boundaries
of the sets \(Q_i,\:1\leqslant i \leqslant m+1\).
Despite the values of the H\"{o}lder constants \(H_k\) and the value \(g_{M}^{*}\) are not
known in advance, they can be estimated in the course of solving the problem.
A set of triples \(\{(x_k,\nu_k,z_k)\}, 1\leqslant k\leqslant n\), constitutes the search information
accumulated by the method upon execution of \(n\) steps.

At the first iteration of the method, the trial is performed in the arbitrary internal point \(x_1\) of
the interval \([0,1]\).
The indices of the points 0 and 1 are considered to be zero indices, the values \(z\) at these points
are undefined.
Let us assume that \(k\geqslant 1\) iterations of the method have been performed. In the course of this performance the
trials were conducted at \(k\) points \(x_i, 1\leqslant i\leqslant k\).
Then, the points \(x^{k+1}\) of the search trials of the next \((k+1)^{\rm th}\) iteration are
defined according to the rules:

Step 1. Reassign lower indices to the points of the set \(X_k=\{x^1,\dotsc,x^k\}\cup\{0\}\cup\{1\}\), which
includes the boundary points of the interval \([0,1]\) as well as the points of preceding trials in the order of increasing coordinate values, i.e.
\begin{equation}
  \label{eq:points}
  0=x_0<x_1<\dotsc<x_{k+1}=1,
\end{equation}
and compare them with the values \(z_{i}=g_{\nu }(x_{i}),\nu =\nu (x_{i}),i={\overline {1,k}}\) computed at these points.

Step 2. For each integer number \(\nu ,1\leqslant \nu \leqslant m+1\), determine the corresponding
set \(I_{\nu }\) of the lower indices of the points, at which the values of the functions \(g_{\nu
}(x)\) are computed:
\begin{displaymath}
  I_{\nu }=\{i:\nu (x_{i})=\nu ,1\leqslant i\leqslant k\},1\leq \nu \leqslant m+1,
\end{displaymath}
and determine the maximum value of the index \(M=\max\{\nu (x_{i}),1\leq i\leq k\}\).

Step 3. Compute current estimate for the unknown H\"{o}lder constant:
\begin{equation}
  \label{step2}
  \mu _{\nu }=\max \left\{ \frac{|g_{\nu }(x_{i})-g_{\nu }(x_{j})|}{(x_{i}-
x_{j})^{\frac{1}{N}}}:i,j\in I_{\nu },i>j \right\}.
\end{equation}
If the set \(I_{\nu }\) contains less than two elements or if the value \(\mu _{\nu }\) is equal to zero, then assume \(\mu _{\nu }=1\).

Step 4. For all the nonempty sets \(I_{\nu },\nu ={\overline {1,M}}\) compute the estimates
\begin{equation}
  \label{eq:step_4}
  z_{\nu }^{*}={\begin{cases}\min\{g_{\nu }(x_{i}):x_{i}\in I_{\nu }\},&\nu =M,\\
  0,&\nu <M.\end{cases}}
\end{equation}

Step 5. For each interval \((x_{i-1},x_{i}),1\leqslant i\leqslant k\) compute the characteristic
\begin{equation}
  \label{step3_1}
  R(i)={\begin{cases}\Delta _{i}+{\frac {(z_{i}-z_{i-1})^{2}}{(r_{\nu }\mu _{\nu
})^{2}\Delta _{i}}}-2{\frac {z_{i}+z_{i-1}-2z_{\nu }^{*}}{r_{\nu }\mu _{\nu }}},&\nu =\nu
(x_{i})=\nu (x_{i-1}),\\2\Delta _{i}-4{\frac {z_{i-1}-z_{\nu }^{*}}{r_{\nu }\mu _{\nu
}}},&\nu =\nu (x_{i-1})>\nu (x_{i}),\\2\Delta _{i}-4{\frac {z_{i}-z_{\nu }^{*}}{r_{\nu }\mu
_{\nu }}},&\nu =\nu (x_{i})>\nu (x_{i-1}),\end{cases}}
\end{equation}
where \(\Delta_{i}=(x_{i}-x_{i-1})^{\frac{1}{N}}\).
The values \(r_{\nu }>1,\nu ={\overline {1,m}}\) are the parameters of the algorithm.
They define the products \(r_{\nu}\mu_{\nu}\) used in computing the characteristics as the estimates of the
unknown H\"{o}lder constants.

Step 6. Select the maximum characteristic:
\begin{equation}
\label{step4}
t=\arg \max_{1\leqslant i \leqslant k+1}R(i).
\end{equation}

Step 7. Perform the next trial in the middle of the interval \((x_{t-1},x_{t})\) if the indices of its
end points are not the same: \(x^{k+1}={\frac {1}{2}}(x_{t}+x_{t-1})\).
Otherwise, perform the trial at the point
\begin{displaymath}
  x^{k+1}={\frac {1}{2}}(x_{t}+x_{t-1})-\operatorname {sgn}(z_{t}-z_{t-1}){\frac {|z_{t}-
z_{t-1}|^{N}}{2r_{\nu }\mu _{\nu }^{N}}},\nu =\nu (x_{t})=\nu (x_{t-1}),
\end{displaymath}
and then increment \(k\) by 1.

The algorithm stops if the condition \(\Delta_{t}\leqslant \varepsilon\) is satisfied. Here
\(\varepsilon>0\) is a predefined precision.
The values
\begin{equation}
\varphi_k^*=\min_{1\leqslant i \leqslant k}\varphi(x_i), \; x_k^*=\arg \min_{1\leqslant i \leqslant
k}\varphi(x_i)
\end{equation}
are assumed as the estimates of the global solution.

Next, following the approach described in \cite{BarkalovStrongin2018}, we shall use \(q\) copies
of IAGS working synchronously to solve the problem series (\ref{eq:many_problems}).
The only difference is that when selecting the interval with the maximum characteristic at Step
6, the choice will be made from all intervals, which were generated by \(q\) copies of IAGS.
If the maximum characteristic corresponds to the \(i^{\rm th}\) problem, then Step 7 is
performed in the \(i^{\rm th}\) copy of the method while the other copies stay idle.
This way, at every iteration the trial is performed for the most promising problem in terms of the characteristics (\ref{step3_1}). This allows distributing the resources of the
method among the problems dynamically. Let's denote this method as MIAGS.

The parallel modification of the method does not differ from the one considered in
\cite{BarkalovStrongin2018} and consists in the selection of \(p\) intervals at Step 6 and
performing  \(p\) trials in parallel at the following step.
All resources of the method within the framework of the iteration may be focused on
a single problem as well as on \(l\leqslant p\) problems simultaneously (depending on the problem,
to which the intervals selected by the method belong).

\subsection{Convergence conditions }
\label{sec:conv_method}

The conditions of convergence of the method described in Section \ref{sec:method} in case of \(q=1\) are given in \cite{Strongin2000}.
\begin{theorem} (Sufficient convergence conditions)
  \label{th:single_conv}
  Let us assume that the following conditions are true:
  \begin{enumerate}
    \item \(D\ne\emptyset\), the problem (\ref{eq:constrained_problem}) has a solution.
    \item Functions \(g_j(y)\leqslant 0, 1\leqslant j\leqslant m + 1\), are Lipschitz functions with
respective constants \(L_i\) over the domain \(D\) (here \(g_{m+1}(y)=\varphi(y)\)).
    \item If \(k\) from (\ref{eq:points}) is sufficiently large,
    the values \(\mu_\nu\) from (\ref{step2}) satisfy the inequalities
    \begin{equation}
      r_\nu\mu_\nu > 2^{3-1/N}L_\nu \sqrt{N+3},\: 1\leqslant \nu \leqslant m + 1.
    \end{equation}
  \end{enumerate}
  Then any limit point \(\overline{y}\) of the sequence \(\{y_k\} = \{y(x_k)\}\) generated
  by the index method, the problem
  (\ref{eq:constrained_problem}) is feasible and satisfies the conditions
\begin{equation}
  \label{eq:conv_cond}
  \varphi(\overline{y})=\inf\{ \varphi(y^k): g_i(y^k)\leqslant 0,1\leqslant i\leqslant m, k=1,2,\dots\}=\varphi(y^*).
\end{equation}
\end{theorem}

\begin{remark}
  \label{rem:r1}
  From the relationship between the H\"{o}lder and Lipschitz constants and from the condition (\ref{eq:conv_cond}) it follows
  that the parameters \(r_\nu\) from (\ref{step3_1}) should satisfy the condition
  \begin{equation}
    r_\nu > 2^{2 - 1/N}.
  \end{equation}
\end{remark}

\begin{theorem} (On the convergence conditions of MIAGS) Let the conditions 1-3 of the Theorem \ref{th:single_conv} for each problem \(i,\:1\leqslant i\leqslant q\) from (\ref{eq:many_problems}), be true
i.e. each problem can be solved by IAGS.
  Then solving all of the \(q\) problems by MIAGS will
  generate \(q\) infinite sequences \(\{y^k_i\},\:1\leqslant i\leqslant q\), such that
  \begin{displaymath}
    \varphi_i(\overline{y_i})=\inf\{ \varphi(y^k_i): g^i_j(y^k_i)\leqslant 0,1\leqslant j\leqslant m_i, k=1,2,\dots\}=\varphi_i(y^*_i).
  \end{displaymath}
\end{theorem}
\begin{proof}
  Let us consider two random problems from the set (\ref{eq:many_problems})
  \begin{equation}
      \begin{array}{lr}
        \min\{\varphi(y):y\in D_1,\: g_j^\varphi(y)\leqslant 0, 1\leqslant j\leqslant m_1\}, \\
        \min\{\psi(y):y\in D_2,\: g_j^\psi(y)\leqslant 0, 1\leqslant j\leqslant m_2\}.
      \end{array}
  \end{equation}
  Let us denote the characteristics (\ref{step3_1}) for the first problem as \(R_\varphi(i)\)
  and for the second problem as \(R_\psi(j)\). Considering that, we have:
  \begin{equation}
      \begin{array}{lr}
        R_\varphi(t_\varphi)=\max_{1\leqslant i\leqslant k}R_\varphi(i), \\
        R_\psi(t_\psi)=\max_{1\leqslant j\leqslant s}R_\psi(j),
      \end{array}
  \end{equation}
  where \(k\) corresponds to the number of trials in the first problem and \(s\)
  corresponds to the number of trials in the second problem. The sequence of trials \(\{v^k\}\)
  corresponds to the first problem and the sequence of trials \(\{u^s\}\)
  corresponds to the second problem. The values \(z^k=g^\varphi_\nu(v_k),\nu =\nu (v_{k})\) correspond to the
  trial points \(\{v^k\}\), and the values \(w^s=g^\psi_\nu(u_s),\nu =\nu(u_{s})\) correspond to the
  trial points \(\{u^s\}\).

  When the two problems are solved simultaneously, the algorithm selects
  an interval for the next trial according to the condition:
  \begin{equation}
    R(t) = \max\{R_\varphi(i),R_\psi(j)\}.
  \end{equation}

  Let the algorithm solve two problems and the iterations counter be \(l = k + s, l=0,1,2,\dots\).
  Then, since Theorem \ref{th:single_conv} is true for all the problems , at least one of the sequences
  \(\{v^k\}\) and \(\{u^s\}\) will be infinite (let it be \(\{v^k\}\)). If we can prove that both   sequences are infinite, this will demonstrate the convergence in both considered problems.

  Let us take a limit point \(\overline{v}\in [v_{i-1},v_i]\), where \(i=i(k)\). The indices \(v_{i-1}\;,v_i\)
  can be equal or different, but, because of convergence, if \(k\) is large enough they will be stable.
  %will be steady  equal to \(m_1+1\) (if \(\overline{v}\) is
  %an interior point of the feasible domain produced by constraints \(g^\varphi_p(y),p=\overline{1,m_1}\))
  %or \(\nu(v_{i-1})\ne\nu(v_i)\) (if \(\exists p: 1\leqslant p\leqslant m_1, g^\varphi_p(y(\overline{v}))=0\)).
  In the first case the algorithm will use the first branch of the rule (\ref{step3_1}), otherwise it will use
  one of the two other branches.

  Let us consider the first case: from (\ref{step2})
  \begin{displaymath}
    \frac{|z_i-z_{i-1}|}{\Delta_i} \leqslant \mu_{\varphi,\nu}.
  \end{displaymath}
  Having considered that we can establish an upper bound:
  \begin{displaymath}
    \frac{(z_i-z_{i-1})^2}{r_\nu^2\mu_{\varphi,\nu}^2\Delta_i}=\frac{(z_i-z_{i-1})^2\Delta_i}{(r_\nu\mu_{\varphi,\nu}\Delta_i)^2}
    \leqslant \frac{\Delta_i}{r_\nu^2}.
  \end{displaymath}
  Therefore using the first branch of form rule (\ref{step3_1}) we get an inequality
  \begin{equation}
    \label{eq:th1}
    R_\varphi(i)\leqslant\Delta_i(1 + \frac{1}{r_\nu^2}) - \frac{2(z_i+z_{i-1}-2z^*_\nu)}{r_\nu\mu_{\varphi,\nu}}.
  \end{equation}
  Because \(\overline{v}\) is the limit point of the sequence \(\{v^k\}\) and \(\varphi(y(\overline{v}))\leqslant z^*_{\nu}, \nu=m+1\) or
  \(z^*_\nu=0, \nu<m+1\) and \(z_{i-1},z_i\to 0\) at \(k\to\infty\):
  \begin{equation}
    \label{eq:th2}
    \Delta_i\to 0, z_i+z_{i-1} - 2 z_\nu^*\to 0.
  \end{equation}
  In the second case (when one of the other two branches of the rule (\ref{step3_1}) is applied) we have
  \begin{displaymath}
    R_\varphi(i)=2\Delta_i - 4\frac{z_i-z^*_\nu}{r_\nu\mu_{\varphi,\nu}}.
  \end{displaymath}
  If \(z^*_\nu \ne 0\) then \(z_i-z^*_\nu \geqslant 0\) and
  \begin{equation}
    \label{eq:th3}
    R_\varphi(i)=2\Delta_i - 4\frac{z_i-z^*_\nu}{r_\nu\mu_{\varphi,\nu}} \leqslant 2\Delta_i.
  \end{equation}
  Otherwise because  \(\overline{v}\) is a feasible point \(z_i\to 0\) at \(k\to\infty\).

  From (\ref{eq:th1}) (\ref{eq:th2}) (\ref{eq:th3}) for any small \(\delta > 0\) there exists a large value of \(k\) such that
  \begin{equation}
    R_\varphi(i)\leqslant \delta.
    \label{eq:th6}
  \end{equation}

  Let \(\alpha = \max\{\nu(u):u\in\{u^s\}\}\). Because \(\alpha\) is currently the highest index
  in the search sequence \(\{u^s\}\) and according to the rule (\(\ref{eq:step_4}\)), \(\exists j: w^*_\alpha=w_j\).

  If \(\nu(w_{j-1})=\nu(w_{j})=\alpha\) then the first branch of the rule (\ref{step3_1}) is applied and
  \begin{equation}
    \begin{array}{l}
      R_\psi(j)=\Delta_j + \frac{(w_j-w_{j-1})^2}{r_\alpha^2\mu_{\psi,\alpha}^2\Delta_j}
        - \frac{2(w_j+w_{j-1}-2w^*_\alpha)}{r_\alpha\mu_{\psi,\alpha}} \geqslant \\
        \geqslant\Delta_j - \frac{2(w_j+w_{j-1}-2w^*_\alpha)}{r_\alpha\mu_{\psi,\alpha}} =
        \Delta_j - \frac{2\Delta_j(w_{j-1}-w_j)}{r_\alpha\mu_{\psi,\alpha}\Delta_j} \geqslant\\
        \geqslant \Delta_j - \frac{2\Delta_j}{r_\alpha} = \Delta_j\left(1-\frac{2}{r_\alpha}\right).
      \end{array}
    \label{eq:th4}
  \end{equation}

  If \(\nu(w_{j-1})\ne\nu(w_{j})=\alpha\) then \(\nu(w_{j-1})<\alpha\) and the third branch of
  the rule (\ref{step3_1}) is applied:
  \begin{equation}
    R_\psi(j)=2\Delta_j - 4 \frac{w_j-w^*_{\alpha}}{r_\alpha \mu_{\psi,\alpha}}=2\Delta_j > 0.
    \label{eq:th5}
  \end{equation}
  Taking into account the Remark \ref{rem:r1}, (\ref{eq:th4}), (\ref{eq:th5}) we can conclude that such an interval
  exists that \(R_\psi(j)>0\). At the same time, (\ref{eq:th6}) is true and
  the inequality
  \begin{displaymath}
    R_\psi(j) > R_\varphi(i)
  \end{displaymath}
  will be true when the value \(k\) is large enough. Thus the next scheduled iteration is performed for
  the second problem with objective \(\psi(y)\), i.e. sequence \(\{v^s\}\) will be infinite as well.

  Since we considered two arbitrary problems from the given set of \(q\) problems, the theorem is
  true for any pair of problems from the set. By induction, the theorem is also true for the whole
  set.
\qed
\end{proof}

\section{Results of numerical experiments}
\label{sec:exps}

The use of set of test problems with known solutions generated by a random mechanisms is
one of the commonly accepted approaches to the comparison of optimization algorithms
\cite{Beiranvand2017}.
Experiments presented in this paper were based on two generators of problems of different nature
\cite{grishaginClass, Gaviano2003} were used.
These generators produce problems without nonlinear constraints. Therefore, the
GCGen\footnote{The original code of the system can be accessed at https://github.com/UNN-
ITMM-Software/GCGen} \cite{GergelBarkalov2019} system was to be used to supplement these generators.
This system allows generating problems with constraints based on arbitrary nonlinear
functions.

The GCGen system comes with the examples of its application and construction of sets of
problems each consisting of an objective function and two constraints generated by \(F_{GR}\)
\cite{grishaginClass} or GKLS \cite{Gaviano2003} generator.
GKLS \cite{Gaviano2003} allows obtaining the functions of predefined dimensionality and
with predefined number of global optimums.
In combination with GCGen, the following sets of problems were generated:
\begin{itemize}
  \item 100 2-dimensional problems with two constraints;
  \item 100 3-dimensional problems with two constraints
  \item 20 4-dimensional problems with two constraints;
  \item a mixed class consisting of 50 problems with
  two-dimensional GKLS functions and 50 problems with \(F_{GR}\).
  This set is generated in order to demonstrate that the efficiency of the method is preserved at essentially
  varying properties of the problems.
\end{itemize}

Examples of contour plots of the considered problems are presented in Fig. \ref{fig:isolines}.
The feasible domain is highlighted.

\begin{figure}[ht]
    \centering
    \subfloat[Solution inside feasible domain]{
    {\includegraphics[width=.5\textwidth]{pic/2.png}}}
    \subfloat[Solution at the boundary of feasible area]{
    {\includegraphics[width=.5\textwidth]{pic/4.png}}}
    \caption{Contour plots and trial points of IAGS in two synthetic problems}
    \label{fig:isolines}
\end{figure}

A test problem was considered to be solved if the optimization method executes the next trial
\(y^k\) in the \(\delta\)-vicinity of the global minimizer \(y^*\), i.e.
\(\left\|y^k-y^*\right\|\leqslant \delta = 0.01\left\|b-a\right\|\),
where \(a\) and \(b\) are the left and right boundaries of the hypercube from (\ref{eq:task}).
If this relation is not fulfilled after the limit of number of iterations had been reached, the problem was
considered to be not solved.

When evaluating the quality of the method and its implementation, besides increased
computation speed due to parallelization,we shall also account for the mean maximum
distance (in terms of \(l_{\infty}\)-norm) from the current estimate of the
optimum to its actual position computed on the set of problems (\ref{eq:many_problems}):
\(D_{avg}\) and \(D_{max}\).
The dynamics of these magnitudes in the course of the optimization shows how uniformly the
method distributes the resources among the problems.

The parallel method was implemented in  C++  with the use OpenMP technology for
parallelization of the trial execution process in the shared memory.
All numerical experiments were carried out using a computer with the following configuration:
Intel Core i7-7800X, 64GB RAM, Ubuntu 16.04 OS, GCC 5.5 compiler.

\subsection{Solution results of generated problems}

The solution results of test problems by the sequential and parallel versions of the
MIAGS are presented in Table~\ref{tab:speedup}.
For all two-dimensional classes of problems, the parameter \(r=4.7\).
In the case of the three and four-dimensional problems, \(r=4.7\).  The convergence speed
was improved by applying
\(\varepsilon\)-reservation technique from \cite{Strongin2000} Chapter 8.3 with
\(\varepsilon=0.1\).
In all the experiments, an additional computational load was introduced into the objective function
and constraints to get the duration of a single call of a problem function equal to
approximately 1 ms.

Table \ref{tab:speedup} shows that the speedup in the iterations \(S_i=\frac{iters(p=1)}{iters(p=i)}\) increased
linearly with increasing number of threads \(p\), whereas the speed of calculations \(S_t=\frac{time(p=1)}{time(p=i)}\) increased
at a lower rate, which points to a non-ideal implementation of the algorithm.
The actual speedup, the upper limit for which is \(S_i\), can be increased by the optimization
of the interaction between the copies of IAGS. We plan to test this approach in our future works.

\begin{table}
  \centering
  \caption{Results of experiments on the sets of synthetic problems}
  \label{tab:speedup}
  \begin{tabular}{c|c|cccc}
    %\cline{1-8}\noalign{\smallskip}
    Problem class & \textit{p} & Number of iterations & Time, s & \(S_i\) & \(S_t\)   \\
    %s\noalign{\smallskip} \cline{4-5} \cline{7-8}  \noalign{\smallskip}
    \hline
    GKLS \& \(F_{GR}\) based \
      & 1 & 51434 & 90.20 & -    & - \\
      & 2 & 25698 & 56.96 & 2.00 & 1.58 \\
      & 4 & 13015 & 36.67 & 3.95 & 2.46 \\
      & 6 & 8332  & 26.85 & 6.17 & 3.36 \\
    \hline
    GKLS based 2d \
      & 1 & 59066 & 97.53 & -    & - \\
      & 2 & 29060 & 60.56 & 2.04 & 1.61 \\
      & 4 & 14266 & 38.92 & 4.14 & 2.51 \\
      & 6 & 9436  & 29.53 & 6.26 & 3.30 \\
    \hline
    GKLS based 3d \
      & 1 & 782544 & 1117.55 & -    & - \\
      & 2 & 397565 & 752.92  & 1.97 & 1.48 \\
      & 4 & 208073 & 526.67  & 3.76 & 2.12 \\
      & 6 & 142089 & 445.45  & 5.50 & 2.51 \\
    \hline
    GKLS based 4d \
      & 1 & 14021720 & 15806.6 & -    & - \\
      & 2 & 6313070 & 7254.85  & 2.22 & 2.18 \\
      & 4 & 3479344 & 4932.55  & 4.03 & 3.20 \\
      & 6 & 2783339 & 3955.38  & 5.04 & 3.99 \\
    \hline
  \end{tabular}
\end{table}

In order to demonstrate the uniform convergence of MIAGS, all test problems have been solved by IAGS as well.
IAGS is comparable with other stochastic and deterministic derivative-free global optimization algorithms \cite{Sovrasov2019}. Thus, it
provides a strong baseline in solving a single global optimization problem.
Fig. \ref{fig:devs_mixed} presents the plots of mean and maximum distances from the actual
optima to the current estimates of the optima when solving a series of problems generated by
two different generators separately (solid line) and jontly (dashed line).
Despite the essential differences in the structure of problems, the MIAGS decreased
the maximum deviations of the estimates from the optima, as well as the mean optima much faster.
It evidences that the uniform convergence over the whole set of problems has been achieved.
In the case of the sequential solving of the problems, the magnitude \(D_{max}\) takes
its maximum value until the last problem is solved.

\begin{figure}[ht]
    \centering
    \subfloat[\(D_{max}\)]{
    \label{fig:max_dev} {\includegraphics[width=.5\textwidth]{pic/mixed_2d_max.pdf}}}
    \subfloat[\(D_{avg}\)]{
    \label{fig:avg_dev} {\includegraphics[width=.5\textwidth]{pic/mixed_2d_avg.pdf}}}
    \caption{Dynamics of \(D_{avg}\) and \(D_{max}\) in the course of solving a set of the two-
dimensional problems generated by two different generators GKLS and \(F_{GR}\)}
    \label{fig:devs_mixed}
\end{figure}

\subsection{Example of solving a multicriteria problem}

In order to demonstrate the efficiency of the approach in the balancing of the load, let us
consider an example, in which a set of problems of the kind (\ref{eq:many_problems}) is
generated as a result of scalarization of a multicriteria optimization problem with constrains.

Let us consider a test problem from \cite{BinhKorn1999}:
\begin{equation}
  \label{eq:mco_probem}
  \begin{array}{l}
      Minimize \left \{
      \begin{array}{l}
        f_1(y) = 4 y_1^2 + 4 y_2^2 \\
        f_2(y) = (y_1-5)^2 + (y_2-5)^2 \\
      \end{array}
      \right .
      , y_1\in [-1,2],y_2\in [-2,1],
      \\s.t.
      \\
      \left \{
      \begin{array}{l}
        g_1(y) = (y_1 - 5)^2 + y_2^2 - 25 \leqslant 0, \\
        g_2(y) = -(y_1 - 8)^2 - (y_2 + 3)^2 + 7.7 \leqslant 0.\\
      \end{array}
      \right .
  \end{array}
\end{equation}

Let us use the Germeyer convolution for the scalarization of the problem
(\ref{eq:mco_probem}).
After the convolution, the scalar objective function takes the form:
\begin{equation}
  \varphi(y,\lambda_1,\lambda_2)=\max\{\lambda_1 f_1(y), \lambda_2 f_2(y)\},
\end{equation}
where \(\lambda_1,\lambda_2\in[0,1],\: \lambda_1+\lambda_2=1\).
Testing all possible convolution coefficients allows finding the whole set of Pareto-optimal
solutions of the problem (\ref{eq:mco_probem}).
For the numerical construction of the Pareto set, let us select 100 sets of coefficients
\((\lambda_1,\lambda_2)\) so that
\(\lambda_1^i=i h,\: \lambda_2^i=1-\lambda_1^i,\: h=10^{-2},\: i=\overline{1, 100}\).

Computational resources were limited to 2500 trials.
The set of auxiliary scalar problems was solved by two methods:
\begin{itemize}
  \item each problem was solved separately using IAGS with a preset limit of 25 trials. This way,
the computational resources were distributed among the problems uniformly;
  \item all problems were solved simultaneously using MIAGS with a preset limit of
2500 trials.
\end{itemize}
In both cases, the parameter \(r=4\).

\begin{figure}[ht]
    \centering
    \subfloat[IAGS, separate solving of the problems]{
    \label{fig:mco_pareto_1} {\includegraphics[width=.5\textwidth]{pic/single_mco.pdf}}}
    \subfloat[MIAGS for the set of problems]{
    \label{fig:mco_pareto_2} {\includegraphics[width=.5\textwidth]{pic/multi_mco.pdf}}}
    \caption{Numerical estimates of Pareto set in the problem (\ref{eq:mco_probem}), obtained
after 2500 trials}
    \label{fig:mco_pareto}
\end{figure}

The plots of solutions obtained by each method are presented in Fig. \ref{fig:mco_pareto_1}
and Fig. \ref{fig:mco_pareto_2}.
All plots agree with the ones presented in \cite{BinhKorn1999} qualitatively (the authors
did not provide any other information to compare).
The Pareto curve in Fig. \ref{fig:mco_pareto_1} has concavities that do not
match the solution presented in \cite{BinhKorn1999}, which means there are not enough resources for solving some auxiliary problems.
To evaluate the quality of solution, the index \(Spacing(SP)\) \cite{RiquelmeLucken2015}
featuring the density of the points approximating the Pareto set was computed.
\begin{equation*}
\begin {array}{l}
  SP(S)=\sqrt{\frac{1}{|S|-1} \sum_{i=1}^{|S|} (\overline{d}-d_i)^2},
  \; \overline{d}=mean\{d_i\},
  \\d_i=\min_{s_i,s_j\in S:s_i\ne s_j}||F(s_i)-F(s_j)||_1,\; F=(f_1,f_2).
  \end{array}
\end{equation*}
When problems were solved separately  \(SP_{single}=0.984\), when the load balancing method was
applied  \(SP_{multi}=0.749\), which evidences a better quality of the approximation
of the solution.

\section{Conclusion}

This paper demonstrates how the support of the non-convex constraints can be implemented
in the algorithm to solve a set of the global optimization problems.
This study allowed to find the sufficient conditions of convergence for the developed method.
The numerical experiments conducted within this research demonstrate the advantages
of the considered approach over separate solution of the problems.
The efficiency of joint solution of a set of problems was demonstrated on the example a
multicriteria problem with nonlinear constraints.
Further research in this direction should improve current implementation
of the algorithm by reducing the support costs of the search information for
the set of problems. This, in turn, should improve the calculation speed
due to parallelization. There are also plans to implement a version of considered algorithm in the distributed memory according to the scheme described in  \cite{BarkalovLebedev2017_2}.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{bibliography}
%
\end{document}
