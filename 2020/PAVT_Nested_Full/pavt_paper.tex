%%%%%%%%%%%%%%%%%%%% author.tex 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%
\usepackage{graphicx}
\usepackage{marvosym}

%\pdfmapfile{+txfonts.map}

%\usepackage[utf8]{inputenc}
%\usepackage[english, russian]{babel} % Русские и английские переносы

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\def\UrlFont{\rmfamily}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
%
\title{Combining Local and Global Search in a Parallel Nested Optimization Scheme\thanks{This study was supported by the Russian Science Foundation, project No.\,16-11-10150.}
}
%
\titlerunning{Combining local and global search}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Konstantin Barkalov\letter\orcidID{0000-0001-5273-2471} \and Ilya 
Lebedev\orcidID{0000-0002-8736-0652} \and Maria Kocheganova \orcidID{0000-0002-4722-6299} 
\and Victor Gergel \orcidID{0000-0002-4013-2329}}
%
\authorrunning{Konstantin Barkalov et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Konstantin Barkalov and Ilya Lebedev and Maria Kocheganova and Victor Gergel}
%
\institute{Lobachevsky State University of Nizhni Novgorod, Russia  \\
	\email{konstantin.barkalov@itmm.unn.ru},
	\email{ilya.lebedev@itmm.unn.ru},
	\email{maria.rachinskaya@itmm.unn.ru},
	\email{gergel@unn.ru}
}
	
\maketitle              % typeset the title of the contribution

\begin{abstract}

This paper considers global optimization problems and numerical methods to solve them. The following assumption is made regarding the dependence of the objective function on its parameters: it is multiextremal concerning selected variables only, and the dependence on the other variables has a local character. Such problems may arise when identifying unknown parameters of the mathematical models based on experimental results. A parallel computational scheme, which accounts for this feature, is being proposed. This novel scheme is based on the idea of nested (recursive) optimization, which suggests optimizing the significant variables at the upper recursion level (using the global optimization method) and solving the local optimization problems at the lower level (using the local method). The generated local subproblems will not affect each other and can be solved in parallel. The efficiency of the proposed parallel computation scheme has been confirmed by the numerical experiments conducted on several hundreds of test problems.


\keywords{Global optimization $\cdot$ Local optimization $\cdot$ Recursive optimization scheme 
$\cdot$ Parallel algorithms}
\end{abstract}

\section{Introduction}

Global optimization problems arise frequently in various fields of modern science and technology. For example, the problems of finding the configurations of various chemical compounds corresponding to the minimum interaction energy are of current interest \cite{Posypkin2014}. In particular,  such problems arise 
in the development of new medical drugs \cite{Sulimov}. 
Another example is the problem of finding optimal control parameters for complex processes \cite{Kalyulin2017,Modorskii2016}.
Many other applications also generate global optimization problems (see the review \cite{Pinter2006}). 

The application of global optimization methods for identifying the parameters of the mathematical models based on the data of experiments has become a conventional practice. These problems require such a search for values of the unknown model parameters, which makes the computational results based on the model close to the ones obtained experimentally.

The number of parameters which are required to be identified in such a way may reach tens and hundreds in complex mathematical models \cite{Akhmadullina2017,Nurislamova2016}. In the problems of such dimensionality, regular search methods for a global solution cannot be applied because of the extremely large computational costs for covering the search domain by the trial points. This remains true even with the use of efficient algorithms (for example, \cite{Evtushenko2009,Jones2009,Paulavicius2011}) constructing essentially nonuniform coverage. 

However, in the problems of model identification, it is usually possible to select the parameters, which will affect the result most significantly. As a rule, the number of such parameters is small. The rest of the parameters either cause an insufficient effect (in this case, multiextremality can be neglected) or dependence on them is local.

Currently, the development of parallel algorithms which would minimize the essentially multidimensional functions (hundreds of variables) and account for the dependence of the objective function on different groups of parameters is relevant. Specifically, the objective function is assumed to be multiextremal for some of the variables. The rest of the variables have a local effect, i.e., the objective function is unimodal for them. In this case, the problem-solving can be organized according to the nested (recursive) optimization scheme. The multiextremal subproblem, which requires the use of complex global search algorithms, will be soleved at the upper level of the recursion. The unimodal subproblems (each corresponding to a fixed set of values of the first part of the parameters) will be solved at the lower level. To solve these subproblems, efficient methods of local optimization, linear programming or linear algebra can be applied (subject to the character of the local subproblems).

Below you will find a description of this paper structure. Section 2 describes the nested optimization scheme taking into account different properties of the objective function at different nesting levels. A particular example of such a problem is presented, along with a general scheme of the parallel computation organization. Section 3 is devoted to the description of the parallel global search algorithm using space-filling curves. The computational scheme of the parallel algorithm is presented, and its main properties are described. Section 4 discusses the methods to solve the local subproblems within the framework of the nested optimization scheme. In addition, this section briefly describes the properties pertaining to the local extremum search problems and outlines the known methods for this type of problems. Section 5 contains the results of a numerical experiment. The scalability of the proposed parallel computation scheme for solving a series of test problems was evaluated. Section 6 concludes the paper.


\section{Nested Optimization Scheme}

Let us consider an optimization problem of the type
\begin{eqnarray}\label{main_problem}
& f(x^\ast,y^\ast)=\min{\left\{f(x,y):x\in S, y\in D\right\}}, \nonumber \\
& S=\left\{x\in R^M: a_i\leq x_i \leq b_i, 1\leq i \leq M\right\}, \\
& D=\left\{y\in R^N: c_i\leq y_i \leq d_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}

Let us assume that at any fixed set of values $\overline{x}$ the function $f(\overline{x},y)$ is a 
multiextremal one and satisfies the Lipschitz condition
\[
\left|f(\overline{x},y')-f(\overline{x},y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty,
\]
with the constant $L$ unknown a priori.
At the same time, at any fixed set of values $\overline{y}$, the function $f(x,\overline{y})$ is a 
unimodal one, i.e. the function $f(x,\overline{y})$ has a single minimum point $x^*$ (generally depending on $\overline{y}$). 

Accounting for such a peculiarity of the considered problem class can essentially reduce the computational costs 
of the optimum search. Indeed, according to the known nested optimization scheme 
\cite{Carr}, the initial problem can be reduced to the problem of finding the global 
minimum of the function $\varphi(y)$
\begin{equation}\label{global_problem}
\varphi(y^*) = \min_{y\in D}  \varphi (y),
\end{equation}
where 
\begin{equation}\label{local_problem}
\varphi(y) = \min_{x\in S} f(x,y).
\end{equation}
According to (\ref{global_problem}), computing a single value of the function $\varphi (y)$ 
(let us call this process a search trial) implies unimodal problem solution (\ref{local_problem}), 
which can be performed with one of the local search methods. The efficiency and sophistication of the 
local optimization methods allow solving subproblems (\ref{local_problem}) with the precision essentially 
exceeding that of the global optimization methods. Correspondingly, the problem 
(\ref{global_problem}) can be considered a global optimization problem, where the objective 
function can be computed with high precision but this operation is time-consuming. 

The following approximation problem can be considered a particular example of the problem, where 
a different character of dependence on different groups of parameters takes place. 
Let us assume $m$ values $u_j = u(t_j), 1 \leq j \leq m, $ of the function $u(t)$ have been obtained 
in an experiment and the analytical expression for $u(t)$ is known to take the form 
\begin{equation}\label{ex_func}
u(t) = u(t,\omega,c)=\sum^{n}_{i=0}\left[c_{2i+1}\sin(\omega_it) + c_{2i+2}\cos(\omega_it)\right].	
\end{equation}
In this expression, $c=\left(c_1,\ldots,c_{2n+2}\right)$, 
$\omega=\left(\omega_0,\omega_1,\ldots,\omega_n\right)$ are the unknown parameters defining 
a particular function $u(t)$.

Let us introduce a measure of function $u(t,\omega,c)$ deviation from the experiential data as a 
sum of squares 
\[
\Delta(\omega,c)= \sum^{m}_{j=1}\left[u_j-u(t_j,\omega,c)\right]^2. 
\]
Now, following the idea of least squares fitting, it is possible to present the approximation problem as 
the problem of minimizing the objective function
\begin{equation}\label{ex_prob}
\Delta(\omega^*,c^*) = \min_{\omega,c} \Delta(\omega,c).	
\end{equation}
The solution $(\omega^*,c^*)$ of this problem will define the best approximation.

Problem (\ref{ex_prob}) can be written in a recursive form 
\begin{equation}\label{outer_prob}
\varphi(\omega^*) = \min_\omega \varphi(\omega),
\end{equation}
\begin{equation}\label{inner_prob}
\varphi(\omega) = \min_c \Delta(\omega,c).
\end{equation}
Nested subproblem (\ref{inner_prob}) is a classical linear least squares problem, and its solution 
can be obtained by solving a system of linear algebraic equations regarding the unknown $c$, which 
can be done, for example, by Gaussian elimination. At the same time, external problem (\ref{outer_prob}) 
will be, in general, a multiextremal one, and its solving will require considerable 
computational resources.

So far, to solve problem (\ref{main_problem}) in nested scheme (\ref{global_problem}), 
(\ref{local_problem}) one can apply a parallel global search algorithm for solving external 
subproblems (\ref{global_problem}). At that, a set of independent subproblems (\ref{local_problem}) 
will be generated at every iteration of the algorithm, the solving of which can be performed in parallel 
using the local methods.
A general scheme of the computation organization using several processors is presented in 
Fig.~\ref{MPI_tree}. 

\begin{figure}
\center{\includegraphics[width=1.0\linewidth]{MPI_Tree.pdf}}
\caption{A tree of subproblems in the parallel nested optimization scheme}
\label{MPI_tree}
\end{figure}

The parallel program processes will form a tree. 
The root process will solve problem (\ref{global_problem}) and distribute subproblems 
(\ref{local_problem}) among the child processes. Each subproblem is solved by a separate process; 
the data exchange between the processes can be organized using MPI. The data transmission between 
the processes will be minimal: it is required to send the coordinates of points $y^1, ..., y^p$ from the 
root to the child processes and to receive the found minimal values back
\[
\varphi(y^1) = \min_x f(x,y^1),\; ..., \; \varphi(y^p) = \min_x f(x,y^p).
\]

A detailed discussion of the algorithms used at different levels of the process tree is given in the next 
sections. 

\section{Parallel Global Search Algorithm}

An efficient parallel global optimization algorithm intended for solving the problems of kind 
(\ref{global_problem}) has been developed in Lobachevsky University of Nizhni Novgorod (UNN) 
\cite{Barkalov2018,globalizerSystem,Strongin2018}. 
The main idea of the parallelization consists in such an organization of computations during which several trials 
are performed in a parallel way. This approach is featured by a high efficiency (the part of the computational 
process with the major amount of computations is parallelized) and generality (it is 
applicable for a wide class of algorithms).

In the developed global search algorithm, a novel method to reduce the dimensionality of 
the problems to be solved is applied. 
The reduction of the dimensionality is based on the fundamental fact, according to which an 
$N$-dimensional hyperparallelepiped $D$ and the interval $[0,1]$ of the real axis are  
equal-power sets, and the interval $[0,1]$ can be mapped continuously into $D$ using the Peano 
curve $y(x)$, i.e. $D = \left\{y(x):0\leq x\leq 1\right\}$.

Utilizing this fact, one can reduce the minimization problem of a multidimensional 
function $\varphi(y)$ to the minimization of a one-dimensional function $\varphi(y(x))$
\begin{equation}\label{1d_problem}
\varphi(y (x^*)) = \min \left\{ \varphi (y(x)) : x\in [0,1] \right\},
\end{equation}
where the function $\varphi(y(x))$ will satisfy a uniform H{\"o}lder condition
\[
\left|\varphi(y(x'))-\varphi(y(x''))\right|\leq H\left|x'-x''\right|^{1/N}
\]
with the H{\"o}lder constant $H$ linked to the Lipschitz constant $L$ by the relation
$ H=2 L \sqrt{N+3}$ and $y(x)$ being the Peano curve from $[0,1]$ onto $D$.

The algorithms for the numerical construction of the Peano curve approximations (called 
\textit{evolvents})
are given in \cite{Sergeyev2013,Strongin2000}. As an illustration, two evolvents are shown in 
Fig.~\ref{fig_evolvent}. The figure demonstrates that the precision of the evolvent depends on the 
density level $m$ used in the construction.

\begin{figure}
\center
\begin{minipage}{0.48\linewidth}
\center{\includegraphics[width=1.0\linewidth]{fig1b.JPG} \\ (a)}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\center{\includegraphics[width=1.0\linewidth]{fig1c.JPG} \\ (b)}
\end{minipage}
\caption{The evolvents in two dimensions with (a) $m=4$ and (b) $m=5$}
\label{fig_evolvent}
\end{figure}

It is worth noting that many well-known global optimization algorithms are based implicitly on the 
idea of the dimensionality reduction and adaptation of one-dimensional algorithms for solving  
multidimensional problems \cite{Evtushenko2013,Sergeyev2006,Zilinskas2008}.

According to \cite{Strongin2018,Strongin2000}, the rules of the global search algorithm, where 
$p$ trials are performed in parallel at every iteration, are as follows.
At the first iteration of the method, the trials are performed in parallel in two boundary points $x^1 = 
0, \; x^2 = 1$ as well as in $(p-2)$ arbitrary internal points of the interval $[0,1]$, i.e. 
$x^i\in(0,1),3\leq i \leq p$.

Supposing $n\geq 1$ iterations of the method have been carried out, where the trials were performed at 
$k=k(n)$ points $x^i,1\leq i \leq k$, then the points $x^{k+1},\dots,x^{k+p}$  of the search trials at the next 
iteration are determined according to the following rules.

Rule 1. Renumber the points $x^1,...,x^k$ of the preceding trials in lowercase in ascending order of the coordinate values, i.e.
\[
0=x_1<x_2<\dots <x_{k-1} <x_k=1,
\]
and juxtapose them with the values $z_i=\varphi(y(x_i)), \; 1 \leq i \leq k$, computed at these points.

Rule 2. Compute the current lower estimates
\begin{equation}\label{Rule_Mu}
\mu = \max\left\{ \frac{\left|z_i-z_{i-1}\right|}{ \Delta_i },\; 2 \leq i \leq k  \right\},
\end{equation}
where $\Delta_i = (x_i-x_{i-1})^{1/N}$. If $\mu$ is equal to zero, then assume $\mu = 1$.

Rule 3. For each interval ($x_{i-1},x_i), \; 2 \leq i \leq k,$ compute
the \textit{characteristics} $R(i)$ :
\begin{equation}\label{Rule_R}
R(i)=\Delta_i+\frac{(z_i-z_{i-1})^2}{r^2 \mu^2\Delta_i}-2\frac{z_i+z_{i-1}}{r \mu},
\end{equation}
where $\Delta_i=(x_i-x_{i-1})^{1/N}$. The value $r > 1$ is an algorithm parameter. An 
appropriate selection of $r$ allows considering the product $r \mu$ as an estimate
of the Lipschitz constant $L$ of the objective function.

Rule 4. Arrange characteristics  $R(i), 2 \leq i \leq k$, in decreasing order 
\begin{equation}\label{Rule_Max}
R(t_1)\geq R(t_2)\geq \dots \geq R(t_{k}) \geq R(t_{k})
\end{equation}
and select $p$ maximum characteristics with interval numbers $t_j, 1\leq j \leq p$.

Rule 5. Carry out new trials at points $x^{k+j}\in(x_{t_j-1},x_{t_j}), \; 1\leq j\leq p$, calculated 
using the formula
\begin{equation}\label{Rule_X}
x^{k+j} = \frac{x_{t_j}+x_{t_j-1}}{2} - 
\frac{\mathrm{sign}(z_{t_j}-z_{t_j-1})}{2r}\left[\frac{\left|z_{t_j}-z_{t_j-1}\right|}{\mu}\right]^
N.
\end{equation}

The algorithm terminates if the condition $\Delta_{t_j}<\epsilon$ is performed at least for one number 
$t_j, 1 \leq j \leq p$ ; $\epsilon>0$ is the predefined accuracy.

This method of parallelization has the following justification. The characteristics of intervals 
(\ref{Rule_R}) used in the method can be considered as probability measures of the global minimum 
point location in these intervals. Inequalities (\ref{Rule_Max}) arrange intervals according to their 
characteristics, and the trials are carried out in parallel in the first $p$ intervals with the largest 
probabilities.

Various modifications of this algorithm and the corresponding theory of
convergence are given in \cite{Barkalov2018,Sergeyev2013,Strongin2018,Strongin2000}.
%Замечание по поводу теоретических свойств и сравнения алгоритмов
The results of the experimental comparison  (see, for example, \cite{Sovrasov2019}) show that the global search algorithm is superior to many known methods of similar purpose, including both deterministic and heuristic  algorithms.

\section{Local Search Algorithms}

In this section, let us consider the issues related to solving subproblem 
(\ref{local_problem}), which (in the notation $f(x) = f(x,\overline{y})$ for the fixed value 
$\overline{y} \in D$) corresponds to the problem of finding a local extremum

\begin{equation} \label{lp}
f(x) \rightarrow \min, x\in S. 
\end{equation}

To date, a huge number of various local optimization methods for the problems of kind (\ref{lp}) 
have been developed. They include such methods as the gradient ones, Newton, and 
quasi-Newton methods, the methods of conjugate directions, etc. The majority of them utilizes 
the principle of local descent when the method progressively passes the points with smaller values of the 
objective function at every step. Almost all these methods can be represented in a 
form of the iterative relation 
\[
x^{k+1} = x^k + s^k d^k,
\]
where $x^k$ are the points of main trials consisting in computing a set of some local characteristics of 
the objective function $I^k=I(x^k)$ at the point $x^k$, where $d^k$ are the shift directions from 
the points $x^k$ computed on the base of the main trial results, and $s^k$ are the coefficients 
defining the magnitudes of the shifts along the directions chosen.

To define the shift magnitudes $s^k$ along the directions $d^k$, the methods can include some  
auxiliary (working) steps. This results in additional measuring of the objective function local characteristics  
along the direction $d^k$. The transitions from the points $x^k$ to the points 
$x^{k+1}$ is performed in such a way that the function $f^k = f( x^k )$ value essentially decreases as a result of this step.

The set of the computed local characteristics $I^k=I(x^k)$ may include: the function values $f^k = 
f( x^k )$, the gradient $\nabla f^k = \nabla f(x^k)$, and the matrix of the second derivatives 
(Hessian) $\Gamma_k=\Gamma^f(x^k)$. A particular set of the characteristics to be measured depends 
on the properties of the problem being solved as well as on the optimization method chosen.

In the local optimization problems arising in the applications, the a priori information on the objective 
functions is usually quite limited. For example, a local character of the function dependence on 
the parameters may only be suggested where the gradient values (and Hessian) are
unknown. In this case, the applied methods are the direct search ones, which do not utilize any suggestions on
the objective function smoothness. When searching for the minimum, these methods measure 
the function values only. The rules of the placement of the iteration points in these ones are based on 
some heuristic logical schemes. 

Hooke--Jeeves method \cite{HookJeeves} and Nelder--Mead method \cite{NelderMead} belong to 
the popular methods of the direct search. Despite their apparent simplicity and theoretical 
non-substantiation of the direct search methods, these two methods are well established in the practical 
computations. This can be explained as follows. Actually, many smooth optimization methods are very 
sensitive to computational errors in the function values transforming the theoretically smooth function 
into the non-smooth one. Because of this, in the practical computations, these methods lose positive properties, which are often promised by theory.  In these conditions, direct search methods allow achieving better results.

\section{Numerical Experiments}

The scheme of parallel computations described in previous sections was implemented in the 
Globalizer software system developed in UNN \cite{globalizerSystem,Sysoyev2017}.  
The numerical experiments were carried out using the Lomonosov supercomputer (Lomonosov Moscow 
State University). Each supercomputer node included two quad-core processors Intel Xeon X5570 
2.93GHz and 12 Gb RAM. To build the Globalizer system for running on the Lomonosov 
supercomputer, we used the GCC 5.5.0 compiler and Intel MPI 2017.

To simulate the applied problems, where the locally-affecting parameters can be selected, 
a test function of the kind 
\begin{equation}\label{test_problem}
f(x,y) = G(y)+R(x)
\end{equation}
was used when conducting the experiments. Here, $G(y)$ is a multiextremal function of the 
dimensionality $N=2$ and $R(x)$ is a unimodal function of the dimensionality $N \gg 2$.
As the multiextremal part of the problem, the functions 
\begin{eqnarray} \nonumber \label{vagris}
G(y)= 
-&\left\{\left(\sum^{7}_{i=1}\sum^{7}_{j=1}A_{ij}g_{ij}(y)+B_{ij}h_{ij}(y)\right)^2+\right. \\
&\left.\left(\sum^{7}_{i=1}\sum^{7}_{j=1}C_{ij}g_{ij}(y)+D_{ij}h_{ij}(y)\right)^2\right\}^{1/2
},\\ \nonumber
\end{eqnarray}
were considered, where
\begin{eqnarray} \nonumber
& y=(y_1,y_2)\in R^2, 0 \leq y_1,y_2 \leq 1, \\ \nonumber
& g_{ij}(y)=\sin(i\pi y_1)\sin(j\pi y_2),  \\ \nonumber
& h_{ij}(y)=\cos(i\pi y_1)\cos(j\pi y_2), \nonumber 
\end{eqnarray}
and the coefficients $A_{ij}, B_{ij}, C_{ij}, D_{ij}$ are the random numbers uniformly distributed  
in the interval $[-1,1]$.
This class of functions is often used for testing global optimization algorithms. 
As the local part of the problem, the modified Rosenbrock function 
\[
R(x)= \sum_{i=1}^{N-1}{\left[(1-x_i)^2+100(x_{i+1}-x_i^2)^2\right]}, -2 \leq x_i \leq 2 , 1\leq 
i\leq N,
\]
was used, where the extremum point was randomly shifted in the search domain using the linear 
coordinate transform.

As an example, contour plots of a pair of functions  $G(y)$ and $R(x)$ are presented in 
Fig.~\ref{fig_level}. These functions are the complex ones for the corresponding global and local 
optimization methods since the function $G(y)$ is essentially multiextremal, and the function 
$R(x)$ has a ravine clearly manifested structure.

\begin{figure}
\center
\begin{minipage}{0.48\linewidth}
\center{\includegraphics[width=1.0\linewidth]{grishagin.png} \\ (a)}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\center{\includegraphics[width=1.0\linewidth]{rosenbrock.png} \\ (b)}
\end{minipage}
\caption{Contour plot of the functions (a) $G(y)$ and (b) $R(x)$}
\label{fig_level}
\end{figure}

In Fig.~\ref{fig_level}, the trial points performed by the global search algorithm and Hooke--Jeeves 
method when solving the corresponding problems are also marked. 
The trial point positions clearly demonstrate the function $G(y)$ multiextremality (the 
trial points form a nonuniform coverage of the search domain) as well as the ravine character 
of the function $R(x)$ (the trial points form an elongated group along the ravine bottom).

The problem was considered to be solved if in the course of minimizing the function $G(y)$, the 
global search algorithm generates the next trial point $y^k$ in the $\epsilon$-neighbourhood of the global 
minimum $y^*$, i.e. $\left\|y^* - y^k\right\|\leq\epsilon$, where $\epsilon = 10^{-2}$. 
At that, the precision of finding the minimum when minimizing the unimodal function $R(x)$ was 
selected 1000 times lower, i.e. $\delta = 10^{-5}$.
In the global search algorithm, the parameter $r=4.5$ from (\ref{Rule_R}) and the evolvent building 
parameter $m=10$ were used; the employed local Hooke--Jeeves method had no additional parameters.

In the first series of experiments, 100 problems of kind (\ref{test_problem}) were solved.  Their local subproblems dimensionality was $N=50$. The problems were solved using 1, 4, and 8 
cluster nodes, which employed from 2 to 32 MPI-processes. According to the 
parallelization scheme presented in Fig.~\ref{MPI_tree}, at the upper nesting level the global optimization problem was solved in a single (master) process, which initiated parallel solving of the lower-level local problems (from 1 up to 31 problems, respectively). So far, the minimum possible number of the employed processes equalled 2, one per each nesting level.

Table \ref{tab1} presents the average number of iterations $K_{av}$ performed by the global search 
algorithm when minimizing the function $G(y)$, the average time of solving the whole problem 
$T_{av}$, and the speed-up $S$ subject to the number of employed MPI processes $p$. 

\begin{table}
\centering
\caption{Results of solving a series of problems of dimensionality $N=52$}\label{tab1}
\begin{tabular}{cccc}
\hline\noalign{\smallskip}
 $\;\;\;p\;\;\;$  &  $\;\;\;K_{av}\;\;\;$ &  $\;\;\;T_{av}\;\;\;$ & $\;\;\;S\;\;\;$ \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 2  & 349.4  & 9.14 & ---  \\
 16 & 25.6   & 0.70 & 13.2 \\
 32 & 12.9   & 0.37 & 24.6 \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}


In the second series of experiments, 10 problems of kind (\ref{test_problem}) were solved, in 
which the dimensionality of the local subproblems was equal to $N=100$. The running parameters were 
the same as in the previous series of experiments. The results of solving are presented in Table 
\ref{tab2}.

\begin{table}
\centering
\caption{The results of solving a problems series with dimensionality $N=102$}\label{tab2}
\begin{tabular}{cccc}
\hline\noalign{\smallskip}
 $\;\;\;p\;\;\;$  &  $\;\;\;K_{av}\;\;\;$ &  $\;\;\;T_{av}\;\;\;$ & $\;\;\;S\;\;\;$ \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 2  & 299.1  & 41.19 & ---  \\
 16 & 22.0   & 2.92 & 14.1 \\
 32 & 11.7   & 1.50 & 27.4 \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

% В последней серии экспериментов было решено 10 задач, в которых в качестве многоэкстремальной части использовались трехмерные задачи из класса GKLS. Данный класс задач часто используется для тестирования методов глобальной оптимизации [ссылки]. Размерность локальных подзадач была равна N = 100. Задачи решались на 1, 8, 16 и 32 узлах кластера, на которых использовалось от 4 до 128 MPI-процессов. Параметры запуска такие же, как в предыдущих сериях экспериментов. Результаты решения представлены в таблице, ускорение посчитано относительно запуска на одном узле на 4 mpi процесса. 

In the last series of experiments, 10 problems were solved, in which three-dimensional functions from GKLS class \cite{Gaviano2003} were used as a multiextremal part. 
This class of functions is often used for investigating global optimization algorithms \cite{Barkalov2018,Paulavicius2014,Sergeyev2015}.
The dimensionality of the local subproblems was equal to $N=100$.
The problems were solved at 1, 8, 16, and 32 cluster nodes, on which from 4 to 128 MPI processes were used.
The running parameters were the same as in the previous series of experiments.
The results of solving are presented in Table \ref{tab3}, where speed-up is calculated for running on a single node using 4 MPI processes.

%In the last series of experiments, 10 problems of the kind (\ref{test_problem}) were solved, in which the dimensionality of the local subproblems was equal $N=100$. The running parameters were the same as in the previous series of experiments. The results of solving are presented in Table \ref{tab3}.

\begin{table}
\centering
\caption{The results of solving a problems series with dimensionality $N=103$}\label{tab3}
\begin{tabular}{cccc}
\hline\noalign{\smallskip}
 $\;\;\;p\;\;\;$  &  $\;\;\;K_{av}\;\;\;$ &  $\;\;\;T_{av}\;\;\;$ & $\;\;\;S\;\;\;$ \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 4  & 730.8  & 63.4 & ---  \\
 32 & 87.4   & 7.37 & 8.6 \\
 64 & 37.0   & 3.77 & 16.8 \\
 128 & 16.5   & 1.78 & 35.7 \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}



\section{Conclusion}

The results of the experiments performed on a series of optimization problems demonstrate that the parallel computation scheme proposed in this work has a good potential for the parallelization. Specifically, when solving a series of 100 problems with a total number of variables equal to 52 (from which the first two variables caused a global effect on the objective function and the other fifty affected the one locally), the obtained speed-up was 24.6 when using 32 processes. When solving problems with more complex local subproblems (with the local variables number $N=100$), the speed-up increased up to 35 using 128 processes. In the last case, the speed-up was calculated subject to using 4 processes on one node. The developed parallel computation scheme can be applied in identifying complex mathematical models, which feature a large number (tens and hundreds) of unknown parameters.

%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}

\end{document}
