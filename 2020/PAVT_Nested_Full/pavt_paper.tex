%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\usepackage{graphicx}
\usepackage{marvosym}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel} % Русские и английские переносы

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\def\UrlFont{\rmfamily}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
%
\title{ Combining local and global methods in a parallel nested optimization scheme 
\thanks{This study was supported by the Russian Science Foundation, project No.\,16-11-10150.}
}
%
\titlerunning{ABBR}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Konstantin Barkalov\letter\orcidID{0000-0001-5273-2471} \and Ilya Lebedev\orcidID{0000-0002-8736-0652} \and Maria Kocheganova \orcidID{0000-0002-4722-6299}}
%
\authorrunning{Konstantin Barkalov et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Konstantin Barkalov and Ilya Lebedev, and Maria Kocheganova}
%
\institute{Lobachevsky State University of Nizhni Novgorod, Russia  \\
	\email{konstantin.barkalov@itmm.unn.ru}
}
	
\maketitle              % typeset the title of the contribution

\begin{abstract}

В статье рассматриваются задачи глобальной оптимизации и численные методы их решения. О характере зависимости целевой функции от ее параметров делается предположение, что она является многоэкстремальной лишь по некоторым из переменных, а зависимость от остальных параметров носит локальный характер. Задачи такого типа могут возникать при идентификации математических моделей по результатам экспериментов. Предложена параллельная схема вычислений, которая учитывает данную особенность. Новая схема основана на идее рекурсивной оптимизации, когда на верхнем уровне рекурсии проводится оптимизации по параметрам, влияющим глобально (с помощью алгоритма глобального поиска), а на нижнем -- решение задач локальной оптимизации (с помощью локальных методов). Возникающие при этом локальные подзадачи не будут оказывать влияния друг на друга, и их решение можно проводить параллельно. Проведены численные эксперименты на нескольких сотнях тестовых задач, подтверждающие эффективность предложенной схемы параллельных вычислений.


\keywords{Global optimization $\cdot$ Local optimization $\cdot$ Recursive optimization scheme $\cdot$ Parallel algorithms}
\end{abstract}

\section{Introduction}


%Вводные слова

Новым элементом, исследуемым в данной работе, является предположение о разном характере зависимости целевой функции от разных переменных параметров. 
Предполагается, что целевая функция является многоэкстремальной лишь по части переменных. Остальные переменные влияют локально, т.е. по ним функция является одноэкстремальной. 

В этом случае решение задачи можно организовать по схеме вложенной (рекурсивной) оптимизации. Решение  первой (многоэкстремальной) подзадачи, для которой требуется использовать сложные алгоритмы глобальной оптимизации, будет проводиться на верхнем уровне рекурсии.
Одноэкстремальные подзадачи (каждая из которых соответствует фиксированному набору значений первой части параметров) будут решаться на нижнем уровне. Для решения одноэкстремальных задач можно применять эффективные методы выпуклой оптимизации.

Примером задачи, в которой наблюдается разный характер зависимостей от параметров, может являться следующая задача аппроксимации.

Пусть в ходе эксперимента получены $m$ значений $u_j = u(t_j), 1 \leq j \leq m, $ функции $u(t)$, причем известно, что аналитическое выражение для  $u(t)$ имеет вид
\begin{equation}\label{ex_func}
u(t) = u(t,\omega,c)=\sum^{n}_{i=0}\left[c_{2i+1}\sin(\omega_it) + c_{2i+2}\cos(\omega_it)\right].	
\end{equation}
В данном выражении $c=\left(c_1,\ldots,c_{2n+2}\right)$, $\omega=\left(\omega_0,\omega_1,\ldots,\omega_n\right)$ являются неизвестными параметрами, определяющими конкретную функцию $u(t)$.

Введем меру отклонения функции $u(t,\omega,c)$ от экспериментальных данных как сумму квадратов
\[
\Delta(\omega,c)= \sum^{m}_{j=1}\left[u_j-u(\tau_j,\omega,c)\right]^2. 
\]
Now, following the idea of least squares fitting, it is possible to present the approximation problem as the problem of minimizing the objective function
\begin{equation}\label{ex_prob}
\Delta(\omega^*,c^*) = \min_{\omega,c} \Delta(\omega,c).	
\end{equation}
Решение $(\omega^*,c^*)$ данной задачи будет определять наилучшее приближение.

Задача (\ref{ex_prob}) может быть записана в рекурсивной форме
\begin{equation}\label{outer_prob}
\varphi(\omega^*) = \min_\omega \varphi(\omega)
\end{equation}
\begin{equation}\label{inner_prob}
\varphi(\omega) = \min_c \Delta(\omega,c).
\end{equation}
Вложенная подзадача (\ref{inner_prob}) является классической линейной задачей наименьших квадратов, и ее решение can be obtained by solving a system of linear algebraic equations regarding the unknown $c$, which can be done, e.g., by Gaussian elimination. Внешняя задача (\ref{outer_prob}) будет являться, вообще говоря, многоэкстремальной, и для ее решения нужно применять методы глобальной оптимизации.

%Структура статьи

\section{Nested optimization scheme}

Рассмотрим задачу оптимизации вида
\begin{eqnarray}\label{main_problem}
& f(x^\ast,y^\ast)=\min{\left\{\varphi(x,y):x\in S, y\in D\right\}}, \nonumber \\
& S=\left\{x\in R^M: a_i\leq x_i \leq b_i, 1\leq i \leq M\right\}, \nonumber \\
& D=\left\{y\in R^N: c_i\leq y_i \leq d_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}

Будем предполагать, что при любом фиксированном наборе значений $x'$ функция $f(x',y)$ является многоэкстремальной и удовлетворяет условию Липшица по переменной $y$, т.е.
% условие Липшица. 
Одновременно с этим при любом фиксированном наборе значений $y'$  функция $f(x,y')$ является унимодальной, т.е. функция $f(x,y')$ имеет единственную точку минимума $x^*$. 

Учет подобной особенности рассматриваемого класса задач может существенно снизить вычислительную сложность поиска оптимума. В самом деле, в соответствии с известной схемой рекурсивной оптимизации (ссылка) решение исходной задачи можно свести к задаче поиска глобального минимума функции $\varphi(y)$
\[
\varphi(y^*) = \min_{y\in D}  \varphi (y)
%\min \left\{ f(x,y): x\in S, y \in D \right\} = \min_{y\in D}  \left\{ \min_{x\in S} f(x,y) \right\}.
\]
где 
\[
\varphi(y) = \min_{x\in S} f(x,y).
\]
В соответствии с данной схемой вычисление одного значения функции $\varphi (y)$ подразумевает решение локальной задачи , которое может быть выполнено одним из методов выпуклой оптимизации.


 

 

%Постановка задачи в целом, условие Липшица, многошаговая схема с 2 уровнями
%Внешний уровень - глобальная задача, вложенный уровень - локальная задача

\section{Parallel global search algorithm}

%Постановка задачи глобальной оптмизации
%Редукция размерности - Пеано
%Параллельный алгоритм

\section{Local search algorithms}

%Локальные задачи и локальные методы
%Стандартные библиотеки - будем использовать одну из них

\section{Numerical experiments}

%Сравнение разных локальных методов
%Эффективность распараллеливания 

\section{Conclusion}

%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}

\end{document}
