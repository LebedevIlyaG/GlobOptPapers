%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\usepackage{graphicx}
\usepackage{marvosym}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel} % Русские и английские переносы

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\def\UrlFont{\rmfamily}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
%
\title{ Combining local and global methods in a parallel nested optimization scheme 
\thanks{This study was supported by the Russian Science Foundation, project No.\,16-11-10150.}
}
%
\titlerunning{ABBR}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Konstantin Barkalov\letter\orcidID{0000-0001-5273-2471} \and Ilya Lebedev\orcidID{0000-0002-8736-0652} \and Maria Kocheganova \orcidID{0000-0002-4722-6299}}
%
\authorrunning{Konstantin Barkalov et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Konstantin Barkalov and Ilya Lebedev, and Maria Kocheganova}
%
\institute{Lobachevsky State University of Nizhni Novgorod, Russia  \\
	\email{konstantin.barkalov@itmm.unn.ru}
}
	
\maketitle              % typeset the title of the contribution

\begin{abstract}

В статье рассматриваются задачи глобальной оптимизации и численные методы их решения. О характере зависимости целевой функции от ее параметров делается предположение, что она является многоэкстремальной лишь по некоторым из переменных, а зависимость от остальных параметров носит локальный характер. Задачи такого типа могут возникать при идентификации математических моделей по результатам экспериментов. Предложена параллельная схема вычислений, которая учитывает данную особенность. Новая схема основана на идее рекурсивной оптимизации, когда на верхнем уровне рекурсии проводится оптимизации по параметрам, влияющим глобально (с помощью алгоритма глобального поиска), а на нижнем -- решение задач локальной оптимизации (с помощью локальных методов). Возникающие при этом локальные подзадачи не будут оказывать влияния друг на друга, и их решение можно проводить параллельно. Проведены численные эксперименты на нескольких сотнях тестовых задач, подтверждающие эффективность предложенной схемы параллельных вычислений.


\keywords{Global optimization $\cdot$ Local optimization $\cdot$ Recursive optimization scheme $\cdot$ Parallel algorithms}
\end{abstract}

\section{Introduction}


%Вводные слова


Примером задачи, в которой наблюдается разный характер зависимостей от параметров, может являться следующая задача аппроксимации.

Пусть в ходе эксперимента получены $m$ значений $u_j = u(t_j), 1 \leq j \leq m, $ функции $u(t)$, причем известно, что аналитическое выражение для  $u(t)$ имеет вид
\begin{equation}\label{ex_func}
u(t) = u(t,\omega,c)=\sum^{n}_{i=0}\left[c_{2i+1}\sin(\omega_it) + c_{2i+2}\cos(\omega_it)\right].	
\end{equation}
В данном выражении $c=\left(c_1,\ldots,c_{2n+2}\right)$, $\omega=\left(\omega_0,\omega_1,\ldots,\omega_n\right)$ являются неизвестными параметрами, определяющими конкретную функцию $u(t)$.

Введем меру отклонения функции $u(t,\omega,c)$ от экспериментальных данных как сумму квадратов
\[
\Delta(\omega,c)= \sum^{m}_{j=1}\left[u_j-u(\tau_j,\omega,c)\right]^2. 
\]
Now, following the idea of least squares fitting, it is possible to present the approximation problem as the problem of minimizing the objective function
\begin{equation}\label{ex_prob}
\Delta(\omega^*,c^*) = \min_{\omega,c} \Delta(\omega,c).	
\end{equation}
Решение $(\omega^*,c^*)$ данной задачи будет определять наилучшее приближение.

Задача (\ref{ex_prob}) может быть записана в рекурсивной форме
\begin{equation}\label{outer_prob}
\varphi(\omega^*) = \min_\omega \varphi{\omega}
\end{equation}
\begin{equation}\label{inner_prob}
\varphi(\omega) = \min_c \Delta{\omega,c}.
\end{equation}
Вложенная подзадача (\ref{inner_prob}) является классической линейной задачей наименьших квадратов, и ее решение can be obtained by solving a system of linear algebraic equations regarding the unknown $c$, which can be done, e.g., by Gaussian elimination. Внешняя задача (\ref{outer_prob}) будет являться, вообще говоря, многоэкстремальной, и для ее решения нужно применять методы глобальной оптимизации.

%Структура статьи

\section{Nested optimization scheme}

%Постановка задачи в целом, условие Липшица, многошаговая схема с 2 уровнями
%Внешний уровень - глобальная задача, вложенный уровень - локальная задача

\section{Parallel global search algorithm}

%Постановка задачи глобальной оптмизации
%Редукция размерности - Пеано
%Параллельный алгоритм

\section{Local search algorithms}

%Локальные задачи и локальные методы
%Стандартные библиотеки - будем использовать одну из них

\section{Numerical experiments}

%Сравнение разных локальных методов
%Эффективность распараллеливания 

\section{Conclusion}

%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}

\end{document}
