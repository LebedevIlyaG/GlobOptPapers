\documentclass{cmi}
\usepackage{upgreek}

% custom packages and commands
\usepackage{doi}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
\renewcommand{\doitext}{DOI: }


\begin{document}


\classify{УДК 004.021, 519.6}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}} %to make a number in end of title delete this line

\title{ПАРАЛЛЕЛЬНЫЙ АЛГОРИТМ ДЛЯ ПОЛУЧЕНИЯ РАВНОМЕРНОГО ПРИБЛИЖЕНИЯ
РЕШЕНИЙ МНОЖЕСТВА ЗАДАЧ ГЛОБАЛЬНОЙ ОПТИМИЗАЦИИ С~НЕЛИНЕЙНЫМИ ОГРАНИЧЕНИЯМИ%
\footnote{Статья рекомендована к публикации программным комитетом Международной
научной конференции <<Параллельные вычислительные технологии (ПаВТ) 2020>>.}%
}

\author{\copyright~2020 В.В.~Соврасов, К.A.~Баркалов}

%\author{User1 \\ \href{mailto:me@somewhere.com}{me@somewhere.com}
%	\and User2 \\ email %\href{mailto:someone@somewhere.com}{someone@somewhere.com} }


\address{Нижегородский государственный университет им. Н.И.~Лобачевского\\
  (603022 Нижний Новгород, пр.~Гагарина, д.~23, к.~2)}

\email{E-mail: \href{mailto:sovrasov.vladislav@itmm.unn.ru}{sovrasov.vladislav@itmm.unn.ru},
\href{mailto:konstantin.barkalov@itmm.unn.ru}{konstantin.barkalov@itmm.unn.ru}}

\received{Поступила в редакцию: 27.02.2020}

\maketitle{}


\renewcommand*{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

% оформление аннотации
\begin{abstract}%
  В данной работе рассматривается построение параллельной версии алгоритма глобальной оптимизации, решающего одновременно множество задач с нелинейными ограничениями и получающего при этом равномерные оценки решений на этом множестве. Последнее свойство позволяет наиболее оптимально распределять вычислительные ресурсы, т. к. в процессе работы алгоритма погрешности численного решения во всех задачах убывают примерно с одинаковой скоростью. Алгоритм присваивает приоритет каждой задаче и на каждой итерации производит вычисления целевых функций в нескольких задачах параллельно. При окончании работы метода в произвольный момент времени во всех задачах из решаемой серии будут получены решения сходного качества. Серии из нескольких задач возникают, если задача глобальной оптимизации имеет дискретный параметр или, например, при решении задачи многокритериальной оптимизации методом свертки критериев. Рассматриваемый алгоритм использует отображения типа кривой Пеано для редукции многомерных задач оптимизации к одномерным. Эффективность реализованного алгоритма протестирована на наборах искусственно сгенерированных задач глобальной оптимизации, а также при решении серии задач, полученной в результате скаляризации задачи многокритериальной оптимизации. Также экспериментально подтверждена равномерная сходимость метода.

\keywords{глобальная оптимизация, параллельные вычисления, алгоритмы прямой оптимизации, равномерная сходимость}
\end{abstract}
\flushleft{\textbf{ОБРАЗЕЦ ЦИТИРОВАНИЯ}}
\justifying
\begin{citationplace}
Соврасов~В.В., Баркалов~К.А. Параллельный алгоритм для получения равномерного приближения
решений множества задач глобальной оптимизации с нелинейными ограничениями //
Вестник ЮУрГУ. Серия: Вычислительная математика и информатика. 2020. Т.~X, №~Y. С.~Z1--Z2.
DOI:~10.14529/cmseXXXXXX.
\end{citationplace}

%Верхние колонтитулы
\markboth{В.В.~Соврасов, К.A.~Баркалов}{Параллельный алгоритм для получения равномерного приближения\ldots}
% Заголовки разделов формируются при помощи команд  \section{}, \subsection{}, \subsubsection{}
\section*{Введение}
\label{sec-intro}
Нелинейная глобальная оптимизация невыпуклых функций традиционно считается одной из самых трудных
задач математического программирования. Отыскание глобального минимума функции от нескольких переменных
зачастую оказывается сложнее, чем локальная оптимизация в тысячемерном пространстве. Для последней может оказаться достаточно
применения простейшего метода градиентного спуска, в то время как чтобы \textit{гарантированно} отыскать глобальный оптимум методам
оптимизации приходится накапливать информацию о поведении целевой функции во всей области поиска \cite{Jones2009,Paulavicius2011,Evtushenko2013,Strongin2000}. Решение серии таких задач при ограниченных вычислительных
ресурсах является еще более сложной задачей: помимо поиска глобального экстремума необходимо
распределять вычислительные ресурсы так, чтобы сразу во всех решаемых задачах положение глобального
экстремума было оценено примерно с одинаковым качеством. Обычно серию из \(q\) задач решают либо последовательно, либо
параллельно порциями по \(p\ll q\) задач, где \(p\) --- количество параллельных вычислительных устройств.
Такой подход ведет к тому, что в каждый момент времени до окончания вычислений
остаются задачи, в которых оценка глобального оптимума не получена вообще, в то время, как в задачах из начала
списка оптимум может быть оценен даже с избыточной точностью.

В данной работе рассматривается обобщение ранее разработанного в ННГУ им. Н. И. Лобачевского
параллельного метода глобальной оптимизации для одновременного решения множества задач \cite{BarkalovStrongin2018} на
случай задач с нелинейными ограничениями. Для учета ограничений используется индексная схема \cite{Strongin2000},
позволяющая работать с частично вычислимыми целевым функциями и обладающая экономичностью,
сравнимой с другими подходами \cite{BarkalovLebedev2017}. Эффективность реализованного
алгоритма показана на примере решения множеств задач, сгенерированных специализированным
механизмом, порождающим наборы задач заданной размерности с заданным количеством нелинейных ограничений \cite{GergelBarkalov2019}.
Кроме искусственно сгенерированных задач, рассматриваемый метод протестирован также
на множестве задач, возникающем при решении задачи многокритериальной оптимизации
с нелинейными ограничениями методом свертки критериев \cite{Ehrgott2005}.

Статья имеет следующую структуру. В разделе \ref{sec:problem} рассмотрена
постановка решаемой задачи, далее в разделе \ref{sec:method} приведено описание
параллельного метода оптимизациии. Результаты численных экспериментов, подтверждающие
эффективность рассматриваемого метода приводятся в разделе \ref{sec:exps}.
В заключении приводится краткая сводка результатов, полученных в ходе работы, и
указаны направления дальнейших усилий по улучшению программной реализации рассмотренного метода.

\section{Постановка задачи глобальной оптимизации}
\label{sec:problem}
В рамках данной работы будем рассматривать следующую постановку задачи глобальной
оптимизации: найти глобальный минимум \(N\)-мерной функции \(\varphi(y)\) в гиперинтервале
\(D=\{y\in \mathbf{R}^N:a_i\leqslant x_i\leqslant{b_i}, 1\leqslant{i}\leqslant{N}\}\).
Для построения оценки глобального минимума по конечному количеству вычислений
значения функции требуется, чтобы скорость изменения \(\varphi(y)\) в \(D\) была ограничена.
В качестве такого ограничения как правило принимается условие Липшица.
\begin{equation}
\label{eq:task}
\varphi(y^*)=\min\{\varphi(y):y\in D\}
\end{equation}
\begin{displaymath}
\label{lip}
|\varphi(y_1)-\varphi(y_2)|\leqslant L\Vert y_1-y_2\Vert,y_1,y_2\in D,0<L<\infty
\end{displaymath}

Существуют различные методы, решающие рассмотренную многомерную задачу напрямую \cite{SergeyevKvasov2017, Jones2009},
а также эффективные методы решения одномерных задач \cite{Norkin1992, Strongin2000}. В данной работе рассматривается одномерный метод,
который применяется совместно со схемой редукции размерности.
Классической схемой редукции размерности исходной задачи для алгоритмов глобальной оптимизации является
использование разверток --- кривых, заполняющих пространство \cite{Sergeyev2013}.
\begin{equation}
\label{cube}
\lbrace y\in \mathbf{R}^N:-2^{-1}\leqslant y_i\leqslant 2^{-1},1\leqslant i\leqslant N\rbrace=\{y(x):0\leqslant x\leqslant 1\}
\end{equation}

Отображение вида (\ref{cube}) позволяет свести задачу в многомерном пространстве к решению
одномерной ценой ухудшения ее свойств. В частности, одномерная функция \(\varphi(y(x))\)
является не Липшицевой, а Гёльдеровой:
\begin{displaymath}
\label{holder}
|\varphi(y(x_1))-\varphi(y(x_2))|\leqslant H{|x_1-x_2|}^{\frac{1}{N}},x_1,x_2\in[0;1],
\end{displaymath}
где константа Гельдера \(H\) связана с константой Липшица \(L\) соотношением
\begin{displaymath}
H=4Ld\sqrt{N},d=\max\{b_i-a_i:1\leqslant i\leqslant N\}.
\end{displaymath}

Область \(D\) также может быть задана с помощью функциональных ограничений, что
значительно усложняет задачу.
Постановка задачи глобальной оптимизации в этом случае будет иметь следующий вид:
\begin{equation}
  \label{eq:constrained_problem}
  \varphi(y^*)=\min\{\varphi(y):g_j(y)\leqslant 0, 1\leqslant j\leqslant m\}
\end{equation}
Обозначим \(g_{m+1}(y)=\varphi(y)\). Далее будем предполагать, что все функции \(g_k(y),1\leqslant k \leqslant m+1\)
удовлетворяют условию Липшица в некотором гиперинтервале, включающем \(D\).

Далее будем интересоваться решением серии из \(q\) задач вида (\ref{eq:constrained_problem}):
\begin{equation}
  \label{eq:many_problems}
  \min\left\{\varphi_1(y), y\in D_1 \right\}, \min\left\{\varphi_2(y), y\in D_2\right\},..., \min\left\{\varphi_q(y), y\in D_q\right\}.
\end{equation}

\section{Описание метода глобальной оптимизации}
\label{sec:method}

Принимая во внимание схему редукции размерности (\ref{cube}), будем при описании метода считать, что
требуется найти глобальный минимум функции \(\varphi(x), x\in[0;1]\),
удовлетворяющей условию Гёльдера, при ограничениях \(g_j(x)\), также
удовлетворяющих этому условию на интервале \([0;1]\).

Рассматриваемый индексный алгоритм глобального поиска (ИАГП) для решения
одномерной задачи (\ref{eq:constrained_problem}) предполагает построение последовательности
точек \(x_k\), в которых вычисляются значения минимизируемой функции или ограничений \(z_k = g_s(x_k)\).
Для учета последних используется индексная схема \cite{Strongin2000}. Пусть \(Q_0=[0;1]\). Ограничение, имеющее номер
 \(j\), выполняется во всех точках области
\begin{displaymath}
  Q_j=\left\{x\in [0;1]:g_j(x)\leq 0\right\},
\end{displaymath}
которая называется допустимой для этого ограничения. При этом допустимая область \(D\)
исходной задачи определяется равенством: \(D=\cap _{j=0}^{m}Q_{j}\).
Испытание в точке \(x\in [0;1]\) состоит в последовательном вычислении значений
величин \(g_{1}(x),...,g_{\nu }(x)\), где значение индекса \(\nu\) определяется условиями:
\(x\in Q_{j},0\leqslant j<\nu ,x\notin Q_{\nu }\). Выявление первого нарушенного ограничения
прерывает испытание в точке \(x\). В случае, когда точка \(x\)  допустима, т. е.
\(x\in D\) испытание включает в себя вычисление всех функций задачи. При этом значение
индекса принимается равным величине \(\nu =m+1\). Пара \(\nu =\nu (x),z=g_{\nu }(x)\),
где индекс \(\nu\) лежит в границах \(1\leqslant \nu \leqslant m+1\), называется результатом
испытания в точке \(x\).

Такой подход к проведению испытаний позволяет свести исходную задачу с функциональными
ограничениями к безусловной задаче минимизации разрывной функции:

\begin{displaymath}
  \begin{array}{lr}
    \psi (x^{*})=\min_{x\in [0;1]}\psi (x), \\
    \psi (x)={\begin{cases}g_{\nu }(x)/H_{\nu }&\nu <M\\(g_{M}(x)-g_{M}^{*})/H_{M}&\nu =M\end{cases}}
  \end{array}
\end{displaymath}

Здесь \(M=\max_{}^{}\left\{\nu (x):x\in [0;1]\right\}\), а \(g_{M}^{*}=\min _{}^{}\left\{g_{M}(x):x\in \cap _{i=0}^{M-1}Q_{i}\right\}\).
В силу определения числа \(M\), задача отыскания \(g_{M}^{*}\)
всегда имеет решение, а если \(M=m+1\), то \(g_{M}^{*}=\varphi(x^{*})\).
Дуги функции \(\psi (x)\) гельдеровы на множествах \(\cap _{i=0}^{j}Q_{i},0\leq j\leq M-1\)
с константой 1, а сама \(\psi (x)\) может иметь разрывы первого рода на границах этих множеств.
Несмотря на то, что значения констант Гёльдера \(H_k\) и величина \(g_{M}^{*}\) заранее неизвестны,
они могут быть оценены в процессе решения задачи.

Множество троек \(\{(x_k,\nu_k,z_k)\}, 1\leqslant k\leqslant n\) составляет поисковую информацию,
накопленную методом после проведения \(n\) шагов.

На первой итерации метода испытание проводится в произвольной внутренней точке \(x_1\)
интервала \([0;1]\). Индексы точек 0 и 1 считаются нулевыми, значения \(z\) в
них не определены. Пусть выполнено \(k\geqslant 1\) итераций метода,
в процессе которых были проведены испытания в \(k\) точках \(x_i, 1\leqslant i\leqslant k\).
Тогда точка \(x^{k+1}\) поисковых испытаний следующей \((k+1)\)-ой
итерации определяются в соответствии с правилами:

Шаг 1. Перенумеровать точки множества \(X_k=\{x^1,\dotsc,x^k\}\cup\{0\}\cup\{1\}\),
которое включает в себя граничные точки интервала \([0;1]\), а также точки предшествующих
испытаний, нижними индексами в порядке увеличения значений координаты, т.е.
\begin{displaymath}
0=x_0<x_1<\dotsc<x_{k+1}=1
\end{displaymath}
и сопоставить им значения \(z_{i}=g_{\nu }(x_{i}),\nu =\nu (x_{i}),i={\overline {1,k}}\).

Шаг 2. Для каждого целого числа \(\nu ,1\leqslant \nu \leqslant m+1\) определить соответствующее
ему множество \(I_{\nu }\) нижних индексов точек, в которых вычислялись значения
функций \(g_{\nu }(x)\):
\begin{displaymath}
  I_{\nu }=\{i:\nu (x_{i})=\nu ,1\leqslant i\leqslant k\},1\leq \nu \leqslant m+1,
\end{displaymath}
определить максимальное значение индекса \(M=\max\{\nu (x_{i}),1\leq i\leq k\}\).

Шаг 3. Вычислить текущие оценки для неизвестных констант Гёльдера:
\begin{equation}
  \label{step2}
  \mu _{\nu }=\max\{\frac{|g_{\nu }(x_{i})-g_{\nu }(x_{j})|}{(x_{i}-x_{j})^{\frac{1}{N}}}:i,j\in I_{\nu },i>j\}.
\end{equation}
Если множество \(I_{\nu }\) содержит менее двух элементов или если значение \(\mu _{\nu }\)
оказывается равным нулю, то принять \(\mu _{\nu }=1\).

Шаг 4. Для всех непустых множеств \(I_{\nu },\nu ={\overline {1,M}}\) вычислить оценки
\begin{displaymath}
  z_{\nu }^{*}={\begin{cases}\min\{g_{\nu }(x_{i}):x_{i}\in I_{\nu }\}&\nu =M\\-\varepsilon _{\nu }&\nu <M\end{cases}},
\end{displaymath}
где вектор с неотрицательными координатами \(\varepsilon _{R}=(\varepsilon _{1},..,\varepsilon _{m})\) называется вектором резервов.

Шаг 5. Для каждого интервала \((x_{i-1};x_{i}),1\leqslant i\leqslant k\) вычислить характеристику
\begin{equation}
  \label{step3_1}
  R(i)={\begin{cases}\Delta _{i}+{\frac {(z_{i}-z_{i-1})^{2}}{(r_{\nu }\mu _{\nu })^{2}\Delta _{i}}}-2{\frac {z_{i}+z_{i-1}-2z_{\nu }^{*}}{r_{\nu }\mu _{\nu }}}&\nu =\nu (x_{i})=\nu (x_{i-1})\\2\Delta _{i}-4{\frac {z_{i-1}-z_{\nu }^{*}}{r_{\nu }\mu _{\nu }}}&\nu =\nu (x_{i-1})>\nu (x_{i})\\2\Delta _{i}-4{\frac {z_{i}-z_{\nu }^{*}}{r_{\nu }\mu _{\nu }}}&\nu =\nu (x_{i})>\nu (x_{i-1})\end{cases}}
\end{equation}
где \(\Delta _{i}=(x_{i}-x_{i-1})^{\frac{1}{N}}\). Величины \(r_{\nu }>1,\nu ={\overline {1,m}}\)
являются параметрами алгоритма. От них зависят произведения \(r_{\nu }\mu _{\nu }\),
используемые при вычислении характеристик в качестве оценок неизвестных констант Гёльдера.

Шаг 6. Выбрать наибольшую характеристику:
\begin{equation}
\label{step4}
t=\argmax_{1\leqslant i \leqslant k+1}R(i)
\end{equation}

Шаг 7. Провести очередное испытание в середине интервала \((x_{t-1};x_{t})\),
если индексы его концевых точек не совпадают: \(x^{k+1}={\frac {1}{2}}(x_{t}+x_{t-1})\).
В противном случае провести испытание в точке
\begin{displaymath}
  x^{k+1}={\frac {1}{2}}(x_{t}+x_{t-1})-\operatorname {sgn}(z_{t}-z_{t-1}){\frac {|z_{t}-z_{t-1}|^{n}}{2r_{\nu }\mu _{\nu }^{n}}},\nu =\nu (x_{t})=\nu (x_{t-1}),
\end{displaymath}
а затем увеличить \(k\) на 1.

Алгоритм прекращает работу, если выполняется условие \(\Delta_{t}\leqslant \varepsilon\),
где \(\varepsilon>0\) есть заданная точность. В качестве оценки глобально-оптимального решения выбираются значения
\begin{equation}
\varphi_k^*=\min_{1\leqslant i \leqslant k}\varphi(x_i), x_k^*=\argmin_{1\leqslant i \leqslant k}\varphi(x_i)
\end{equation}

Далее следуя подходу, описанному в \cite{BarkalovStrongin2018}, для решения серии задач (\ref{eq:many_problems}) будем
использовать \(q\) синхронно работающих копий ИАГП с тем лишь отличием, что на шаге 6 при выборе
интервала с наилучшей характеристикой, выбор будет осуществляться из всех интервалов, которые
породили на данный момент \(q\) копий ИАГП. Если наибольшая характеристика соответствует
задаче \(i\), то выполняется шаг 7 в копии метода с номером \(i\), а остальные копии метода простаивают.
Таким образом, на каждой итерации испытание проводится в задаче, наиболее перспективной с точки зрения
характеристик (\ref{step3_1}), что позволяет динамически распределять ресурсы метода между задачами.

В \cite{BarkalovStrongin2018} приведена теория сходимости такого подхода на случай решения задач без ограничений.
При наличии ограничений характеристики интервалов, на концах которых нарушено разное количество ограничений,
вычисляются в соответствии с нижними строчками из (\ref{step3_1}). Нетрудно заметить, что и в этом случае
величины характеристик нормированы, а в случае точных оценок \(z_{\nu }^{*}\) и \(\mu _{\nu }\) их
масштаб не зависит от целевой функции и ограничений конкретной задачи. Таким образом, рассуждения
из \cite{BarkalovStrongin2018} можно провести и в случае использования индексной схемы.

Параллельная модификация метода не отличается от рассматриваемой в \cite{BarkalovStrongin2018}
и заключается в выборе \(p\) интервалов на шаге 6 и выполнения \(p\) испытаний параллельно
на следующем шаге. При этом все ресурсы метода в рамках итерации могут быть направлены как на одну, так и
на \(l\leqslant p\) задач одновременно (в зависимости от того, какой из задач принадлежат выбранные методам интервалы).

\section{Результаты численных экспериментов}
\label{sec:exps}

Использование сгенерированных некоторыми случайными механизмами
наборов тестовых задач с известными решениями является одним из общепринятых подходов
к сравнению алгоритмов оптимизации \cite{Beiranvand2017}. В данной работе
будем использовать два генератора тестовых задач, порождающих задачи различной природы \cite{grishaginClass, Gaviano2003}.
Эти генераторы порождают задачи без нелинейных ограничений, поэтому в дополнение к ним использована
система GCGen\footnote{Исходный код системы доступен по ссылке https://github.com/UNN-ITMM-Software/GCGen} \cite{GergelBarkalov2019}, позволяющая генерировать задачи с ограничениями на основе произвольных нелинейных
функций.

Вместе с системой GCGen распространяются примеры ее использования и построения
наборов задач, каждая из которых состоит из целевой функции и двух ограничений,
порожденных генератором \(F_{GR}\) \cite{grishaginClass} или GKLS \cite{Gaviano2003}.

% Обозначим набор из 50 задач, полученных с помощью GCGen и первого генератора из \cite{grishaginClass} как \(F_{GR}^C\). Механизм построения функций \(F_{GR}\) не предусматривает контроля за размерностью (она равна 2) и количеством локальных оптимумов, однако известно, что порождаемые функции
% являются существенно многоэкстремальными.

Генератор GKLS \cite{Gaviano2003} позволяет получать функции заданной размерности и с заданным количеством экстремумов.
В сочетании с GCGen были порождены два множества по 100 задач размерности 2 и 3. Каждая из задач имеет два ограничения.
Также с целью демонстрации того, что свойства метода сохраняются при существенно разных свойствах задач
был сгенерирован смешанный класс, состоящий из 50 задач с двухмерными функциями GKLS и 50 задач с функциями \(F_{GR}\).
На \figref{pic/2.png} и \figref{pic/4.png} представлены примеры линий уровня рассматриваемых задач. Допустимая область закрашена.

\fig{scale=0.3}{pic/2.png}{Пример линий уровня задачи с решением внутри допустимой области}
\fig{scale=0.3}{pic/4.png}{Пример линий уровня задачи с решением на границе допустимой области}

Будем считать, что тестовая задача решена, если метод оптимизации провел очередное испытание \(y^k\) в
\(\delta\)-окрестности глобального минимума \(y^*\), т.е. $\left\|y^k-
y^*\right\|\leqslant \delta = 0.01\left\|b-a\right\|$, где \(a\) и \(b\) --- левая и правая границы гиперкуба из (\ref{eq:task}).
Если указанное соотношение не выполнено до истечения лимита на количество испытаний, то задача считается нерешенной.

%В качестве характеристик метода оптимизации на каждом из классов будем рассматривать среднее число
%испытаний, затраченное для решения одной задачи, и количество решенных задач. Чем меньше число испытаний, тем быстрее метод сходится
%к решению, а значит и меньше обращается к потенциально трудоемкой процедуре вычислений целевой функции.
При оценке качества метода и его реализации кроме ускорения от распараллеливания и времени выполнения также будем принимать во внимание среднее максимальное расстояния (в смысле \(l_{\inf}\)-нормы) текущей оценки оптимума до его реального положения,
вычисленное на множестве задач (\ref{eq:many_problems}): \(D_{avg}\) и \(D_{max}\). Динамика этих величин в процессе оптимизации
показывает, насколько равномерно метод распределяет ресурсы между задачами.

Реализация параллельного метода была выполнена на языке C++ с использованием технологии OpenMP
для распареллеливания процесса проведения испытаний на общей памяти. Все вычислительные
эксперименты проведены на машине со следующей конфигурацией: Intel Core i7-7800X, 64GB RAM, Unubtu 16.04 ОS, GCC 5.5 compiler.

\subsection{Результаты решения сгенерированных задач}

Результаты решения тестовых задач последовательной и параллельной версией модифицированного ИАГП
для решения множества задач представлены в \tabref{tab:speedup}. Для всех двухмерных классов задач параметр \(r=4.7\).
В случае трехмерных задач \(r=4.7,\: \varepsilon_\nu=0.1\).
Во всех экспериментах в целевые функции и ограничения была внесена дополнительная вычислительная нагрузка так,
чтобы время одного обращения к функции задачи было равно примерно 1мс.

Из таблицы видно, что ускорение по итерациям \(S_i\) растет линейно с увеличением числа потоков \(p\),
в то время, как ускорение по времени \(S_p\) увеличивается не так быстро, что говорит о неидеальной
реализации алгоритма. Увеличить реальное ускорение, верхней границей для которого является
\(S_i\), возможно путем оптимизации взаимодействий между копиями ИАГП и это планируется сделать в ходе будущей работы.

\tab{tab:speedup}{Результаты экспериментов на наборах синтетических задач}{
  \begin{tabular}{c|c|cccc}
    %\cline{1-8}\noalign{\smallskip}
    Класс задач & \textit{p} & Количество итераций & Время, с & \(S_i\) & \(S_t\)   \\
    %s\noalign{\smallskip} \cline{4-5} \cline{7-8}  \noalign{\smallskip}
    \hline
    GKLS \& \(F_{GR}\) based \
      & 1 & 51434 & 90.20 & -    & - \\
      & 2 & 25698 & 56.96 & 2.00 & 1.58 \\
      & 4 & 13015 & 36.67 & 3.95 & 2.46 \\
      & 6 & 8332  & 26.85 & 6.17 & 3.36 \\
    \hline
    GKLS based 2d \
      & 1 & 59066 & 97.53 & -    & - \\
      & 2 & 29060 & 60.56 & 2.04 & 1.61 \\
      & 4 & 14266 & 38.92 & 4.14 & 2.51 \\
      & 6 & 9436  & 29.53 & 6.26 & 3.30 \\
    \hline
    GKLS based 3d \
      & 1 & 782544 & 1117.55 & -    & - \\
      & 2 & 397565 & 752.92  & 1.97 & 1.48 \\
      & 4 & 208073 & 526.67  & 3.76 & 2.12 \\
      & 6 & 142089 & 445.45  & 5.50 & 2.51 \\
    \hline
  \end{tabular}
}

Для того, чтобы показать равномерную сходимость все тестовые задачи были также решены ИАГП
в режиме решения отдельных задач. На \figref{pic/mixed_2d_max.pdf} и \figref{pic/mixed_2d_avg.pdf} указаны графики величин
средних и максимальных расстояний от реальных оптимумов до их текущих оценок при решении
серии из задач, порожденных двумя разными генераторами, по отдельности (сплошная кривая) и совместно (пунктирная кривая).
Не смотря на значительную разницу в структуре задач, модифицированный ИАГП
гораздо быстрее уменьшает как максимальное, так и среднее отклонение оценок от оптимумов.
Это говорит о наличии равномерной сходимости по всему множеству совместно решаемых задач.
При этом в случае последовательного решения задач величина \(D_{max}\) имеет наибольшее значение вплоть
до решения последней задачи.

\fig{scale=0.5}{pic/mixed_2d_max.pdf}{Динамика величины \(D_{max}\) в процессе решения множества двухмерных задач,
порождённых двумя разными генераторами GKLS и \(F_{GR}\)}
\fig{scale=0.5}{pic/mixed_2d_avg.pdf}{Динамика величины \(D_{avg}\) в процессе решения множества двухмерных задач,
порождённых двумя разными генераторами GKLS и \(F_{GR}\)}

\subsection{Пример решения многокритериальной задачи}

Для демонстрации эффективности подхода к балансировке нагрузки рассмотрим пример,
в котором множество задач вида (\ref{eq:many_problems}) порождено в результате скаляризации
многокритериальной задачи оптимизации с ограничениями.

Рассмотрим тестовую задачу, предложенную в \cite{BinhKorn1999}:
\begin{equation}
  \label{eq:mco_probem}
  \begin{array}{l}
      Minimize \left \{
      \begin{array}{l}
        f_1(y) = 4 y_1^2 + 4 y_2^2 \\
        f_2(y) = (y_1-5)^2 + (y_2-5)^2 \\
      \end{array}
      \right .
      y_1\in [-1;2],y_2\in [-2;1]
      \\s.t.
      \\
      \left \{
      \begin{array}{l}
        g_1(y) = (y_1 - 5)^2 + y_2^2 - 25 \leqslant 0 \\
        g_2(y) = -(y_1 - 8)^2 - (y_2 + 3)^2 + 7.7 \leqslant 0\\
      \end{array}
      \right .
  \end{array}
\end{equation}

Будем использовать свертку Гермейера для скаляризации задачи (\ref{eq:mco_probem}).
После свертки скалярная целевая функция имеет вид:
\begin{equation}
  \varphi(y,\lambda_1,\lambda_2)=\max\{\lambda_1 f_1(y), \lambda_2 f_2(y)\},
\end{equation}
где \(\lambda_1,\lambda_2\in[0,1],\: \lambda_1+\lambda_2=1\). Перебирая все возможные
коэффициенты свертки, можно найти все множество парето-оптимальных решений в
задаче (\ref{eq:mco_probem}). Для численного построения множества Парето выберем
100 наборов коэффициентов \((\lambda_1,\lambda_2)\) таких, что
\(\lambda_1^i=i h,\: \lambda_2^i=1-\lambda_1^i,\: h=10^{-2},i=\overline{1, 100}\).

В качестве ограничения на вычислительные ресурсы был выбран лимит в 2500 испытаний.
Множество вспомогательных скалярных задач решалось двумя способами:
\begin{itemize}
  \item каждая задача решается отдельно с помощью ИАГП с установленным лимитом в
  25 испытаний. Таким образом, вычислительные ресурсы равномерно распределены между задачами;
  \item все задачи решаются одновременно с помощью обобщенного ИАГП с установленным лимитом в
  2500 испытаний.
\end{itemize}
В обоих случаях параметр \(r=4\).

\fig{scale=0.5}{pic/single_mco.pdf}{Численная оценки множества Парето в задаче (\ref{eq:mco_probem}),
                                    полученная при раздельном решении задач}
\fig{scale=0.5}{pic/multi_mco.pdf}{Численная оценки множества Парето в задаче (\ref{eq:mco_probem}),
                                   полученная при совместном решении задач}

На \figref{pic/single_mco.pdf} и \figref{pic/multi_mco.pdf}
представлены графики решений, полученных каждым из методов.
Все графики качественно совпадают с указанным в \cite{BinhKorn1999} (авторы не предоставили другой
информации и решениях для сравнения). Видно, что на \figref{pic/single_mco.pdf}
кривая Парето имеет вогнутости, что не соответствует решению, указанному в \cite{BinhKorn1999} и
означает нехватку ресурсов для решения некоторых из вспомогательных задач.
Для оценки качества решения также был вычислен показатель \(Spacing(SP)\) \cite{RiquelmeLucken2015},
характеризующий плотность точек аппроксимации множества Парето.
\begin{displaymath}
  SP(S)=\sqrt{\frac{1}{|S|-1} \sum_{i=1}^{|S|} (\overline{d}-d_i)},
  \:\overline{d}=mean\{d_i\},\:d_i=\min_{s_i,s_j\in S:s_i\ne s_j}||F(s_i)-F(s_j)||_1,\: F=(f_1,f_2)
\end{displaymath}
В случае отдельного решения задач \(SP_{single}=0.984\), а при решении задач методом с балансировкой нагрузки
\(SP_{multi}=0.749\), что говорит о более качественном приближении решения.

\section*{Заключение}

В ходе работы была реализована поддержка нелинейных ограничений в алгоритме, решающeм
множество задач глобальной оптимизации. Проведены численные эксперименты, демонстрирующие
преимущество рассматриваемого подхода над решением задач по отдельности. Показана эффективность
совместного решения множества задач на примере решения многокритериальной задачи с
нелинейными ограничениями.

В ходе дальнейшей работы планируется улучшить текущую реализацию алгоритма,
сократив расходы на содержание поисковой информации для множества задач и тем самым улучшив
показатели параллельного ускорения по времени. Также планируется реализовать версию
рассматриваемого алгоритма, работающего на распределенной памяти по схеме, описанной в \cite{BarkalovLebedev2017_2}.

\vspace{1em}
{\it Исследование выполнено при поддержке РНФ, проект №\,16-11-10150.}

\begin{biblio}
  \bibitem{BarkalovLebedev2017}
  Barkalov K., Lebedev I.
  \newblock Comparing two approaches for solving constrained global optimization
    problems
  \newblock // Learning and Intelligent Optimization. Springer International
    Publishing, Cham. 2017. P. 301--306.
  \newblock \doi{10.1007/978-3-319-69404-7_22}

  \bibitem{BarkalovLebedev2017_2}
  Barkalov K., Lebedev I.
  \newblock Parallel algorithm for solving constrained global optimization
    problems
  \newblock // Parallel Computing Technologies. Springer International
    Publishing, Cham. 2017. P. 396--404.
  \newblock \doi{10.1007/978-3-319-62932-2_38}

  \bibitem{BarkalovStrongin2018}
  Barkalov K., Strongin R.
  \newblock Solving a set of global optimization problems by the parallel
    technique with uniform convergence //
  \newblock Journal of Global Optimization. 2018. Vol. 71, no. 1. P. 21--36.
  \newblock \doi{10.1007/s10898-017-0555-4}

  \bibitem{Beiranvand2017}
  Beiranvand V., Hare W., Lucet Y.
  \newblock Best practices for comparing optimization algorithms //
  \newblock Optimization and Engineering. 2017. Vol. 18, no. 4. P. 815--848.
  \newblock \doi{10.1007/s11081-017-9366-1}

  \bibitem{Ehrgott2005}
  Ehrgott M.
  \newblock Multicriteria Optimization.
  \newblock Springer-Verlag, Berlin, Heidelberg. 2005.
  \newblock \doi{10.1007/3-540-27659-9}

  \bibitem{Evtushenko2013}
  Evtushenko Y., Posypkin M.
  \newblock A deterministic approach to global box-constrained optimization //
  \newblock Optimization Letters. 2013. Vol. 7 P. 819--829.
  \newblock \doi{10.1007/s11590-012-0452-1}

  \bibitem{Gaviano2003}
  {Gaviano M., Kvasov D.E., Lera D., Sergeev Ya.D.}
  \newblock Software for generation of classes of test functions with known local
    and global minima for global optimization //
  \newblock ACM Transactions on Mathematical Software. 2003. Vol. 29, no. 4. P.
    469--480.
  \newblock \doi{10.1145/962437.962444}

  \bibitem{GergelBarkalov2019}
  Gergel V., Barkalov K., Lebedev I., Rachinskaya M., Sysoyev A.
  \newblock A flexible generator of constrained global optimization test
    problems // Proceedings LeGO - 14th International Global Optimization Workshop. 8–21 September 2018, Leiden, The Netherlands. Vol. 2070, no. 1. P. 020009.
  \newblock \doi{10.1063/1.5089976}

  \bibitem{Jones2009}
  {Jones D.R.}
  \newblock The direct global optimization algorithm
  \newblock // The Encyclopedia of Optimization. Springer, Heidelberg. 2009. P.
    725--735.
  \newblock \doi{10.1007/978-0-387-74759-0_128}

  \bibitem{Paulavicius2011}
  {Paulavivcius R., Zilinskas J., Grothey A.}
  \newblock Parallel branch and bound for global optimization with combination of
    lipschitz bounds //
  \newblock Optimization Methods and Software. 1997. Vol. 26, no. 3. P. 487--498.
  \newblock \doi{10.1080/10556788.2010.551537}

  \bibitem{RiquelmeLucken2015}
  {Riquelme} N., {Von Lucken} C., {Baran} B.
  \newblock Performance metrics in multi-objective optimization
  \newblock // 2015 Latin American Computing Conference (CLEI). 2015. P. 1--11.
  \newblock \doi{10.1109/clei.2015.7360024}

  \bibitem{SergeyevKvasov2017}
  Sergeyev Y., Kvasov D. Deterministic Global Optimization. 2017. 136 p.
  \newblock \doi{10.1007/978-1-4939-7199-2}

  \bibitem{Sergeyev2013}
  {Sergeyev Y.D., Strongin R.G., Lera D.} Introduction to global optimization
    exploiting space-filling curves.
  \newblock Springer Briefs in Optimization, Springer, New York. 2013.
  \newblock \doi{10.1007/978-1-4614-8042-6}

  \bibitem{Strongin2000}
  {Strongin R.G., Sergeyev Ya.D.} Global optimization with non-convex
    constraints. Sequential and parallel algorithms.
  \newblock Kluwer Academic Publishers, Dordrecht. 2000. 704 p.
  \newblock \doi{10.1007/978-1-4615-4677-1}

  \bibitem{BinhKorn1999}
  To T.B., Korn B.
  \newblock MOBES: A multiobjective evolution strategy for constrained optimization problems
  // Proceedings of the third international conference on genetic algorithms (Mendel 97). 1997. P. 176--182.

  \bibitem{grishaginClass}
  {Гришагин В.А.}
  \newblock {Операционные характеристики
    некоторых алгоритмов глобального поиска}
  \newblock // Проблемы статистической
    оптимизации. Зинатне, Рига. 1978. №7. С. 198–206.

  \bibitem{Norkin1992}
  {Норкин В. И.}
  \newblock О методе Пиявского для решения общей
    задачи глобальной оптимизации //
  \newblock Журнал вычислительной математики и математической физики. 1992. № 32 С. 992--1006.
\end{biblio}

% Сведения об авторах (Ф.И.О.(полностью), степень, звание, кафедра (отдел), вуз, e-mail)
Владислав Валерьевич Соврасов, аспирант, Кафедра математического обеспечения и
суперкомпьютерных технологий, Нижегородский государственный университет им. Н.И.~Лобачевского (Нижний Новгород, Российская Федерация)

Константин александрович Баркалов, к.ф.м.н., доцент, Кафедра математического обеспечения и
суперкомпьютерных технологий, Нижегородский государственный университет им. Н.И.~Лобачевского (Нижний Новгород, Российская Федерация)


\newpage
\classify{} % Код MSC не нужен
%\classify{MSC 00000} % Коды MSC см. http://www.ams.org/msc/

\title{\uppercase{Parallel global optimization algorithm for~obtaining uniform convergence
when~simultaneously solving a~set of~global optimization problems}}

\author{\copyright~2020 V.V.~Sovrasov, K.A.~Barkalov}

\address{Lobachevsky State University of Nizhni Novgorod, Russia (pr. Gagarina 23(2), Nizhni Novgorod, 603022 Russia)}

\email{E-mail: \href{mailto:sovrasov.vladislav@itmm.unn.ru}{sovrasov.vladislav@itmm.unn.ru},
\href{mailto:konstantin.barkalov@itmm.unn.ru}{konstantin.barkalov@itmm.unn.ru}}

\received{Received: 27.02.2020}

\maketitle{}

\begin{abstract}%

In this work building of a parallel version of a method simultaneously solving a set of constrained global optimization problems is considered. This method converges uniformly to solutions of all the problems. That allows the method to arrange computational resources in an optimal way, since uniform convergence guarantees approximately equal precision of numerical solutions at the whole set of problems at the each iteration of optimization. The algorithm assigns a priority to each problem, and then at the each iteration carries out calculation of objective functions and constraints in several problems in parallel. If the method stops at any arbitrary moment, in all the problems numerical sulutions with the similar accuracy will be obtained. Sets of similar global optimization problems appear for an instance after scalarization of multi-objective problems or when a global optimization problem has a discrete parameter which takes a finite number of possible values. The considered method uses Peano-type curves to transform multidimensional problems into univariate ones. Efficiency of the implemented parallel algorithm is evaluated on several sets of synthetically generated constrained global optimization problems and on a scalarized multi-objective problem. Also the uniform convergence was confirmed numerically by validation quality of intermediate solutions during the optimization process.

  \keywordsen{global optimization, parallel computations, derivative-free optimization, uniform convergence}
\end{abstract}

\begin{flushleft}
  \textbf{FOR CITATION}
\end{flushleft}
\justifying

\begin{citationplace}
Sovrasov~V.V., Barkalov~K.A. Parallel global optimization algorithm for obtaining uniform convergence
when simultaneously solving a set of global optimization problems. Bulletin of the South Ural State University. Series: Computational Mathematics and Software Engineering. 2020. Vol.~X, no.~Y. P.~Z1--Z2. (in Russian) DOI: 10.14529/cmseXXXXXX.
\end{citationplace}

\begin{biblio_lat}
%\itemsep -2pt

\bibitem{BarkalovLebedev2017}
Barkalov K., Lebedev I.
\newblock Comparing two approaches for solving constrained global optimization
  problems.
\newblock Learning and Intelligent Optimization. Springer International
  Publishing, Cham. 2017. P. 301--306.
\newblock \doi{10.1007/978-3-319-69404-7_22}

\bibitem{BarkalovLebedev2017_2}
Barkalov K., Lebedev I.
\newblock Parallel algorithm for solving constrained global optimization
  problems.
\newblock Parallel Computing Technologies. Springer International
  Publishing, Cham. 2017. P. 396--404.
\newblock \doi{10.1007/978-3-319-62932-2_38}

\bibitem{BarkalovStrongin2018}
Barkalov K., Strongin R.
\newblock Solving a set of global optimization problems by the parallel
  technique with uniform convergence.
\newblock Journal of Global Optimization. 2018. Vol. 71, No. 1. P. 21--36.
\newblock \doi{10.1007/s10898-017-0555-4}

\bibitem{Beiranvand2017}
Beiranvand V., Hare W., Lucet Y.
\newblock Best practices for comparing optimization algorithms.
\newblock Optimization and Engineering. 2017. Vol. 18, No. 4. P. 815--848.
\newblock \doi{10.1007/s11081-017-9366-1}

\bibitem{Ehrgott2005}
Ehrgott M.
\newblock Multicriteria Optimization.
\newblock Springer-Verlag, Berlin, Heidelberg. 2005.
\newblock \doi{10.1007/3-540-27659-9}

\bibitem{Evtushenko2013}
Evtushenko Y., Posypkin M.
\newblock A deterministic approach to global box-constrained optimization.
\newblock Optimization Letters 2013. Vol. 7 P. 819--829.
\newblock \doi{10.1007/s11590-012-0452-1}

\bibitem{Gaviano2003}
{Gaviano M., Kvasov D.E., Lera D., Sergeev Ya.D.}
\newblock Software for generation of classes of test functions with known local
  and global minima for global optimization.
\newblock ACM Transactions on Mathematical Software. 2003. Vol. 29, No. 4. P.
  469--480.
\newblock \doi{10.1145/962437.962444}

\bibitem{GergelBarkalov2019}
Gergel V., Barkalov K., Lebedev I., Rachinskaya M., Sysoyev A.
\newblock A flexible generator of constrained global optimization test
  problems // Proceedings LeGO - 14th International Global Optimization Workshop. 8–21 September 2018, Leiden, The Netherlands. Vol. 2070, no. 1. P. 020009.
\newblock \doi{10.1063/1.5089976}

\bibitem{Jones2009}
{Jones D.R.}
\newblock The direct global optimization algorithm.
\newblock The Encyclopedia of Optimization. Springer, Heidelberg. 2009. P.
  725--735.
\newblock \doi{10.1007/978-0-387-74759-0_128}

\bibitem{Paulavicius2011}
{Paulavivcius R., Zilinskas J., Grothey A.}
\newblock Parallel branch and bound for global optimization with combination of
  lipschitz bounds.
\newblock Optimization Methods and Software. 1997. Vol. 26, No. 3. P. 487--498.
\newblock \doi{10.1080/10556788.2010.551537}

\bibitem{RiquelmeLucken2015}
{Riquelme} N., {Von Lucken} C., {Baran} B.
\newblock Performance metrics in multi-objective optimization.
\newblock2015 Latin American Computing Conference (CLEI). 2015. P. 1--11.
\newblock \doi{10.1109/clei.2015.7360024}

\bibitem{SergeyevKvasov2017}
Sergeyev Y., Kvasov D. Deterministic Global Optimization. 2017. 136 p.
\newblock \doi{10.1007/978-1-4939-7199-2}

\bibitem{Sergeyev2013}
{Sergeyev Y.D., Strongin R.G., Lera D.} Introduction to global optimization
  exploiting space-filling curves.
\newblock Springer Briefs in Optimization, Springer, New York. 2013.
\newblock \doi{10.1007/978-1-4614-8042-6}

\bibitem{Strongin2000}
{Strongin R.G., Sergeyev Ya.D.} Global optimization with non-convex
  constraints. Sequential and parallel algorithms.
\newblock Kluwer Academic Publishers, Dordrecht. 2000. 704 p.
\newblock \doi{10.1007/978-1-4615-4677-1}

\bibitem{BinhKorn1999}
To T.B., Korn B.
\newblock MOBES: A multiobjective evolution strategy for constrained optimization problems //
Proceedings of the third international conference on genetic algorithms (Mendel 97). 1997. P. 176--182.

\bibitem{grishaginClass}
{Grishagin V.}
\newblock {Operating characteristics of some global search algorithms.}
\newblock Problems of Stochastic Search. Zinatne, Riga. 1978. No. 7. P. 198–206. (in Russian)

\bibitem{Norkin1992}
{Norkin V. I.}
\newblock Towards Pijavskyj's method for solving common global optimization problem.
\newblock Computational Mathematics and Mathematical Physics. 1992. Vol. 32, No. 7. P. 992--1006. (in Russian)

\end{biblio_lat}

\end{document}
