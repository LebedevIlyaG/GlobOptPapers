\documentclass{aip-cp}


\usepackage[numbers]{natbib}
\usepackage{rotating}
\usepackage{graphicx}

%% ДЛЯ РУССКОГО ТЕКСТА закомментировать потом!
\usepackage{inputenc}
\usepackage[T2A,T1]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{cmap}
%%


% Document starts
\begin{document}

% Title portion
\title{Использование методов машинного обучения для разделения параметров в задачах глобальной оптимизации большой размерности}

\author[aff1]{Konstantin Barkalov\corref{cor1}}
\author[aff1]{Marina Usova}

\affil[aff1]{Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia}
\corresp[cor1]{Corresponding author: konstantin.barkalov@itmm.unn.ru}

\maketitle

\begin{abstract}
В работе представлены результаты исследования подхода к решению задач оптимизации, в которых зависимость от разных групп параметров носит разный характер.
Предложена схема выделения параметров задачи, которые оказывают локальное влияние на целевую функцию, что позволяет решать существенно многомерные задачи с использованием nested optimization scheme. При этом на разных уровнях рекурсии используются разные оптимизационные алгоритмы. Предложенный подход показал свою работоспособность при решении нескольких серий тестовых задач.

\end{abstract}

% Head 1
\section{INTRODUCTION}

В настоящее время методы глобальной оптимизации используются для решения широкого круга задач, возникающих в различных областях науки и техники. 
Например, традиционной сферой применения методов глобальной оптимизации является идентификация значений параметров математических моделей по данным экспериментов. К их числу относятся, например, inverse problems of chemical kinetics \cite{Gubaydullin2021}.  В задачах такого вида требуется провести поиск значений неизвестных параметров модели, при которых результаты моделирования наиболее близки к результатам, полученным экспериментально.

Число параметров, которые требуется идентифицировать подобным образом, для задач химической кинетики может составлять десятки и сотни. Использование детерминированных методов глобальной оптимизации для решения задачах такой размерности крайне ограничено из-за чрезвычайно больших вычислительных затрат на покрытие search domain точками испытаний. Это остается справедливым даже в случае использования эффективных алгоритмов (например, \cite{Paulavicius2011,Evtushenko2009,Jones2009}), строящих существенно неравномерные покрытия. 

Многие обратные задачи характеризуются тем, что зависимость от разных групп параметров носит разный характер. Например, от одной группы параметров зависимость может быть близка к линейной, тогда как по второй группе зависимость может носить сложный многоэкстремальный характер.

В этом случае решение задачи можно организовать по схеме recursive optimization. Решение  многоэкстремальной подзадачи, для которой требуется использовать сложные алгоритмы глобальной оптимизации, будет проводиться на верхнем уровне рекурсии.
Унимодальные подзадачи (каждая из которых соответствует фиксированному набору значений первой части параметров) будут решаться на нижнем уровне. Для их решения можно применять обладающие быстрой сходимостью методы локальной оптимизации.

Однако заранее указать разделение на группы параметров с разным характером поведения целевой функции, как правило, не представляется возможным, т.к. целевая функция в обратных задачах задается как черный ящик. 
Таким образом, актуальной является разработка алгоритмов минимизации существенно многомерных функций, в которых учитывается разный характер зависимости целевой функции от разных групп параметров. 
В данной работе предложен конкретный механизм такого разделения, основанный на анализе регрессионной модели функции по каждой переменной в отдельности.
В качестве примеров, подтверждающих работоспособность предложенной схемы, решены как тестовые, так и прикладные задачи.

\section{ОПТИМИЗАЦИОННЫЕ АЛГОРИТМЫ И РЕДУКИЦЯ РАЗМЕРНОСТИ}

Рассмотрим задачу оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y): y\in D\right\}}, \nonumber \\
& D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Будем предполагать, что функция $\varphi(y)$ является многоэкстремальной и удовлетворяет Lipschitz condition
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty,
\]
with the constant $L$ unknown a priori.
Одновременно с этим будет предполагать, что зависимость целевой функции от $N_1$ параметров является многоэкстремальной, а от оставшихся $N_2 = N - N_1$ параметров зависимость являеся близкой к линейной. При этом разделение  параметров является неизвестным. В данной работе предлагается подход к разделению параметров задачи на указанные две группы, конкретный механизм описан в следуюем разделе. 

Учет подобной особенности задач может существенно снизить вычислительную сложность поиска оптимума. В самом деле, в соответствии с известной схемой рекурсивной оптимизации \cite{Carr} решение исходной задачи можно проводить следующим образом:
\begin{equation}\label{global_problem}
\varphi(y^*) = \min_{y\in D} \varphi (y) = \min_{y^1\in D_1} f(y^1), \; D_1=\left\{y\in R^{N_1}: a_i\leq y_i \leq b_i, \; 1\leq i \leq N_1\right\},
\end{equation}
где 
\begin{equation}\label{local_problem}
f(y^1) = \min_{ y^2 \in D_2} \varphi(y^1,y^2), \; D_2=\left\{y\in R^{N_2}: a_i\leq y_i \leq b_i, \; N_1+1\leq i \leq N\right\}.
\end{equation}
В соответствии с (\ref{global_problem}) вычисление одного значения функции $\varphi^1 (y^1)$ (данный процесс будем называть search trial) подразумевает решение унимодальной задачи (\ref{local_problem}), которое может быть выполнено одним из методов локальной оптимизации. Эффективность и проработанность методов локальной оптимизации позволяет решать подзадачи (\ref{local_problem}) с точностью, значительно превышающей точность методов глобальной оптимизации. Соответственно, можно рассматривать задачу (\ref{global_problem}) как задачу глобальной оптимизации, в которой значение функции может быть вычислено с высокой точностью, но данная операция является трудоемкой. 

В проведенном исследовании для решения задач глобальной оптимизации (\ref{global_problem}) мы использовали global search algorithm в сочетании со схемой редукции размерности на основе кривых Пеано \cite{Sergeyev2013,Lera2021}. Данный алгоритм зарекомендовал себя как мощный инструмент для решения сложных многоэкстремальных задач \cite{Kvasov2013,Kalyulin2017,Cavoretto2021}.
Для решения локальной задачи (\ref{local_problem}) мы использовали Hooke-Jeevse pattern search method (see, e.g., \cite{Kelley}). В отличие от методов типа BFGS (see, e.g., \cite{Nocedal}), данный алгоритм не требует вычисления градиента и хорошо работает в условиях наличия вычислительных ошибок в значениях функций, что является типичным при решении прикладных задач.

\section{РАЗДЕЛЕНИЕ ПАРАМЕТРОВ ЗАДАЧИ}

Для разделения параметров задачи на локальные и глобальные предлагается следующая последовательность действий.

Этап 1. Зафиксировать опорную точку $\overline{y} = (\overline{y}_1, \overline{y}_2,...,\overline{y}_N)$ для целевой функции $\varphi(y)$. Выбор точки осуществляется одним из следующих способов:
\begin{enumerate}
\item точка фиксируется исходя из физического смысла решаемой прикладной задачи;
\item в качестве фиксированной точки используется решение, найденное локальным методом за $K$ итераций (нашем исследовании использовался Hooke-Jeevse pattern search method).
\item в качестве фиксированной точки используется решение, найденное глобальным методом за $K$ итераций (нашем исследовании использовался global search algorithm).
\end{enumerate}

Этап 2. Для всех компонент $y_i, \; 1\leq i \leq N$, провести исследование на локальность:
\begin{enumerate}
\item вычислить $P+1$ значение целевой функции $\varphi(y)$ в точках $z_i^j = (\overline{y}_1,...,\overline{y}_{i-1},y_i^j,\overline{y}_{i+1},...,\overline{y}_N)$, где
$y_i^j =  a_i + jh, \; h=(b_i-a_i)/P, \; 0\leq j \leq P$;
\item сформировать множество $Q$, представляющее собой набор из $P+1$ пары вида $\left\{y_i^j, \varphi(z_i^j)\right\} $ 
%\[
%Q = \left\{ (\overline{y}^k_i,  \varphi({\overline{y}^k})): \overline{y}^k = (\overline{y}_1, \overline{y}_2, … , \overline{y}^k_i, … ,\overline{y}_N), \overline{y}^k_i \sim U[a_i, b_i], 1\leq i \leq N, 1\leq k \leq P \right\};
%\]
\item построить регрессионную модель заданной степени $deg$ на основе данных из $Q$;
\item вычислить оценку $R^2$ и провести классификацию i-й переменной:
\begin{itemize}
\item если $R^2$ больше заданного порога $T$, то добавить переменную к множеству локальных переменных,
\item иначе добавить переменную к множеству глобальных переменных.
\end{itemize}
\end{enumerate}

Этап 3. Если локальные переменные были обнаружены, то продолжить решение задачи с использованием nested optimization scheme, на верхнем уровне которой будет использоваться алгоритм глобального поиска, а на нижнем уровне -- метод локальной оптимизации. В противном случае запустить алгоритм глобального поиска, считая все переменные глобальными.


%
%Комментарий 1. В основе проводимой регрессии лежит метод наименьших квадратов (МНК), использующий следующие шаги для выбора подходящй модели:
%\begin{enumerate}
%\item для очередной модели:
%\begin{itemize}
%\item вычислить ошибки между рассматриваемой моделью и точками множества данных,
%\item возвести в квадрат каждую ошибку,
%\item просуммировать все квадраты ошибок,
%\end{itemize}
%\item выбрать модель, в которой сумма квадратов ошибок минимальна.
%\end{enumerate}

В основе проводимой регрессии лежит метод наименьших квадратов (МНК). Оценка качества построенной модели на шаге 2 в пункте 4 производится с помощью adjusted coefficient of determination $R^2_a$. Указанный коэффициент характеризует схожесть построенной модели регрессии с целевой функцией в зафиксированном сечении. Значение $R^2_a$, как правило, находится между 0 и 1. Результат, близкий к 1, показывает, что точность модели высока.

Значаения  $K$, $deg$ и $T$ являются параметрами метода и устанавливаются исследователем в соотвествии со свойствами решаемой задачей. В алгоритме возможно использование степени полинома регрессии $deg = 1$ или $deg = 2$, что позволяет построить модель линейной или квадратичной регрессии соответственно.
%Для контроля определения "функция достаточно схожа с локальной" используется параметр метода $T$.

\section{NUMERICAL EXPERIMENTS}

В первом наборе тестовых задач (будем обозначать его GRIS-L) использовалось 100 20-мерных задач комбинированного типа: локальная подзадача размерности 18, глобальная подзадача размерности 2. В качестве локальной составляющей использовались 18-мерные подзадачи, представляющие собой комбинацию близких к линейным одномерных функций:
\begin{equation}\label{X2_problem}
\overline{\varphi}_j(y) = \sum_{i=1}^{18} \left(M_{ij} y_i + L_{ij} y_i^2\right) ,
\end{equation}
где $-2.2 \leq y_i \leq 1.8$, а параметры $0.5 \leq |M_{ij}| \leq 1$ и $0.01 \leq L_{ij} \leq 0.3$  распределены случайно равномерно в соответствующих интервалах.
В качестве глобальной составляющей задачи использовались многоэкстремальные функции, предложенные Гришагиным (см., например, \cite{Grishagin1994}).
%Марина, где-то здесь надо сказать, как формировалась целевая функция - общий вид, "перемешанные" параметры, и т.п. Буквально 1-2 предложения, не больше

Во втором наборе задач (далее GRIS-LQ) параметры $M_{ij}$ и $L_{ij}$ локальной подзадачи (\ref{X2_problem}) были устроены таким образом, чтобы в 60\% случаев переменная имела несущественную квадратичную часть, в 30\% вклад квадратичной составляющей был существенным (как минимум, соизмеримым со вкладом линейной составляющей), а оставшиеся 10\% переменных были чисто линейными.

В третьей серии экспериментов (далее GKLS-L) в качестве многоэкстремальной части использовались 4-мерные функции из класса GKLS (см., например, в \cite{Sergeyev2015,Grishagin2018}), в качестве локальной составляющей использовалась локальные подзадачи серии GRIS-L, размер серии составлял 20 задач.

В результате решения серии GRIS-L с заданной точностью $0.01$ и параметрами метода $K=100$, $deg=1$ и $T=0.6$ во всех задачах было проведено корректное разделение на глобальные и локальные переменные, а затем задачи были успешно решены при помощи nested optimization scheme за приемлемое время и количество итераций. Однако решение усложнённой серии GRIS-LQ с применением линейной регрессии не даёт хорошего результата, в силу наличия переменных параболического вида, для которых процент схожести с линейной моделью крайне низок. В связи с этим для проведения эксперимента были подобраны другие параметрs запуска $K=100$, $deg=2$ (квадратичная регрессия) и $T=0.95$. В результате все параметры задач были правильно разбиты на глобальные и локальные, а сами задачи затем успешно решены за приемлемое время и количество итераций.
 
%\begin{figure}[ht]
%\includegraphics[width=1.0\linewidth]{regression_94.png}
%\caption{Построение регрессионных моделей на 94-й задаче серии GRIS-L}
%\label{fig}
%\end{figure}

В третьей серии экспериментов в силу особенностей построения функций GKLS успешно решить всю серию задач за приемлемое время и количество итераций не удалось. Большинство переменных функций GKLS в различных сечениях сильно подобны линейным и квадратичным функциям, отчего приходится завышать пороговое значение $T$, чтобы случайно не отсеять глобальную переменную, как следствие часть локальных параметров также выносятся в глобальные. Большое количество глобальных параметров закономерно способствует увеличению количества итераций и времени решения задачи. С другой стороны, если поставить порог вхождения в локальные функции достаточно низким, то глобальных переменных в некоторых функциях метод в принципе не обнаружит.

Численные результаты серий экспериментов приведены в Таблице \ref{tab1}. В ней представлены данные о числе решенных задач $S$, среднем числе итераций глобального поиска на верхнем уровне рекурсии $G_{av}$, среднем числе испытаний на нижнем уровне $L_{av}$, среднем времени решения задачи $t_{av}$.
Вычисления проводились на компьютере с CPU Intel Core™ i7 10750H, 2.6 GHz. Для построения и анализа регрессионной модели использовался пакет scikit-learn из Python; global search algorithm and nested optimization scheme реализованы на C++.


\begin{table}[ht]
	\caption{Результаты решения серий тестовых задач}
	\label{tab1}
		\begin{tabular}{ l c c c c c c } \hline
		 & $S$ &  $G_{av}$ &  $L_{av}$ & $t_{av}, sec.$ \\
    \hline
		GRIS-L & 100/100  & 847 &  451 861 & 1.4 \\
		GRIS-LQ & 100/100 & 751 &  448 326 & 1.2 \\
		GKLS-L & 18/20 & 9531 &  3 899 417 & 16.4 \\
		\hline
		\end{tabular}
\end{table}




% Acknowledgement
\section{ACKNOWLEDGMENTS}
This study was supported by the Russian Science Foundation, project No 21-11-00204.



% References

%\nocite{*}
\bibliographystyle{aipnum-cp}%
\bibliography{bibliography}%


\end{document}
