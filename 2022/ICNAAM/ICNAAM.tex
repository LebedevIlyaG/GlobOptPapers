\documentclass{aip-cp}


\usepackage[numbers]{natbib}
\usepackage{rotating}
\usepackage{graphicx}

%% ДЛЯ РУССКОГО ТЕКСТА закомментировать потом!
\usepackage{inputenc}
\usepackage[T2A,T1]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{cmap}
%%


% Document starts
\begin{document}

% Title portion
\title{Использование методов машинного обучения для разделения параметров в задачах глобальной оптимизации большой размерности}

\author[aff1]{Konstantin Barkalov\corref{cor1}}
\author[aff1]{Marina Usova}

\affil[aff1]{Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia}
\corresp[cor1]{Corresponding author: konstantin.barkalov@itmm.unn.ru}

\maketitle

\begin{abstract}
В работе представлены результаты исследования подхода к решению задач оптимизации, в которых зависимость от разных групп параметров носит разный характер.
Предложена схема выделения параметров задачи, которые оказывают локальное влияние на целевую функцию, что позволяет решать существенно многомерные задачи с использованием nested optimization scheme. При этом на разных уровнях рекурсии используются разные оптимизационные алгоритмы. Предложенный подход показал свою работоспособность при решении тестовых и прикладных задач.

\end{abstract}

% Head 1
\section{INTRODUCTION}

В настоящее время методы глобальной оптимизации используются для решения широкого круга задач, возникающих в различных областях науки и техники. 
Например, традиционной сферой применения методов глобальной оптимизации является идентификация значений параметров математических моделей по данным экспериментов. К их числу относятся, например, inverse problems of chemical kinetics \cite{Gubaydullin2021}.  В задачах такого вида требуется провести поиск значений неизвестных параметров модели, при которых результаты моделирования наиболее близки к результатам, полученным экспериментально.

Число параметров, которые требуется идентифицировать подобным образом, для задач химической кинетики может составлять десятки и сотни. Использование детерминированных методов глобальной оптимизации для решения задачах такой размерности крайне ограничено из-за чрезвычайно больших вычислительных затрат на покрытие search domain точками испытаний. Это остается справедливым даже в случае использования эффективных алгоритмов (например, \cite{Paulavicius2011,Evtushenko2009,Jones2009}), строящих существенно неравномерные покрытия. 

Многие обратные задачи характеризуются тем, что зависимость от разных групп параметров носит разный характер. Например, от одной группы параметров зависимость может быть близка к линейной, тогда как по второй группе зависимость может носить сложный многоэкстремальный характер.

В этом случае решение задачи можно организовать по схеме recursive optimization. Решение  многоэкстремальной подзадачи, для которой требуется использовать сложные алгоритмы глобальной оптимизации, будет проводиться на верхнем уровне рекурсии.
Унимодальные подзадачи (каждая из которых соответствует фиксированному набору значений первой части параметров) будут решаться на нижнем уровне. Для их решения можно применять обладающие быстрой сходимостью методы локальной оптимизации.

Однако заранее указать разделение на группы параметров с разным характером поведения целевой функции, как правило, не представляется возможным, т.к. целевая функция в обратных задачах задается как черный ящик. 
Таким образом, актуальной является разработка алгоритмов минимизации существенно многомерных функций, в которых учитывается разный характер зависимости целевой функции от разных групп параметров. 
В данной работе предложен конкретный механизм такого разделения, основанный на <указать>.
В качестве примеров, подтверждающих работоспособность предложенной схемы, решены как тестовые, так и прикладные задачи.

\section{ОПТИМИЗАЦИОННЫЕ АЛГОРИТМЫ И РЕДУКИЦЯ РАЗМЕРНОСТИ}

Рассмотрим задачу оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y): y\in D\right\}}, \nonumber \\
& D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Будем предполагать, что функция $\varphi(y)$ является многоэкстремальной и удовлетворяет Lipschitz condition
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty,
\]
with the constant $L$ unknown a priori.
Одновременно с этим будет предполагать, что зависимость целевой функции от $N_1$ параметров является многоэкстремальной, а от оставшихся $N_2 = N - N_1$ параметров зависимость являеся близкой к линейной. При этом разделение  параметров является неизвестным. Такой характер зависимостей является типичным, например, для обратных задач химической кинетики [ссылка]. В данной работе предлагается подход к разделению параметров задачи на указанные две группы, конкретный механизм описан в следуюем разделе. 

Учет подобной особенности задач может существенно снизить вычислительную сложность поиска оптимума. В самом деле, в соответствии с известной схемой рекурсивной оптимизации \cite{Carr} решение исходной задачи можно проводить следующим образом:
\begin{equation}\label{global_problem}
\varphi(y^*) = \min_{y\in D} \varphi (y) = \min_{y^1\in D_1} f(y^1), \; D_1=\left\{y\in R^{N_1}: a_i\leq y_i \leq b_i, \; 1\leq i \leq N_1\right\},
\end{equation}
где 
\begin{equation}\label{local_problem}
f(y^1) = \min_{ y^2 \in D_2} \varphi(y^1,y^2), \; D_2=\left\{y\in R^{N_2}: a_i\leq y_i \leq b_i, \; N_1+1\leq i \leq N\right\}.
\end{equation}
В соответствии с (\ref{global_problem}) вычисление одного значения функции $\varphi^1 (y^1)$ (данный процесс будем называть search trial) подразумевает решение унимодальной задачи (\ref{local_problem}), которое может быть выполнено одним из методов локальной оптимизации. Эффективность и проработанность методов локальной оптимизации позволяет решать подзадачи (\ref{local_problem}) с точностью, значительно превышающей точность методов глобальной оптимизации. Соответственно, можно рассматривать задачу (\ref{global_problem}) как задачу глобальной оптимизации, в которой значение функции может быть вычислено с высокой точностью, но данная операция является трудоемкой. 

В проведенном исследовании для решения задач глобальной оптимизации (\ref{global_problem}) мы использовали global search algorithm в сочетании со схемой редукции размерности на основе кривых Пеано \cite{Sergeyev2013,Lera2021}. Данный алгорим зарекомендовал себя как мощный инструмент для решения сложных многоэкстремальных задач \cite{Kvasov2013,Kalyulin2017,Cavoretto2021}.
Для решения локальной задачи (\ref{local_problem}) мы использовали Hooke-Jeevse pattern search method (see, e.g., \cite{Kelley}). В отличие от методов типа BFGS (see, e.g., \cite{Nocedal}), данный алгоритм не требует вычисления градиента и хорошо работает в условиях наличия вычислительных ошибок в значениях функций, что является типичным при решении прикладных задач.

\section{РАЗДЕЛЕНИЕ ПАРАМЕТРОВ ЗАДАЧИ}

Для разделения параметров задачи на локальные и глобальные предлагается следующий механизм действий:

Шаг 1. Зафиксировать некоторое сечение $\widetilde{y} = (\widetilde{y}_1, \widetilde{y}_2, … ,\widetilde{y}_N)$ целевой функции $\varphi(y)$. Выбор сечения осуществляется одним из предложенных способов:
\begin{enumerate}
\item сечение фиксируется из физического смысла прикладной задачи;
\item в качестве фиксируемого сечения принимается решение, найденное локальным методом за $K$ итераций, используемый локальный метод – Hooke-Jeevse pattern search method [используется по умолчанию].
\end{enumerate}

Шаг 2. Для очередной переменной $y_i$, $1\leq i \leq N$, провести исследование на локальность:
\begin{enumerate}
\item вычислить $P$ значений целевой функции $\varphi(y)$ в зафиксированном сечении $\widetilde{y}$ при равномерно изменяемой исследуемой переменной $y_i$ в своей области значений $a_i\leq y_i \leq b_i$;
\item сформировать множество $Q$, представляющее собой набор из $P$ пар вида «значение исследуемой переменной – значение целевой функции в сечении $\widetilde{y}$ при указанном значении исследуемой переменной»
\[
Q = \left\{ (\widetilde{y}^k_i,  \varphi({\widetilde{y}^k})): \widetilde{y}^k = (\widetilde{y}_1, \widetilde{y}_2, … , \widetilde{y}^k_i, … ,\widetilde{y}_N), \widetilde{y}^k_i \sim U[a_i, b_i], 1\leq i \leq N, 1\leq k \leq P \right\};
\]
\item построить регрессионную модель заданной степени $deg$ на основе данных $Q$;
\item вычислить оценку $R^2$ и провести классификацию переменной:
\begin{itemize}
\item если $R^2$ меньше порога вхождения в локальные функции $T$, то добавить переменную к множеству глобальных переменных,
\item иначе – добавить переменную к множеству локальных переменных.
\end{itemize}
\end{enumerate}

Шаг 3. Если локальные переменные были обнаружены – запустить последовательную блочную многошаговую схему, на верхнем уровне которой глобальные переменные, а на нижнем - локальные. В противном случае запустить обычный АГП, считая все переменные глобальными.

Комментарий 1. В основе проводимой регрессии лежит метод наименьших квадратов (МНК).
%
%Комментарий 1. В основе проводимой регрессии лежит метод наименьших квадратов (МНК), использующий следующие шаги для выбора подходящй модели:
%\begin{enumerate}
%\item для очередной модели:
%\begin{itemize}
%\item вычислить ошибки между рассматриваемой моделью и точками множества данных,
%\item возвести в квадрат каждую ошибку,
%\item просуммировать все квадраты ошибок,
%\end{itemize}
%\item выбрать модель, в которой сумма квадратов ошибок минимальна.
%\end{enumerate}

Комментарий 2. Оценка качества построенной модели на шаге 2 в пункте 4 производится с помощью метрики $R^2$. Значение $R^2$ или коэффициент детерминации - это единица минус доля необъяснённой дисперсии в дисперсии зависимой переменной. Она позволяет оценить эффективность моделей машинного обучения на основе регрессии, то есть характеризует схожесть построенной модели регрессии с целевой функцией в зафиксированном сечении. Значение $R^2$, как правило, находится между 0 и 1. Результат, близкий к 1, показывает, что точность модели высока.

Комментарий 3. Значаения  $K$, $deg$ и $T$ являются параметрами метода и устанавливаются в соотвествии с особенностями решаемой задачей. В алгоритме возможно использование степени полинома регрессии $deg = 1$ или $deg = 2$, что позволяет построить модель линейной или квадратичной регрессии соотвественно.  Для контроля определения "функция достаточно схожа с локальной" используется параметр метода $T$.

\section{NUMERICAL EXPERIMENTS}

В первом наборе тестовых задач (далее GRIS-L) использовалось 100 20-мерных задач комбинированного типа: локальная подзадача размерности 18, глобальная подзадача размерности 2. В качестве локальной составляющей использовались 18-мерные задачи, предсталяющие собой комбинацию почти линейных одномерных функций:
\begin{equation}\label{X2_problem}
\widetilde{\varphi}_1(y_1, ... , y_{18}) = \sum_{i=1}^{18} (M_{ij} y_i + L_{ij} y_i^2),
\end{equation}
где $-2.2 \leq y_1, .. ., y_{18} \leq 1.8$, а параметры $0.5 \leq |M_{ij}| \leq 1$ и $0.01 \leq L_{ij} \leq 0.3$  являются независимыми случайно равномерно распределёнными величинами.
В качестве глобальной составляющей задачи использовались многоэкстремальные функции Гришагина \cite{}.

Во втором наборе задач (далее GRIS-LQ) параметры $M_{ij}$ и $L_{ij}$ локальной подзадачи (\ref{X2_problem}) были устроены таким образом, чтобы в 60\% случаев переменная имела несущественную квадратичную часть, в 30\% вклад квадратичной составляющей был существенным (как минимум, соизмеримым со вкладом линейной составляющей), а оставшиеся 10\% переменных были чисто линейными.

В третьей серии экспериментов (далее GKLS-L) функции Гришагина заменялись 4-мерными функциями Сергеева \cite{}, в качестве локальной составляющей использовалась локальные подзадачи серии GRIS-L, размер серии был уменьшен до 20 задач.

В результате решения серии GRIS-L с заданной точностью $0.01$ и параметрами метода $K=100$, $deg=1$ и $T=0.6$ во всех задачах было проведено корректное разделение на глобальные и локальные переменные, а затем задачи были успешно решены при помощи многошаговой схемы за приемлемое время и количество итераций. Однако решение усложнённой серии GRIS-LQ с применением также линейной регрессии не даёт хорошего результата, в силу наличия переменных параболического вида, для которых процент схожести с линейной моделью крайне низок.В связи с этим для проведения эксперимента были подобранны другие параметрв запуска $K=100$, $deg=2$ (квадратичная регрессия) и $T=0.95$. В результате все параметры задач серии были правильно разбиты на глобальные и локальные переменные, а затем успешно дорешены за приемлемое время и количество итераций.
 
%\begin{figure}[ht]
%\includegraphics[width=1.0\linewidth]{regression_94.png}
%\caption{Построение регрессионных моделей на 94-й задаче серии GRIS-L}
%\label{fig}
%\end{figure}

В третьей серии экспериментов в силу особенностей построения функций Сергеева успешно решить всю серию задач за приемлемое время и количество итераций не удалось. Большинство переменных функций GKLS в различных сечениях сильно подобны линейным и квадратичным функциям, отчего приходится сильно занижать порог вхождения $T$, чтобы случайно не отсеить глобальную переменную, как следствие часть локальных параметров также выносятся в глобальные. Большое количество глобальных параметров закономерно способствует увеличению количества итераций и времени решения задачи. С другой стороны, если поставить порог вхождения в локальные функции достаточно высоким, то глобальных переменных в некоторых функциях метод в принципе не обнаружит.

Численные результаты серий экспериментов приведены в Таблице \ref{tab1}.

\begin{table}[ht]
	\caption{Результаты решения серий тестовых задач}
	\label{tab1}
		\begin{tabular}{ l c c c c c c } \hline
		 & Количество &  Среднее & Среднее & Среднее & Среднее время \\ 
		 & решенных  & число & число & число  & решения \\ 
		 & задач & итераций & испытаний & запусков & задачи, с\\
		 &  & & & локального & \\
		 & & & & метода & \\ \hline
		GRIS-L & 100/100  & 847 & 1692 & 451861 & 1,38 \\
		GRIS-LQ & 100/100 & 751 & 1501 & 448326 & 1,21 \\
		GKLS-L & 18/20 & 9531 & 19061 & 3899417 & 16,45 \\
		\hline
		\end{tabular}
\end{table}

%\begin{figure}%[ht]
%\includegraphics[width=1.0\linewidth]{fig1.png}
%\caption{Schematic representation of the computational domain, with the geometry parameterization points marked in red and the black points independent of the optimization iterations. The grey lines show the possible deformation of the geometry.}
%\label{fig}
%\end{figure}

The calculation was carried out ... processor Intel Core™ i7 10750H, 6 ядер, 2,6 GHz.

% Acknowledgement
\section{ACKNOWLEDGMENTS}
This study was supported by the Russian Science Foundation, project No 21-11-00204.



% References

%\nocite{*}
\bibliographystyle{aipnum-cp}%
\bibliography{bibliography}%


\end{document}
