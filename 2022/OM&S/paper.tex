% interacttfssample.tex
% v1.05 - August 2017

\documentclass[]{interact}

%\usepackage{color}
\usepackage{hyperref}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled
%\setlength\bibindent{2em}% To increase hanging indent in bibliography when line spacing is doubled

\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\bibpunct[, ]{[}{]}{,}{n}{,}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

\begin{document}

%\articletype{ARTICLE TEMPLATE}% Specify the article type or omit as appropriate

\title{A fast algorithm for finding efficient decisions in multi-objective problems with black-box multiextremal functions}

\author{
\name{Konstantin Barkalov, Vladimir Grishagin and Evgeny Kozinov\thanks{CONTACT Evgeny Kozinov. Email: evgeny.kozinov@itmm.unn.ru}}
\affil{Software Department, Lobachevsky State University, Nizhni Novgorod, Russia}
}

\maketitle

\begin{abstract}
This template is for authors who are preparing a manuscript for a Taylor \& Francis journal using the \LaTeX\ document preparation system and the \texttt{interact} class file, which is available via selected journals' home pages on the Taylor \& Francis website.
\end{abstract}

\begin{keywords}
Multi-objective optimization; global optimization; dimensionality reduction; search information; nature-inspired metaheuristics; numerical comparison
\end{keywords}


\section{Introduction}

In the field of mathematical modeling the intelligent decision-making processes, the problems of multicriterial optimization (MCO) present a wide class of models that are rich and interesting from a theoretical point of view and very important in real applications \cite{Marler2009,Hillermeier2005}. Multicriterial problems describe decision-making situations with multiple goals which are considered as functional criteria to be optimized.

In general case, there exist three key factors determining the complexity of analyzing these models. The crucial feature corresponds to the typical case where criteria are contradictory: increasing one criterion leads to decreasing the other. Such inconsistent behavior generates the necessity of introducing a compromise in the joint analysis of criteria and consideration of a set of compromised (partial) solutions as a complete solution of a multiobjective problem (Pareto set), see, e.g., \cite{Collette2004,Ehrgott2005}.
The second complexity factor is associated with the dimensionality or number of varied parameters of the problem because increasing the problem dimension leads to significant growth of computational cost. 

Finally, the last factor is the multiextremality of criteria \cite{Pardalos2017}. First of all, multiextremal criteria can generate complicated Pareto sets consisting, for example, of several disconnected parts. Moreover, the combination of such factors as multiextremality and greater dimension leads to significant computational complexity of the problem because the computational cost increases exponentially when dimension grows. From this point of view the multiextremal problems are the most complicated ones in multicriterial optimization and this MCO class is the subject of interest in the given paper. 

The simplest way to build an approximation of the Pareto set consist in immersion in the feasible domain of problem's parameters a uniform grid (deterministic or random) and selection of all non-dominated nodes of the grid. Unfortunately, this universal approach is too costly because it requires in the worst case pairwise comparison of all the grid nodes and does not take into account the properties of the problem for reducing the computational costs.

Another idea applies a parameterization scheme for the criteria of the problem and reduces a MCO problem to a family of single-criterion (scalar) optimization problems being problems of mathematical programming which can be solved by corresponding well-developed optimization methods \cite{Collette2004,Ehrgott2005}. In this scheme a non-negative coefficient (weight) is assigned to each criterion which reflects numerically an ``importance'' of the criterion and on the base of such weighted criteria a scalar function (convolution) is built and minimized. Under appropriate choice of convolution the solution of the built scalar optimization problem is a partial solution of the initial multicriterial problem. Changing weights we can obtain different Pareto points. 

There exist various convolution functions but weighted sum function (linear convolution) and maximum from weighted criteria (maximum convolution) are most commonly used. The latter is more universal because it enables to build completely the Pareto set taking all the possible combinations of weights, whereas the linear convolution can lose Pareto points in non-convex case (in particular, if criteria are multiextremal).

For optimization of convolutions a wide spectrum of algorithms can be used in dependence on convolution properties generated by the features of criteria in the initial MCO problem. In particular, for the multiextremal case where it is required to find the global optimum the publications \cite{Evtushenko2014,Zilinskas2015,GERGEL2017_1,Gergel2019_2,Barkalov2021} describe applications of the global optimization algorithms to MCO problems in combination with the convolution approach.

Among other methods of solving MCO problems it is necessary to pay attention to the important class of metaheuristic nature-inspired algorithms. Many of them are nature-inspired, i.e.,  are  based on modeling physical processes (for example, simulated annealing \cite{Locatelli2002,Aarts2014}) or mainly behavior of biological agents including evolutionary \cite{Price2005,Coello2007} and as their part genetic algorithms \cite{Ruiz2015,Deb2002,Zitzler2001} and particle swarm methods modeling the swarming intelligence of bees, fireflies, birds, ants, etc. \cite{Mostaghim2007,Nebro2009,Durillo2010}.

Consideration of different approaches to solving MCO problems and a relevant references can be found, for example, in monographs and papers \cite{Miettinen1999,Ehrgott2005,Zhou2011,Nedjah2015,Pardalos2017}.
In this paper we consider multiextremal black-box MCO problems and propose a new scheme of solving these problems. This scheme is based on maximum convolution, dimensionality reduction by means of Peano curves, information-statistical global optimization algorithm used for the first time in MCO problems. Moreover, in the framework of the scheme the information already obtained in the course of solving scalar subproblems of convolution optimization can be utilized for solving a new convolution subproblem. Briefly, the way is as follows. Before beginning the optimization of a new convolution we have at our disposal values of criteria computed already and now can recalculate them to values of the current convolution taking into account these values as initial information for optimization algorithm. This procedure accelerates significantly the optimization process.

The quality of the proposed MCO method is experimentally studied on several multidimensional multiextremal test problems. Comparison with known evolutionary algorithms (2 particle swarm methods and 3 genetic algorithms have been taken) on the base of the hypervolume index \cite{Evtushenko2014,Zilinskas2015} reflecting quality of Pareto set estimation demonstrates qualitative results of the proposed method.

The rest of the paper is organized in the following manner. Section 2 considers the statement of the MCO problem to be studied. Section 3 is devoted to the description of the global search algorithm applied for scalar optimization of convolutions and the scheme of joint analyzing the family of ones. Section 4 contains results of computational experiments. The last Section concludes the paper.


\section{MCO problem statement}

The model of decision making to be considered hereinafter as the MCO problem consists in the following.

Let $w_i(x), 1 \leq i \leq k,$ be real-valued functions depending on vector of arguments $x=(x_1,...,x_N)$  and defined over the hyperparallelepiped 
\begin{equation}\label{Q}
Q = \left\{x \in R^N : a_j \leq x_j \leq b_j, 1 \leq i \leq N \right\}
\end{equation}
in $N$-dimensional Euclidean space $R^N$. These functions reflect objectives of decision making and it is considered that the less the function value, the better the result of achieving this objective.

The problem of multicriterial optimization is formulated as minimizing over the domain (\ref{Q}) the vector function
\begin{equation}\label{W}
W(x) = \left(w_1(x), ..., w_k(x)\right).
\end{equation}

This problem can be symbolically written as
\begin{equation}\label{problem}
W(x) \rightarrow \min , \; x \in Q.
\end{equation}

In terminology of decision making the vector function $W(x)$  is called \textit{vector criterion} of the problem, functions $w_i(x), 1 \leq i \leq k,$ are \textit{partial criteria} or \textit{objective functions}, $Q$ is the domain of feasible decisions or just the \textit{feasible domain} and points $x \in Q$ are \textit{feasible decisions}.

As a rule, each criterion attains its minimum at a point different from minimum points of other criteria, i.e., it is impossible to find out the decision $x^*$   providing in the region $Q$ minimum values for all the partial criteria simultaneously. This situation generates the contradictoriness of criteria, when decreasing one criterion leads to the growth of the other, and complicates the notion of optimal decision (solution to the problem (\ref{problem})).

Traditionally, optimal decisions in the problem (\ref{problem}) are defined on the base of Pareto optimality concept. To complete this item, let us give the known classical definitions concerning this concept.

\begin{definition} 
Let $x,z \in Q$. Vector $x$ is said to \textit{dominate} vector $z$ ($x \succ z$) if $w_i(x) \leq w_i(z), \; 1 \leq i \leq k,$ and there exists a number $p, \; 1 \leq p \leq k,$  such that $w_p(x) < w_p(z)$.
\end{definition}
\begin{definition} 
A decision vector $x^* \in Q$ is a \textit{Pareto optimal point} if in the domain $Q$ there is no vector $z \neq x^*$ dominating $x^*$.
\end{definition}
\begin{definition} 
The set of all Pareto optimal points (Pareto set $P$) is the \textit{Pareto optimal solution} to the MCO problem (\ref{problem}).
\end{definition}
\begin{definition} 
The set $F=W(P)=\left\{W(x):x \in P\right\}$  is called the \textit{Pareto front} of the problem (\ref{problem}).
\end{definition}

As mentioned in Introduction, one way of building the Pareto set consists in reducing the multiobjective problem to a family of single-criteria, or scalar optimization problems and solving them by methods of mathematical programming.  The global minimum point of a problem from this family will be a Pareto optimal point under corresponding assumptions. This approach can be realized by means of applying a convolution of the vector criterion (\ref{W}), for example, the maximum convolution
\begin{equation}\label{conv}
\Gamma(\lambda,x) = \max_{1 \leq i \leq k}{\lambda_i w_i(x)},
\end{equation}
where coefficients $\lambda_i, \; 1 \leq i \leq k,$ (weights of criteria) satisfy the conditions
\begin{equation}\label{lambda}
\lambda_i \geq 0, \; 1 \leq i \leq k, \; \sum_{i=1}^k{\lambda_i} = 1.
\end{equation}

If
\begin{equation}\label{positive}
w_i(x) > 0, \; 1 \leq i \leq k,
\end{equation}
in the domain (\ref{Q}), then global solution to the problem
\begin{equation}\label{conv_problem}
\Phi_\lambda(x) = \Gamma(\lambda,x) + \gamma \sum_{i=1}^k{\lambda_i w_i(x)} \rightarrow \min, \; x \in Q,
\end{equation}
with a sufficiently small parameter $\gamma > 0$ is a Pareto optimal point of the initial MCO problem (\ref{problem}) \cite{Wierzbicki} [Wierzbicki, Krasnoshekov]. The positiveness of criteria is not restrictive requirement because it is possible to transform easily the MCO problem with non-positive criteria to the form (\ref{positive}) without loss of Pareto solution.

In the case of multiextremal criteria the problem (\ref{conv_problem}) is multiextremal as well and it is necessary to use global optimization algorithms for its solving. Many references to such the algorithms can be found in monographs \cite{Strongin2000,Pinter1996,Zhigljavsky2008,Sergeyev2013,PaulaviciusZilinskas2014,Sergeyev2017}. For solving multiextremal problems the information-statistical algorithms \cite{Strongin2000,Sergeyev2013} are among of the most efficient.

In the next section we describe an information-statistical algorithm with accelerated convergence and its application to searching for Pareto optimal solutions in multiextremal MCO problems.

\section{Computational scheme for multiextremal multiobjective optimization}

In this section we consider the MCO problem under additional assumption according to which the criteria $w_i(x), \; 1 \leq i \leq k,$ are black-box functions and satisfy in the domain $Q$ the Lipschitz condition
\begin{equation}\label{lip}
\left| w_i(x) - w_i(y)\right| \leq L_i \left\| x-y \right\|, \; x,y, \in Q, \; 1 \leq i \leq k,
\end{equation}
with corresponding Lipchitz constants $L_i > 0$ that are, as a rule, unknown. Here $\left\|*\right\|$   denotes the Euclidean norm in $R^N$.

The Lipschitzian functions are, in general case, multiextremal. If we consider the convolution (\ref{conv}) aggregating multiextremal criteria it will be multiextremal as well.

In the computational scheme proposed in the paper we consider a family $S$ of scalar problems (\ref{conv_problem}) that are solved jointly and take into account the information obtained in the course of already completed optimizations. For this purpose in the region of weight coefficients determined by the conditions (\ref{lambda}) a uniform grid is built and the family $S$ consists of the problems (\ref{conv_problem}) with coefficients $\lambda$ corresponding the nodes of the grid.

For solving the problems of the family a global optimization technique based on reducing the multidimensional problem (\ref{conv_problem}) to an equivalent univariate subproblem [Strongin+Sergeyev, Pintér, Zhigljavsky+, Sergeyev+ Strongin, Paulavičius+, Sergeyev+ Kvasov]  in combination with one-dimensional information algorithm with accelerated convergence.

Let us consider this technique in more detail.

To simplify the description of the technique let us present the problems (\ref{conv_problem}) in the following unified form
\begin{equation}\label{problem_f}
f(x) \rightarrow \min, \; x \in Q,
\end{equation}
where 
\[
f(x) = \max_{1 \leq i \leq k}{\lambda_i w_i(x)} + \gamma \sum_{i=1}^k{\lambda_i w_i(x)}
\]
and $f(x)$ meets the Lipschitz condition 
\begin{equation}\label{lip_1}
\left| f(x) - f(y)\right| \leq L \left\| x-y \right\|, \; x,y, \in Q,
\end{equation}
with the constant $L>0$.

It is known as a fundamental fact that $N$-dimensional hyperparallelepiped (\ref{Q}) and the interval $[0,1]$ of the real axis are the equipotent sets and the interval $[0,1]$ can be mapped onto the parallelepiped (\ref{Q}) unambiguously and continuously. Such mappings are called \textit{Peano-type curves} or \textit{evolvents}.


Let $x(t), \; t \in [0,1]$  be a Peano-type curve, and the function  $f(x)$ from (\ref{problem_f}) be continuous. As
\[
Q =\left\{x(t) : t \in [0,1]\right\},
\]
$f(x)$ and $x(t)$ are continuous, then
\[
\min_{x \in Q} f(x) = \min_{t \in [0,1]}f(x(t)),
\]
i.e., solving the multidimensional problem (\ref{problem_f}) can be reduced to the minimization of the one-dimensional function $f(x(t))$.

However, in the case when the function $f(x)$ is Lipschitzian this property is not valid for the reduced function. The function $f(x(t))$ will satisfy the H{\"o}lder condition
\begin{equation}\label{holder}
\left|f(x(t'))-f(x(t''))\right| \leq H \left|t'-t''\right|^{1/N}, \; t',t'' \in [0,1],
\end{equation}
with a H{\"o}lder constant $H>0$ \cite{Strongin2000,Sergeyev2013}.

If the function $f(x)$ satisfies the Lipschitz condition (\ref{lip_1}) then $H=2L\sqrt{N+3}$.

As a consequence, minimization of the function $f(x(t))$  requires the use of special algorithms oriented at functions with property (\ref{holder}).

In the proposed scheme for solving reduced univariate minimization problem we apply modified information-statistical algorithm \cite{Strongin2000} with local refinements (ALR) accelerating convergence to global optimum of functions that satisfy the H{\"o}lder condition.

Before description of this algorithm let us follow again the path of simplification and formulate the general one-dimensional optimization problem as
\begin{equation}\label{problem_fi}
\varphi(t) \rightarrow \min, \; t \in [\alpha,\beta].
\end{equation}
In our consideration $\varphi(t) = f(x(t))$, $[\alpha,\beta] = [0,1]$ and $\varphi(t)$  meets the H{\"o}lder condition with the constant $H$.

The description of ALR can be presented in the following manner.

Let in the optimization problem the term ``trial'' denote the computation of objective function value at a point of the feasible domain. 



Solving the first problem from $S$ we reduce it to the univariate problem (\ref{problem_fi}) that is solved by the univariate method ALR. This method carries out 2 first iterations at boundary points and the next trials are executed in accordance with the points 1--4 of the algorithm's description up to stopping condition to be met.

In the course of optimization the algorithm builds the trial points $t^1,...,t^n$ which correspond to the points $x^1 = x(t^1),...,x^n=x(t^n)$ in the domain $Q$ . At these points the values $v^j = \Phi_\lambda(x^j), \; 1 \leq j \leq n,$ of the function $\Phi_\lambda(x)$ from (\ref{conv_problem}) are computed as values $\varphi(t^j)$ of the function $\varphi_t$  minimized by ALR.

In turn, obtaining the values $\Phi_\lambda(x^j)$  is implemented through the computation of criteria values $a_s^j = w_s(x^j), \; 1 \leq s \leq k, \; 1 \leq j \leq n,$  that can be considered as components of the matrix $A(k \times n)$ containing \textit{a posteriori information} about the problem.

This information can be used for decreasing the number of trials during solving the problem (\ref{conv_problem}) with other weight vector $\lambda$. Indeed, if we take the weight vector $\mu = (\mu_1,...,\mu_k) \neq \lambda$, it is easy on the base of matrix $A$ to calculate values $\Phi_\mu(x^j), \; 1 \leq j \leq n,$ at points $x^1 = x(t^1),...,x^n=x(t^n)$, i.e. values $\varphi(t^1),...,\varphi(t^n)$ of the function $\varphi(t)$ in the problem (\ref{problem_fi}) being the reduced problem (\ref{conv_problem}) with weight vector $\mu$. But in this case in ALR we can take points $t^1,...,t^n$ as points of initial trials in which values $ \varphi(t^j) = \Phi_\mu(x^j), \; 1 \leq j \leq n,$ have been already computed and continue optimization following the rules 1--4 of the algorithm. In the course of continuation, ALR will generate new points $t^q$ in which the values $\Phi_\mu(x(t^j))$, and, therefore, values $w_s(x(t^j)), \; 1 \leq s \leq k,$ will be evaluated. The latters can be added to the matrix $A$ and be taken into account in the same manner for solving other problems (\ref{conv_problem}). 

The proposed scheme of utilizing a posteriori information allows one to decrease significantly the total number of criteria computation when joint optimizing problems of the family $S$. On the whole, the given approach is useful  if the time spent to calculate the criteria at one point is significantly higher than the time taken by the algorithm for implementation of the trial at that point, i.e., it is oriented at time consuming MCO problems.
 
The efficiency of the described general scheme is estimated in the next section in comparison with several evolutionary methods. 




\section{Numerical experiments}


\section*{Disclosure statement}

No potential conflict of interest was reported by the authors.

\section*{Funding}

This work was supported by the Russian Science Foundation, project No. 21-11-00204.



\bibliographystyle{tfs}
\bibliography{bibliography}

\end{document}
+