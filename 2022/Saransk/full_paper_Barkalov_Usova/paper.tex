% !!!!!!!!!!!!!!!!!!   ВНИМАНИЕ !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% Заголовки разделов формируются при помощи команд \section{}, \subsection{}, \subsubsection{}
% Не используйте уровень вложенности заголовков больше трех!
% -----------------------------
% Для оформления теорем, лемм, следствий используйте окружения 
% Def     - Определение
% Teor    - Теорема
% Lem     - Лемма
% Predl   - Предложение
% Ass     - Утверждение
% Cor     - Следствие
% Example - Пример
% -----------------------------
% Доказательство теоремы начинается командой \proof и завершается командой \endproof
% -----------------------------
% Литература помещается в окружение biblio.

\documentclass[11pt, oneside, a4paper]{article}
%\usepackage[cp1251]{inputenc} % кодировка
\usepackage[utf8]{inputenc} % кодировка
\usepackage[english, russian]{babel} % Русские и английские переносы
\usepackage{graphicx}          % для включения графических изображений
\usepackage{cite}              % для корректного оформления литературы
\usepackage{enumitem}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtext}


%стилевой пакет
\usepackage{schoolseminar2022}


\begin{document}
% \udk     - универсальный десятичный классификатор
% \msc     -
% \title   - название статьи
% \authors - список авторов

\setcounter{page}{1}

\title{Разделение параметров в задачах глобальной оптимизации с помощью методов машинного обучения
\renewcommand{\thefootnote}{*}\footnote{Работа выполнена при поддержке Министерства науки и высшего образования РФ (проект \textnumero~0729-2020-0055) и научно-образовательного математического центра <<Математика технологий будущего>> (проект \textnumero~075-02-2021-1394).}}


\authors{К.А.~Баркалов, М.А.~Усова}
\organizations{Нижегородский государственный университет им. Н.И. Лобачевского \\ Нижний Новгород, Россия}



\abstractru{
В работе представлены результаты исследования подхода к решению задач глобальной оптимизации с разным характером зависимости от разных групп параметров.
Предложена схема выделения параметров задачи, которые оказывают локальное влияние на целевую функцию, что позволяет решать существенно многомерные задачи с использованием многошаговой схемы редукции размерности. При этом на разных уровнях рекурсии используются разные оптимизационные алгоритмы. Проведено исследование работоспособности предложенного подхода.
}

\keywords{глобальная оптимизация, локальная оптимизация, редукция размерности, разделение параметров}


% \section{название} - заголовок раздела первого уровня
% \subsection{название} - заголовок раздела второго уровня
% \subsubsection{название} - заголовок раздела третьего уровня
% Не используйте уровень вложенности заголовков больше трех!
% Каждый абзац текста в статье начинается командой \par или пустой
% строкой.

\bigskip

\section{Постановка задачи}

В настоящее время методы глобальной оптимизации используются в различных областях науки и техники, например, для идентификации значений параметров математических моделей, при которых результаты моделирования наиболее близки к результатам, полученным экспериментально.
Число параметров, которые требуется определить подобным образом, например, для задач химической кинетики, может составлять десятки и сотни. В данном случае использование детерминированных методов глобальной оптимизации крайне ограничено в силу чрезвычайно больших вычислительных затрат на покрытие области поиска точками испытаний. Это остается справедливым даже в случае использования эффективных алгоритмов (например, \cite{Evtushenko2009,Paulavicius2016}), строящих существенно неравномерные покрытия. 
При этом частым явлением в подобных задачах является близкая к линейной зависимость по некоторой группе параметров, в то время как для остальных параметров наблюдается сложный многоэкстремальный характер. Заранее указать разделение на группы параметров с разным характером поведения целевой функции, как правило, не представляется возможным, т.к. целевая функция в обратных задачах задается как черный ящик. 

Рассмотрим задачу оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y): y\in D\right\}}, \nonumber \\
& D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Будем предполагать, что функция $\varphi(y)$ является многоэкстремальной, заданной в виде черного ящика и удовлетворяет условию Липшица
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty,
\]
с априори неизвестной константой $L$.
Целевые функции большинства практических задач оптимизации обладают данным свойством.
Одновременно с этим будем предполагать, что зависимость целевой функции от $N_1$ параметров является многоэкстремальной, а от оставшихся $N_2 = N - N_1$ параметров зависимость является близкой к линейной или квадратичной. При этом разделение  параметров является неизвестным.

\section{Общее описание подхода к решению}

Решение поставленной задачи можно организовать с помощью многошаговой схемы редукции размерности \cite{Grishagin2007}.

Решение  многоэкстремальной подзадачи, для которой требуется использовать сложные алгоритмы глобальной оптимизации, будет проводиться на верхнем уровне рекурсии с помощью алгоритма глобального поиска \cite{Strongin13} в сочетании со схемой редукции размерности на основе кривых Пеано \cite{Sergeyev2013}. Данный алгоритм зарекомендовал себя как мощный инструмент для решения сложных многоэкстремальных задач.

Унимодальные подзадачи будут решаться на нижнем уровне с помощью метода Хука-Дживса. В отличие от методов типа BFGS, данный алгоритм не требует вычисления градиента и хорошо работает в условиях наличия вычислительных ошибок в значениях функций, что является типичным при решении прикладных задач.

Данный подход поможет существенно снизить вычислительную сложность поиска оптимума.

\section{Разделение параметров задачи}

Для разделения параметров задачи на локальные и глобальные предлагается следующая последовательность действий:

\textbf{Этап 1.} Зафиксировать опорную точку $\overline{y} = (\overline{y}_1, \overline{y}_2,...,\overline{y}_N)$ для целевой функции $\varphi(y)$ одним из следующих способов:
\begin{itemize}
\item точка фиксируется исходя из физического смысла решаемой прикладной задачи;
\item в качестве фиксированной точки используется решение, найденное локальным методом за $K$ итераций;
\item в качестве фиксированной точки используется решение, найденное глобальным методом за $K$ итераций.
\end{itemize}

\textbf{Этап 2.} Для всех компонент (переменных) $y_i, \; 1\leq i \leq N$ провести исследование на локальность:
\begin{enumerate}
\item вычислить $P+1$ значение целевой функции $\varphi(y)$ в точках $z_i^j = (\overline{y}_1,...,\overline{y}_{i-1},y_i^j,\overline{y}_{i+1},...,\overline{y}_N)$, где
$y_i^j =  a_i + jh, \; h=(b_i-a_i)/P, \; 0\leq j \leq P$;
\item сформировать множество $Q$, представляющее собой набор из $P+1$ пары вида $\left\{y_i^j, \varphi(z_i^j)\right\} $;
\item построить регрессионную модель заданной степени $deg$ на основе данных из $Q$;
\item вычислить оценку $R^2$ и провести классификацию i-й переменной:
\begin{itemize}
\item если $R^2$ больше заданного порога $T$, то добавить переменную к множеству локальных переменных,
\item иначе добавить переменную к множеству глобальных переменных.
\end{itemize}
\end{enumerate}

Если локальные переменные были обнаружены, то продолжить решение задачи с использованием описанной ранее многошаговой схемы. В противном случае запустить алгоритм глобального поиска, считая все переменные глобальными.

Значения  $K$, $deg$ и $T$ являются параметрами метода и устанавливаются исследователем в соответствии со свойствами решаемой задачей. В реализованном методе $deg = 1$ или $deg = 2$, что позволяет построить модель линейной или квадратичной регрессии соответственно.

\begin{figure}[h!]
\centering \includegraphics[width=1\linewidth]{regression_94}
\caption{Линейные регрессионные модели 15-мерной тестовой задачи  (2 глобальных параметра)}\label{regression_94_}
\end{figure}
\begin{figure}[h!]
\centering \includegraphics[width=1\linewidth]{regression_94_3}
\caption{Квадратичные регрессионные модели 15-мерной тестовой задачи (2 глобальных параметра)}\label{regression_94_3_}
\end{figure}

В основе проводимой регрессии лежит метод наименьших квадратов (МНК). Оценка качества построенной на этапе 2 модели производится с помощью скорректированного коэффициента детерминации $R^2_a$. Указанный коэффициент характеризует схожесть построенной модели регрессии с целевой функцией в зафиксированном сечении. 

\section{Численные эксперименты}

Предложенный подход показал свою работоспособность при решении нескольких серий тестовых задач, представляющих собой линейные комбинации подзадач с глобальными параметрами (функции Гришагина или GRIS, функции Сергеева или GKLS \cite{Grishagin2001}) и подзадач с локальными параметрами, представляющих собой комбинацию близких к линейным одномерных функций (далее L) или  комбинацию  функций с разным вкладом квадратичной составляющей (далее LQ).

По результатам экспериментов при решении серий задач с существенной многоэкстремальностью (GRIS) было проведено корректное разделение на глобальные и локальные переменные, а затем задачи были успешно решены при помощи многошаговой схемы за приемлемое время и количество итераций. Задачи с функциями  GKLS для алгоритма являются более сложными, в силу схожести части параметров функций с параболоидами, поэтому решить все задачи серии за ограниченное количество итераций метода не удалось.

Вычисления проводились на компьютере с CPU Intel Core™ i7 10750H, 2.6 GHz. Для построения и анализа регрессионной модели использовался пакет scikit-learn из Python.
Численные результаты серий экспериментов приведены в Таблице \ref{tab1}.
В ней представлены данные о числе решенных задач $S$, среднем числе итераций глобального поиска на верхнем уровне рекурсии $G_{av}$, среднем числе испытаний на нижнем уровне $L_{av}$, среднем времени решения задачи $t_{av}$ (в секундах).

\begin{table}[ht]
	\caption{Результаты решения серий тестовых задач}
	\label{tab1}
	\begin{center}
		\begin{tabular}{ l c c c c c c } \hline
		 & $S$ &  $G_{av}$ &  $L_{av}$ & $t_{av}$ \\
    \hline
		GRIS-L & 20/20  & 705 &  378 545 & 1.3 \\
		GRIS-LQ & 20/20 & 705 &  420 397 & 1.4 \\
		GKLS-L & 18/20 & 9531 &  3 899 417 & 16.4 \\
		\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{biblio}

\bibitem{Evtushenko2009} Евтушенко Ю. Г., Малкова В. У., Станевичюс А.-И. А. Параллельный поиск глобального экстремума функций многих переменных // {\it Журн. вычисл. матем. и матем. физ.} 2009. Т. 49. № 2. C. 255--269.

\bibitem{Paulavicius2016} Paulavi{\v c}ius R., {\v Z}ilinskas J. Advantages of Simplicial Partitioning for Lipschitz Optimization Problems with Linear Constraints // {\it Optim. Lett.} 2016. V. 10. No. 2. P. 237--246.

\bibitem{Grishagin2007} Городецкий С. Ю., Гришагин В. А. Нелинейное программирование и многоэкстремальная оптимизация. Н. Новгород: Изд-во ННГУ, 2007. 489 с.

\bibitem{Strongin13} Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А. Параллельные вычисления в задачах глобальной оптимизации. М.: Изд-во МГУ, 2013. 280 с.

\bibitem{Sergeyev2013} Sergeyev, Y.~D. and Strongin, R.~G. and Lera, D. Introduction to global optimization exploiting space-filling curves // {\it Journal of Global Optimization}, Springer. 2013. V. 60, No. 3. P. 595--596.

\bibitem{Grishagin2001} Sergeyev Y., Grishagin V. Parallel Asynchronous Global Search and the Nested Optimization Scheme //  {\it J. Comput. Anal. Appl.} 2001. V. 3, No. 2. P. 123--145.

%\bibitem{Sergeyev2008} Сергеев Я. Д., Квасов Д. Е. Диагональные методы глобальной оптимизации. М.: Физматлит, 2008. 352 с.


\end{biblio}

\newpage

\title{Separation of parameters using machine learning methods in global optimization problems}

\authors{K.A.~Barkalov,  M.A.~Usova}
\organizations{Lobachevsky State University of Nizhny Novgorod \\ Nizhny Novgorod, Russia}

% abstract is contained in the  abstract environment
\abstracten{
The paper presents the findings from research into an approach to solving global optimization problems with a different nature of dependence on diverse groups of parameters.
A scheme for choosing the problem parameters, which have a local effect on the objective function is proposed, which allows to solve essentially  multidimensional problems using the nested optimization scheme.
At the same time, different optimization algorithms are used at differing levels of recursion (nesting levels).
The proposed approach has demonstrated its efficiency in solving several series of test problems.
The effectiveness of the proposed approach was studied.}

\keywordsen{global optimization, local optimization, nested optimization scheme, high-dimensional problems, parameter separation}

\begin{biblioen}

\bibitem{Evtushenko2009} Evtushenko Yu. G., Malkova V. U., Stanevichyus A. A. Parallel global optimization of functions of several variables. // {\it USSR Computational Mathematics and Mathematical Physics} 2009. V. 49, No. 2. P. 255--269.

\bibitem{Paulavicius2016} Paulavi{\v c}ius R., {\v Z}ilinskas J. Advantages of Simplicial Partitioning for Lipschitz Optimization Problems with Linear Constraints // Optim. Lett. 2016. V. 10. No. 2. P. 237--246.

\bibitem{Grishagin2007} Gorodetskii S. Yu., Grishagin V. A. Nonlinear Programming and Optimization Multiextremal. Nizhny Novgorod, UNN Publ, 2007. 489 p.

\bibitem{Strongin13} Strongin, R.G., Gergel, V.P., Grishagin, V.A., Barkalov K.A. Parallel Computations in the Global Optimization Problems. Moscow, MSU Publ, 2012. 280 p.

\bibitem{Sergeyev2013} Sergeyev, Y.~D. and Strongin, R.~G. and Lera, D. Introduction to global optimization exploiting space-filling curves // Journal of Global Optimization, Springer. 2013. V. 60, No. 3. P. 595--596.

\bibitem{Grishagin2001} Sergeyev Y., Grishagin V. Parallel Asynchronous Global Search and the Nested Optimization Scheme // J. Comput. Anal. Appl. 2001. V. 3, No. 2. P. 123--145.

\end{biblioen}

\end{document}
