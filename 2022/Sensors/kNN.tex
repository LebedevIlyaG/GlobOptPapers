%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[entropy,article,submit,moreauthors,pdftex]{Definitions/mdpi} 

% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerospace, agriculture, agriengineering, agronomy, ai, algorithms, allergies, analytica, animals, antibiotics, antibodies, antioxidants, appliedchem, applmech, applmicrobiol, applnano, applsci, arts, asi, atmosphere, atoms, audiolres, automation, axioms, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomechanics, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, brainsci, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coatings, colloids, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, crops, cryptography, crystals, curroncol, cyber, dairy, data, dentistry, dermato, dermatopathology, designs, diabetology, diagnostics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, entropy, environments, environsciproc, epidemiologia, epigenomes, fermentation, fibers, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fractalfract, fuels, futureinternet, futuretransp, futurepharmacol, futurephys, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, hazardousmatters, healthcare, hearts, hemato, heritage, highthroughput, histories, horticulturae, humanities, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijtm, ijtpp, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jcdd, jcm, jcp, jcs, jdb, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jzbg, kidney, land, languages, laws, life, liquids, literature, livers, logistics, lubricants, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, metabolites, metals, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, minerals, mining, modelling, molbank, molecules, mps, mti, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nri, nursrep, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photochem, photonics, physchem, physics, physiolsci, plants, plasma, pollutants, polymers, polysaccharides, proceedings, processes, prosthesis, proteomes, psych, psychiatryint, publications, quantumrep, quaternary, qubs, radiation, reactions, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, risks, robotics, safety, sci, scipharm, sensors, separations, sexes, signals, sinusitis, smartcities, sna, societies, socsci, soilsystems, solids, sports, standards, stats, stresses, surfaces, surgeries, suschem, sustainability, symmetry, systems, taxonomy, technologies, telecom, textiles, thermo, tourismhosp, toxics, toxins, transplantology, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, vetsci, vibration, viruses, vision, water, wevj, women, world 

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%%%% ДЛЯ РУССКОГО ТЕКСТА закомментировать потом!
%\usepackage{inputenc}
%\usepackage[T2A,T1]{fontenc}
%\usepackage[english,russian]{babel}
%\usepackage{cmap}
%%%%


%=================================================================
% MDPI internal commands
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2022}
\copyrightyear{2022}
%\externaleditor{Academic Editor: Firstname Lastname} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{} 
\dateaccepted{} 
\datepublished{} 
\hreflink{https://doi.org/} % If needed use \linebreak
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, paracol, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{A Fast kNN Algorithm Using Multiple Space-Filling Curves}
		   %Acceleration of Global Optimization Algorithm by Detecting Local Extrema Based on Machine Learning

% MDPI internal command: Title for citation in the left column
\TitleCitation{}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0001-5273-2471} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0003-1809-7173} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0000-0003-1542-7624} % Add \orcidC{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Konstantin Barkalov $^{1}$*\orcidA{} , Anton Shtanyuk $^{1}$\orcidB{} and Alexander Sysoyev $^{1}$\orcidC{}}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Konstantin Barkalov, Anton Shtanyuk and Alexander Sysoyev}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Barkalov, K.; Shtanyuk, A.; Sysoyev, A.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address[1]{%
$^{1}$ \quad Department of Mathematical Software and Supercomputing Technologies, Lobachevsky University, 603950 Nizhny Novgorod, Russia; anton.shtanyuk@itmm.unn.ru (A.Sh.), alexander.sysoyev@itmm.unn.ru (A.S.)}
%$^{2}$ \quad Affiliation 2; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: konstantin.barkalov@itmm.unn.ru (K.B.)}
%; Tel.: (optional; include country code; if there are multiple corresponding authors, add author initials) +xx-xxxx-xxx-xxxx (F.L.)}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{
The paper considers a time-efficient implementation of the $k$ nearest neighbours (kNN) algorithm. A well-known approach for accelerating the kNN algorithm is to utilize dimensionality reduction methods based on the use of space-filling curves. In this paper, we take this approach further and propose an algorithm that employs multiple space-filling curves and is faster (with comparable quality) compared to the kNN algorithm which uses kd-trees to determine the nearest neighbours. A specific method for constructing multiple Peano curves is outlined, and statements are given about the preservation of object proximity information in the course of dimensionality reduction. An experimental comparison with known kNN implementations using kd-trees was performed using test and real-life data.
}

% Keywords
\keyword{machine learning, kNN, dimensionality reduction, multiple space-filling curves} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Currently, machine learning (ML) methods are being used successfully to solve a wide range of problems in various application areas. One example of a class of problems where ML has demonstrated its effectiveness are the tasks of identifying the main properties of the phenomena, which are characterized by their stochastic nature or the presence of hidden parameters \cite{Golovenkin2020,Gonoskov2019,Kastalskiy2021}. 

In many cases, solving application problems comes down to the problem of classification, i.e. assigning the object under study to one of the available classes. One of the well-known classification methods is the kNN ($k$ nearest neighbours) method. Although the earliest papers related to this method appeared more than $50$ years ago \cite{Cover1967, Cover1968}, theoretical studies of various aspects of using kNN are still going on \cite{Pestov2013,Mirkes2020}. This is not to mention hundreds of publications in which this method is used to solve applied classification problems. Over many years of use, kNN has established itself as a simple and reliable method that yields acceptable results when solving many different problems. 

Among the undoubted merits of kNN is its ``explainability'', since the decision about whether a test object belongs to one class or another is clearly explained by similar properties of the test object and its nearest (in some metric) neighbours.
Along with the study of the method's theoretical properties, the issue of its effective implementation is also investigated. Thus, direct implementation of the method by using the  ``brute-force'' approach has computational complexity $O(n)$, where $n$ is the number of objects. Fast algorithms, which have computational complexity $O(log(n))$, use complex data structures of kd-tree type \cite{Bentley1975MultidimensionalBS}.

One approach to accelerating the kNN algorithm is to use dimensionality reduction methods involving space-filling curves (Peano-Hilbert curves) \cite{Liao2001, Schubert2015FastAS}. In this paper, we develop this approach further and propose an algorithm that employs multiple space-filling curves and is faster (with comparable quality) compared to the algorithm using kd-trees. A specific method for constructing multiple Peano curves is outlined, and statements are given about the preservation of object proximity information in the course of dimensionality reduction. Comparison with known kNN implementations (in particular, those using kd-trees) was performed using test and real-life data with small dimensionality, i.e., just those data for which kd-trees show the best performance in terms of speed.

The paper is organized as follows. Section \ref{ps} contains the problem statement and description of the kNN method. Section \ref{Peano} describes the scheme for constructing a Peano-Hilbert curve approximation (evolvent) and the kNN method, which uses the evolvent to reduce the dimensionality of the data. The fundamental drawbacks of this dimensionality reduction method are noted. Section \ref{MPeano} proposes a way to overcome the drawbacks mentioned, based on the use of multiple evolvents. A theoretical statement is formulated about preserving some proximity information in the multidimensional space on one of the one-dimensional scales. A kNN scheme using multiple evolvents is presented in Section \ref{knnme}. Section \ref{results} contains experimental results comparing different implementations of kNN using synthetic and real-life data. Section \ref{conclusions} concludes the paper.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statement}\label{ps}

In this study, the classification problem will be understood as the problem of assigning an object $\omega$  to one of the predetermined classes based on its features. We will assume that each of the objects $\omega$ is represented as a feature vector 
\begin{equation}
y = (y_1, y_2, ..., y_N) \in R^N,
\end{equation} 
where the value of the $j$-th vector coordinate corresponds to the numerical value of the $j$-th feature of the object $\omega$.
We will also assume that there is already some number of objects exactly classified (a test sample), i.e. for each object we know which class it belongs to.

The $k$ nearest neighbours (kNN) method is based on the following simple rule: an object is considered to belong to the class to which most of its nearest neighbours belong. Here, ``neighbours'' refers to objects that are close (in one sense or another) to the one being studied.
Note that in order to apply this method, some metric $L(\omega_i, \omega_j)$, i.e. the distance function, should be introduced in the feature space of objects. As a rule, the Euclidean distance is employed here, although other metrics may also be used \cite{Mirkes2020}.

The general scheme of object classification using kNN can be formulated as follows:
\begin{itemize}
\item calculate the distance from the object being classified to each of the objects in the training set;
\item select $k$ objects of the training set with the minimum distance to them;
\item the class of the object being classified is the class most often found among the $k$ nearest neighbours.
\end{itemize}

In the practical implementation of kNN, an important indicator is the estimate of the time complexity of the neighbour finding procedure. The algorithms and data structures on which the search procedure is based include, in particular, the following.


\begin{itemize}
\item Brute force method. It is based on calculating all distances from the test object to other objects of the class and determining the smallest value. This method has complexity $O(n)$, where $n$ is the number of objects.
\item KD-tree method. It is based on a special kind of binary tree, where each node represents a point in the multidimensional space. The search procedure has complexity $O(log(n))$.
\item Ball-tree method. It is yet another kind of tree structure, which has logarithmic complexity. This method is applicable to problems that have a large dimensionality.
\end{itemize}

The main drawback of the brute force method is the unacceptable running time and the rapid growth of the computation volume as the number of objects increases. This method can be applied when the number of objects is relatively small and the dimension of $y$ is small, and when the ``curse of dimensionality'' is not yet in full effect.

For kd-tree, the main drawback is that it slows down when the number of objects grows, which is caused by increasing complexity of the internal tree structure. However, kd-trees have proven to be a good solution for problems with small dimensionality, which will be considered in this study.

As it was mentioned, the computational cost of searching for nearest objects increases with increasing dimensionality, both in the case of exhaustive search for all distances and when using tree structures. One mechanism for speeding up the search is dimensionality reduction, by reducing the problem of searching in a multidimensional space to the search in a one-dimensional space (over an interval). This is possible through the use of space-filling curves (Peano-Hilbert curves) that fill the multidimensional space. Such curves are used, for example, in global optimization \cite{Lera2015,Lera2018,Strongin2018,Gergel2021}, in numerical approximation of solutions to systems of nonlinear inequalities \cite{Lera2021}, in image processing \cite{Liang2008,Costa2012,Herrero2015}, etc.

An algorithm based on data curves can offer some advantages over tree-based algorithms by reducing the search time. Neighbour detection for test objects includes two phases: building and initialising the data structures (build step) and performing a search or query (query step). In most cases, the build phase occurs only once, while queries are repeated many times and can significantly affect the overall speed of the classification algorithm. 

Note that in practical classification tasks, different features can have different scales, which can significantly distort the real distance between objects. Therefore it will be assumed that prior to applying kNN data scaling has been performed 
\begin{equation}
y_j = \frac{y_j - y_{min}} {y_{max} - y_{min}} - \frac{1} {2}.
\end{equation} 

Thus, the variation domain for all feature values will form a unit hypercube
\begin{equation}\label{D}
D = \{ y \in R^N: -1/2 \leq y_j \leq 1/2,  1 \leq j \leq N \}.
\end{equation} 
The $1/2$ hypercube offset  has been made for the convenience of further labelling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dimensionality reduction using space-filling curves}\label{Peano}

Let us briefly describe a general scheme for constructing a space-filling curve. We will consider here Hilbert's scheme for constructing such a curve.


1. Divide hypercube $D$ form (\ref{D}) with edge length of $1$ by coordinate hyperplanes into $2^N$ hypercubes of the first partition (with edge length of $1/2$).

Then divide every hypercube of the first partition into $2^N$  hypercubes of the second partition (with edge length of $1/4$) by hyperplanes parallel to coordinate hyperplanes and passing through midpoints of hypercube edges orthogonal to them.

By continuing the above process, i.e. by sequentially partitioning each subcube of the current partition into $2^N$ of the next partition, we can construct hypercubes of any $M$-th partition with edge length of $(1/2)^M$. The total number of subcubes of the $M$-th partition will be equal to $2^{NM}$.

2. Now divide the segment $[0, 1]$ into $2^N$ equal parts, divide  each of them also into $2^N$ equal parts and so on. Denote the subinterval of the $M$-th partition by $d(M,v)$, where $v$ is the coordinate of the left boundary point of the subinterval. Obviously, the length of the subinterval $d(M,v)$ will be equal to $2^{-NM}$. We will assume that the left boundary point belongs to the subinterval and the right boundary point does not. The only exception is the subinterval whose right boundary point is $1$, in which case it also belongs to the subinterval.

3. Establish a one-to-one correspondence between subintervals and subcubes of the $M$-th partition. We denote by $D(M,v)$ a subcube corresponding to the subinterval $d(M,v)$. This correspondence should satisfy the following requirements.

Condition 1. $D(M+1,v^\prime) \in D(M,v^{\prime\prime})$ i.f.f. $d(M+1,v^\prime) \in d(M,v^{\prime\prime})$.

Condition 2. Two subintervals $d(M,v^\prime)$ and $d(M,v^{\prime\prime})$ share a common endpoint i.f.f. the corresponding subcubes $D(M,v^\prime)$ and $D(M,v^{\prime\prime})$  are  contiguous, i.e. share a common edge.

Condition 3.  If $x \in d(M,v)$ then $y(x) \in D(M,v), M \geq 1$.  

Note that the centre  $y^c(x)$ of the $M$-th partition subcube containing point $y(x)$ can be regarded as an approximation to $y(x)$ with the accuracy of $\varepsilon = 2^{-(M+1)}$, i.e.
\begin{equation}
\max_{1 \leq j \leq N} \left| y_j^c(x)-y_j(x) \right| \leq \varepsilon = 2^{-(M+1)}.
\end{equation} 

The function $y^c(x)$ maps a uniform grid with step $2^{-NM}$, constructed in the interval $[0,1]$, onto a uniform grid with step $2^{-M}$, constructed in the hypercube $D$. A constructive way of establishing such a correspondence is described and theoretically justified in \cite{Strongin2000, Sergeyev2013}.

Using the Peano-Hilbert curve, one can propose the following scheme for implementing the kNN algorithm.

Let $\omega_i, 1 \leq i \leq S$, be the $i$-th object from the set of objects correlated with class $C_j$. Previously, a feature vector $y = (y_1, y_2, ..., y_N) \in R^N$, corresponding to each object under consideration was defined, i.e. 
\[
\omega_i \leftrightarrow y = (y_{i1}, y_{i2}, ..., y_{iN}).
\] 

Using the Peano curve, each object is assigned the value of $x_i$, i.e.
\[
\omega_i \leftrightarrow x_i, \; y(x_i) = (y_{i1}, y_{i2}, ..., y_{iN}).
\] 

Consider a test object $\omega^*$, to which the value $x^*$ is assigned. The degree of proximity of the test object $\omega^*$ to one of the objects $\omega_i$ is determined based on the distance on the one-dimensional scale $d_i = |x_i - x^* |$.
Thus, the closest object to the test object has the following property:
\[
d_{min} = \min \{ d_i: 1 \leq i \leq S \}.
\] 

The operation of the algorithm to determine the nearest neighbour of the test object involves two phases: data preparation and the search itself. Data preparation consists of calculating the correspondence $\omega_i \rightarrow x_i$ for each input object, including the test object, and ordering the objects by the value of $x$.
\[
x^1(\omega^1) \leq x^2(\omega^2) \leq ... \leq x^S(\omega^S).
\] 

Since the corresponding object preimages $x_i \in [0,1]$ are ordered, the closest object can be found using a fast binary search algorithm.

When $K$ neighbours need to be found, this search procedure can be repeated $K$ times. In this case it is necessary to exclude previously found items from the search. This can be done by excluding $x_i$ from the set being searched or by comparing the found $x_i$ with the previously found ones to find a match. For example, one possible implementation of searching for $K$ nearest neighbours could be as follows: in relation to $x^*$, we look for the nearest values to the left and right on the one-dimensional scale, choose the minimum one and continue searching sideways until exactly $K$ neighbours are found.

One obvious drawback of using the Peano curve in the kNN method is the loss of much of the information about the proximity of objects in multidimensional space when constructing their preimages on the one-dimensional scale. 

Indeed, the point $x \in [0,1]$ on the one-dimensional scale has only left and right neighbours, whereas its corresponding object $y(x) \in R^N$ can have neighbours along $N$ coordinate directions. As a result, when using Peano curve type mappings, close objects $y^\prime$, $y^{\prime\prime}$ in the $N$-dimensional space can have their quite distant corresponding preimages $x^\prime$, $x^{\prime\prime}$ on the interval $[0,1]$. In Figure~\ref{fig:Peano} the green dots show two objects that are close in the two-dimensional space, while their preimages on the one-dimensional scale are far away from each other. The blue dots correspond to two objects, the distance between which is retained when changing to the one-dimensional scale.

\begin{figure}
\center
\includegraphics[width=0.5\linewidth]{fig1.png}
\caption{Peano curve}\label{fig:Peano}
\label{fig1}
\end{figure}   

This property makes dimensionality reduction using a single Peano curve practically inapplicable. To overcome this drawback, various approaches have been proposed, for example, the simultaneous use of two or more space-filling curves of different types \cite{Scott1999}, data shifting \cite{Liao2001}. In the following section, we propose a constructive way to preserve some information on object proximity during dimensionality reduction, based on the use of multiple same-type evolvents.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Constructing a family of Peano curves}\label{MPeano}

Consider a family of Peano curves
\begin{equation}
Y_L(x)=\{ y^0(x), y^1(x), ..., y^L(x) \}
\end{equation} 
instead of a single curve $y(x)$. We will construct the family of curves as follows. Let us introduce a family of hypercubes
\begin{equation}\label{hypercubes}
D_i = \{ y \in R^N: -2^{-1} \leq y_i + 2^{-l} \leq 3 \cdot 2^{-1},  1 \leq i \leq N \}, \; 0 \leq l \leq L,
\end{equation} 
where the hypercube $D_{l+1}$ is obtained by shifting the hypercube $D_l$ along the main diagonal by step $2^{-l}$ for each coordinate.

Let a Peano curve type mapping $y^0(x)$ map the interval $[0,1]$ onto the hypercube $D_0$ of (\ref{hypercubes}), i.e.
\begin{equation}\label{hypercubeD0}
D_0 = \{y^0(x): x \in [0, 1] \}.
\end{equation} 

Any subcube of the $M$-th partition of the hypercube $D_0$ will have an edge of length $2^{-(M-1)}$ and will be denoted by $D_0(M, v)$, where $v$ is the left boundary point of the subinterval $d(M, v)$ corresponding to this subcube. Then the evolvents $y^l(x)=\{y_1^l(x), ..., y_N^l(x)\}$, whose coordinates are determined by the conditions
\begin{equation}\label{maps}
y_i^l(x) = y_i^{l-1}(x) + 2^{-l}, \; 1 \leq i \leq N, \; 1 \leq l \leq L,
\end{equation} 
map the interval $[0,1]$ onto the corresponding hypercubes  $D_l, \; 1 \leq l \leq L$ (the broken line in Figure~\ref{fig2} shows the image of the interval $[0,1]$, which is obtained by using the evolvent $y^0(x), x \in [0,1]$).

For any subcube $D_0(M,v)$ of the $M$-th partition of the hypercube $D_0$, there will exist a subcube $D_l(M,v)$ of the $M$-th partition of the hypercube $D_l$, and $D_l(M,v)$ can be obtained by shifting $D_0(M,v)$ along the main diagonal by the distance of 
\[
2^{-1}+2^{-2}+...+2^{-l}.
\] 

It follows from formulae (\ref{hypercubeD0}) and (\ref{maps}) that if an interval $d(M,v)$ is mapped onto the subcube $D_0(M,v)$, then there exists a family of subcubes
\[
D_l(M,v) = y^l(d(M,v)), 1 \leq l \leq L,
\]
connected with the corresponding subintervals $d_l(M,v_l) \subset [0,1]$, where $d(M,v) = d_0(M,v_0), v_0 = v$, such that 
\[
D_l(M,v) = y^l(d_l(M,v)), 1 \leq l \leq L.
\] 

Since the hypercube $D$ from (\ref{D}) belongs to the common part of the family of hypercubes (\ref{hypercubes}) (the boundary of the hypercube $D$ is highlighted in Figure~\ref{fig2}), then, by introducing an additional function 
\[
g_0(y) = \max \{ |y_i|-2^{-1}: \; 1 \leq i \leq N \},
\] 
the original hypercube $D$ can be represented as
\[
D = \{y^l(x): x \in [0,1], \; g_0(y^l(x)) \leq 0 \}, \; 0 \leq l \leq L,
\] 
i.e. $g_0(y) \leq 0$, if $y \in D$, and $g_0(y) > 0$ if otherwise. Hence, any point $y \in D$ has its preimage $x^l \in [0,1]$ at every mapping $y^l(x), \; 0 \leq l \leq L$, i.e.
\[
y = y^l(x^l), \; x^l \in [0, 1], \; 0 \leq l \leq L.
\] 

By using multiple mappings $y^l(x), \; 0 \leq l \leq L$, the following relationship between neighbourhoods on one-dimensional scales and neighbourhoods in the original multidimensional domain is defined.

\textbf{Theorem 1.} Let $y^*$ be an arbitrary point from the domain $D$ belonging to the interval with endpoints $y^\prime, y^{\prime\prime} \in D$, which differ in the values of the only coordinate, and let
\[
|y_j^\prime - y^{\prime\prime}_j| \leq 2^{-p}, \; y^\prime_i = y^{\prime\prime}_i = y_i^*, \; 1 \leq i \leq N, i \ne j,
\] 
where $p$ is an integer, $1 \leq p \leq L-1$, and $j$ is the number of coordinate whose values for points $y^*, y^\prime, y^{\prime\prime}$ are different. Then there exists at least one correspondence $y^l(x), 0 \leq l \leq L$, and preimages $x^*, x^\prime, x^{\prime\prime} \in [0, 1]$ such that
\[
 y^* = y^l(x^*), y^\prime = y^l(x^\prime), y^{\prime\prime} = y^l(x^{\prime\prime}), \\
\max \{ |x^\prime - x^*|, |x^{\prime\prime} - x^*|, |x^\prime - x^{\prime\prime}| \} \leq 2^{-pN}.
\] 

\textbf{Remark}. The conditions of the theorem distinguish a specific neighbourhood of the point $y^*$. This neighbourhood comprises only such neighbours of this point which can be obtained by  shifting $y^*$ parallel to one of the coordinate axes by a distance not exceeding $2^{-p}$. By varying the value of $j, 1 \leq j \leq N$, under the conditions of the theorem, one can identify the nearest neighbours of point $y^*$ along $N$ coordinate directions. According to the statement, the proximity of points in the $N$-dimensional space in a particular direction will be reflected by the proximity of their preimages on one of the one-dimensional segments. In this case, the corresponding one-dimensional intervals can be different for different directions.

The proof of this theorem is given in \cite{Strongin2000}.

As an illustration, Figure~\ref{fig2} shows a family of space-filling curves in extended domains $D_l$, where a square with a dark border highlights the hypercube $D$ belonging to each extended domain. The dots indicate proximate objects in the domain $D$, the distance between which will vary depending on the one-dimensional scale used. The effect of maintaining distance proximity on one of the scales is clearly visible. For example, objects marked with green dots will be far away on the first two one-dimensional scales and close to each other on the third one. Objects marked in blue will be far away on the first and third scales and close to each other on the second scale. 

\end{paracol}

\begin{figure}	
\widefigure
\includegraphics[width=1.0\linewidth]{fig2.png}
\caption{Family of Peano curves\label{fig2}}
\end{figure}  

\begin{paracol}{2}
\linenumbers
\switchcolumn

\section{Implementation features of the kNN method using multiple space-filling curves}\label{knnme}

The scheme of implementation of the kNN algorithm using multiple Peano curves will look as follows.
For every object $\omega_i$ a set of its preimages on different one-dimensional scales is assigned 
\[
\omega_i \rightarrow \{ x_i^0, x_i^1, ..., x_i^L \},
\] 
where $l, \; 0 \leq l \leq L$, is the number of the one-dimensional interval.

The degree of proximity of a test object $\omega^*$ to one of the known objects $\omega_i$ can be expressed in terms of the distance
\[
d_i^l = |x_i^l - x^{*l}|
\]
on the $l$-th scale.

Thus, the distance $d(\omega_i,\omega^*)$ between two objects $\omega_i$ and $\omega^*$ on a one-dimensional scale will correspond to the minimum distance among all the mappings 
\[
d(\omega_i,\omega^*) = \min_{0\leq l\leq L} d_i^l = \min_{0\leq l\leq L}  |x_i^l - x^{*l}| .
\]
If we find the minimum distance
\[
d_{min} = \min_{1\leq i\leq n} d(\omega_i,\omega^*) ,
\]
we get the nearest neighbour.

Next, the search procedure needs to be run as many times as there are nearest neighbours to be identified. The main problem is that an object previously found on one scale can be found again, including on another scale. This requires either removing the values of $x$ found objects from all scales or searching for $K$ nearest neighbours within each scale and then merging the results.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Implementation features of the kNN method using multiple space-filling curves}

The procedure of finding $K$ nearest neighbours using multiple space-filling curves was implemented in two ways. The first of them (denoted hereafter as Algorithm A) involves searching for $K$ neighbours on each scale and then merging the results and selecting $K$ neighbours from all scales. The peculiarity of this algorithm is that there is no procedure to remove the objects found from the scales.

The second way (Algorithm B) involves finding the nearest object to the test object on each scale, determining the nearest object among those found, and then removing the selected object from all scales. In this case, the search for the next nearest neighbour is performed by repeatedly calling the same procedure. After all nearest neighbours are determined, the previously excluded objects are returned to their places and the algorithm is ready to work with the next test object.

Here is a more detailed description of these algorithms.
\\[12pt]
\textit{Data preparation}
\\[12pt]
To run Algorithms A and B, we need to compute the preimages $\{x_i^0, x_i^1, ..., x_i^L\}$ on one-dimensional scales for all objects $\omega_i$, and order them in ascending order. It is convenient to use RB-tree-based data structures to store ordered one-dimensional values. The number of instances of tree structures is equal to the number of scales.

The input data for kNN are the values of object features imported into the program from external files. A text label indicating the class to which the object belongs is read with each such object.

The input data of the algorithm also includes the feature vector $y^*$ corresponding to the test object $\omega^*$.\\

%\\[12pt]
\underline{Procedure A}

\begin{itemize}
\item For each scale, search for $K$ objects closest to $\omega^*$. The result of the search is a set of vectors of  the nearest objects on each scale.
\item The vectors of the objects found are inserted into an RB-tree according to the distance criterion $d_{min}$.
\item After processing all the scales, the first $K$ objects are extracted from the resulting tree and they are considered to be the search result.
\item The number of objects belonging to each class is counted and the decision is made whether   the test object belongs to a particular class.
\end{itemize}

%\\[12pt]
\underline{Procedure B}

\begin{itemize}
\item For each scale, search for the closest object to $\omega^*$; the result is a set of distances between the test object and the closest object for each scale.
\item The object with minimal distance on all scales is selected.
\item This object is saved in a special vector and excluded from the set of objects.
\item The whole procedure is repeated $K$ times, resulting in a vector of $K$ nearest neighbours.
\item The number of objects belonging to each class is counted and the decision is made whether   the test object belongs to a particular class.
\item If re-classification is required, the procedure is called to recover the previously excluded objects and search for neighbours.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}\label{results}

Let us compare the kNN method that uses multiple evolvents (hereafter referred to as kNN-ME) with the kNN method using KD-tree (hereafter referred to as kNN-KD) \cite{Hou2018} to handle multidimensional data.

The study was performed using several datasets, both randomly generated for testing and real-world  datasets from the database \url{https://archive.ics.uci.edu/ml/index.php}:
\begin{enumerate}
\item
\textbf{DS-Random-1} -- a set of random data generated using the random module of the Python NumPy library. The data has a uniform distribution and contains values on an interval from 0 to 1.

\item
\textbf{DS-Random-2} -- a set of random data generated by the scikit-learn library (version 1.0.2) function \texttt{sklearn.datasets.make\_classification}.

\item
\textbf{DS-SkinSegmentation} -- a dataset of dimension 3 (RGB values) to distinguish human skin colour from other objects in images \cite{ds-1}.

\item
\textbf{DS-CarEvaluation} -- a dataset of dimension 6, representing data from a car valuation model based on the values of 6 features \cite{ds-2}.
\end{enumerate}

Two kinds of studies were carried out:
\begin{itemize}
\item The time required to find $K$ nearest neighbours for a test object was measured;
\item The accuracy of recognition of test objects was studied, based on the analysis as to whether their neighbours belong to certain classes.
\end{itemize}

We employed in our experiments the kNN-KD method from the scikit-learn library; this method was used with default settings. For our kNN-ME method, we used the evolvent density $m=12$, the number of evolvents simultaneously used was $L=10$ for datasets 1 -- 3 and $L=7$ for the last dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Test infrastructure}

The computational experiments were performed on the following test infrastructure (Table \ref{tab:t1}).

\begin{specialtable}[H] 
    \caption{Test infrastructure}\label{tab:t1}
	\center
\begin{tabular}{ll}
\toprule
	  Computer specifications & \\
\midrule													
		CPU & Intel Core i3-7100 (3.9 HHz) \\
		RAM & 8 GB \\ 
		Operating system & Windows 10 \\
		Compiler & Intel(R) oneAPI DPC++/C++ Compiler, Version 2022.0.0 \\
\bottomrule
\end{tabular}
\end{specialtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Investigation of neighbour search times using test datasets}

Several datasets were synthesized to examine the search time for neighbours. First, DS-Random-1 and DS-Random-2 sets consisting of $100\;000$ and $10\;000$ objects, respectively, were generated.

When generating DS-Random-1, the standard random module from the Python NumPy library was used. The data dimension $N$ varied from 2 to 4; the generated data had a uniform distribution on $[0, 1]$.

To generate the DS-Random-2 set, the \texttt{sklearn.datasets.make\_classification} function from the scikit-learn library (version 1.0.2) was used with the following parameters: 

\begin{itemize}
\item \texttt{n\_samples} = $10\;000$ (number of objects),
\item \texttt{n\_features} =  $N$ (dimensionality),
\item \texttt{n\_redundant} = 0 (number of redundant features),
\item \texttt{n\_informative} = $N - 1$  (number of informative features),
\item \texttt{n\_clusters\_per\_class} = 1 (number of clusters per class of objects generated).
\end{itemize}
  
The running times used by the different methods (kNN-ME, procedure A, kNN-ME, procedure B, kNN-KD) for DS-Random-1 and DS-Random-2 datasets with $N = 2$ are shown in Figure~\ref{fig3}. The abscissa axis represents the number of nearest neighbours that the algorithm is searching for, the ordinate axis represents the running time of the method.

\begin{figure}
\widefigure
\includegraphics[width=1.0\linewidth]{fig3.pdf}
\caption{Running times of the algorithms with $N = 2$\label{fig3}}
\end{figure}  

Running times of the methods for nearest neighbours search with $N = 3, 4$ are given in Figures \ref{fig4}, \ref{fig5} respectively.

\begin{figure}
\widefigure
\includegraphics[width=1.0\linewidth]{fig4.pdf}
\caption{Running times of the algorithms with $N = 3$\label{fig4}}
\end{figure}  

\begin{figure}
\widefigure
\includegraphics[width=1.0\linewidth]{fig5.pdf}
\caption{Running times of the algorithms with $N = 4$\label{fig5}}
\end{figure}  

All the methods considered show a small variation in running time when switching from one dimension to another. The number of objects being processed slightly increases the search time, and there is also a quasi-linear increase in time as the number of neighbours identified increases. All these results are in good agreement with theoretical estimates of the time complexity of the methods under study.

On the whole, the experimental results show that the kNN-ME method based on search without removal (procedure A) works on average 2 times faster than the method that involves removal of elements found (procedure B). In fact, both variants of the kNN-ME method are 5-7 times faster (depending on the dataset) than the kNN-KD method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Investigation of neighbour search times on real-world data}

Let us consider the DS-Skin set taken from the collection \cite{ds-1}. The objects in the set belong to two classes: the first class represents points in RGB colour space that belong to human skin images; the second class represents colour coordinates that do not belong to such images.

The number of objects of the first class is about $17\;000$, the number of objects of the second class is $43\;000$. The total number of objects examined is $60\;000$.

\begin{figure}[h]
\widefigure
\includegraphics[width=0.5\linewidth]{fig6.pdf}
\caption{Comparison of neighbour search times when determining the colour of an image point\label{fig6}}
\end{figure}

The results of performance comparison of the methods on real-world data are qualitatively similar to those obtained by comparison on test data. The kNN-ME method (implementation A) is 1.5 times faster than implementation B, and both implementations are at least 5 times faster than the kNN-KD method. Therefore, we will work with the kNN-ME(A) when investigating classification accuracy.

The performance of kNN-ME and kNN-KD methods was also tested using a real-world dataset \cite{ds-2}. There are 4 classes of objects in this dataset of dimension $N = 6$. One particular feature of the dataset is the fact that the number of objects belonging to each of the 4 classes differs widely (Table \ref{tab:t2}).

\begin{specialtable}[H] 
  \caption{Classes in the CarEvaluation set}\label{tab:t2}
	\center
\begin{tabular}{cc}
\toprule
	  Object class & Number of objects in class \\
\midrule													
		1 & 1400 \\ 
		2 & 400 \\
		3 & 70 \\
		4 & 65 \\
\bottomrule
\end{tabular}
\end{specialtable}

The time required to find $K$ nearest neighbours on the CarEvoluation dataset is shown in Figure~\ref{fig6a}. In this experiment, the number of evolvents used simultaneously was 7. 
\begin{figure}
\widefigure
\includegraphics[width=0.5\linewidth]{fig6a.pdf}
\caption{Comparison of neighbour search times for the CarEvoluation dataset\label{fig6a}}
\end{figure}
The results of the comparison on this dataset also demonstrate that the kNN-ME method is faster compared to the kNN-KD method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Investigation of classification accuracy}

According to the general idea of the kNN method, after finding $K$ nearest neighbours to the test object, an attempt is made to determine the class of the test object. The more neighbours belong to a certain class, the higher the probability that the test object belongs to the same class. We will choose an odd number of neighbours, so as to avoid a situation when the nearest objects in equal numbers belong to different classes.

Unfortunately, determining the class of the test object is prone to error. Errors may occur because of the ``location'' of the object under test on the hypercube edge in immediate proximity to objects of another class. Figure \ref{fig7} shows an example of a random test set where some objects of one class are surrounded by objects of an ``alien'' class.

\begin{figure}[h]
\widefigure
\includegraphics[width=0.7\linewidth]{fig7.png}
\caption{``Surrounded by aliens''\label{fig7}}
\end{figure}

DS-Random-2 with $N = 2, 3, 4$ and DS-Skin were used as datasets. The experiment to recognise the class of test objects was carried out as follows.

\begin{enumerate}
\item The source dataset was divided into two samples: a training sample and a test sample. The test sample size was 10\% of the initial number of objects.
\item $K$ neighbours were searched for each object in the test sample.
\item The class of an object under test was determined from the set of neighbours found.
\item The recognition result was checked against the known test object class and an error was recorded if there was a discrepancy with the result. The errors were summed up for the whole test sample.
\end{enumerate}

To divide the source dataset into a training sample and a test sample, we used the \texttt{sklearn.model\_selection.train\_test\_split} function from the scikit-learn library (version 1.0.2).

The results of investigating recognition errors on the synthetically generated random datasets (DS-Random-2) are given in Table \ref{tab:t3}. The numerical values in the table show the percentage of recorded recognition errors.

\begin{specialtable}[H] 
  \caption{Percentage of errors in DS-Random-2 recognition}\label{tab:t3}
	\center
	\begin{tabular}{ccccccccc}
\toprule
$K$ & \multicolumn{2}{c}{ $N=2$ } & & \multicolumn{2}{c}{$N=3$} & & \multicolumn{2}{c}{$N=4$} \\
		\noalign{\smallskip} \cline{2-3} \cline{5-6} \cline{8-9} \noalign{\smallskip}
  & kNN-KD & kNN-ME & & kNN-KD & kNN-ME & & kNN-KD & kNN-ME \\
\midrule													
5  & 5 & 5 & & 11 & 12 & & 6 & 8 \\
15 & 4 & 6 & & 10 & 11 & & 10 & 9 \\
25 & 5 & 7 & & 10 & 11 & & 7 & 9 \\
35 & 6 & 6 & & 11 & 13 & & 7 & 10 \\
45 & 7 & 6 & & 10 & 13 & & 7 & 10 \\
\bottomrule
\end{tabular}
\end{specialtable}

Based on the results of the experiment with random sets, we can conclude that the kNN-ME method has a recognition accuracy close to that of kNN-KD. The error rates of the methods differ, on average, by no more than 2-3\%.

It should be noted that both kNN-ME and kNN-KD demonstrated high error rates, which can be explained by the fact that when generating random data, numerous objects at the class boundaries are generated that are classified incorrectly.

The results of measuring the recognition accuracy on the real-world DS-Skin dataset show that on real-world data both methods show the same percentage of misclassification. The specific error percentages obtained with different numbers of neighbours $K$ are shown in Table \ref{tab:t4}.

\begin{specialtable}[H] 
  \caption{Percentage of DS-Skin recognition errors, $N=3$}\label{tab:t4}
	\center
\begin{tabular}{ccc}
\toprule
$K$ & kNN-KD & kNN-ME \\
\midrule													
5 & 0.5 & 0.5 \\
15 & 0.5 & 0.5 \\
25 & 0.5 & 0.5\\
35 & 1 & 1\\
45 & 1 & 1\\
\bottomrule
\end{tabular}
\end{specialtable}

Let us consider the results of determining the class of objects in the CarEvaluation set. For objects of classes 1 and 2, we generated test samples containing 20 objects, and for classes 3 and 4, samples containing 5 objects were generated. The kNN-ME and kNN-KD methods showed almost identical results: objects from the first two classes were identified with an average error not exceeding 5\%, while objects of classes 3, 4 were identified with an average error of 15\%. The average recognition error rates for all classes are shown in Table \ref{tab:t5}.

\begin{specialtable}[H] 
  \caption{Average error rate (in percent) for DS-CarEvaluation recognition, $N = 6$}\label{tab:t5}
	\center
\begin{tabular}{ccc}
\toprule
$K$ & kNN-KD & kNN-ME \\
\midrule													
5 & 8 & 11 \\
7 & 9 & 10 \\
9 & 8 & 10 \\
11 & 10 & 11 \\
13 & 9 & 11 \\
\bottomrule
\end{tabular}
\end{specialtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{conclusions}

The paper considers a classical machine learning method, the $k$ nearest neighbours (kNN) method, which is widely used in machine learning practice and has been proven to perform very well because of its ``explainability'' (the decision  regarding the choice of an object class can be clearly explained). The results of practical implementation of one of kNN acceleration approaches based on dimensionality reduction using space-filling curves are presented.

An implementation of the kNN algorithm that uses multiple space-filling curves to better transfer the metric properties of multidimensional space to one-dimensional scales is proposed. Theoretical statements are presented about the preservation of the proximity property of objects in multidimensional space and on one of one-dimensional scales when dimensionality reduction is performed.

The software implementation of the kNN method using multiple evolvents (kNN-ME) shows 5-7 times faster performance compared to the algorithm using kd-trees (kNN-KD). The comparison was conducted on test and real-world data sets of different size and dimensionality. 

Both algorithms demonstrate comparable performance, measured as the percentage of misclassification of objects from the test sample. The number of neighbours in the classification was varied from 5 to 100. When applied to the synthesized random data both kNN-ME and kNN-KD show a classification error rate of approximately 10\%, while with real-world data both methods show an error rate of about 1\%.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Patents}

%This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization and methodology, K.B.; software and validation, A.S., A.Sh.; formal analysis, K.B.; investigation, A.S.; data curation, A.Sh.; writing --- original draft preparation, K.B.; writing --- review and editing, K.B.; visualization, A.S.; funding acquisition, K.B. All authors have read and agreed to the published version of the manuscript.}

\funding{This research was funded by the Ministry of Science and Higher Education of the Russian Federation, agreement number 075-15-2020-808.}

\institutionalreview{Not applicable.}

\informedconsent{Not applicable.}

\dataavailability{The data analyzed in this study are openly available at\\ \url{https://archive.ics.uci.edu/ml/index.php}.}

%\acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments).}

\conflictsofinterest{The authors declare no conflict of interest.} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{paracol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\externalbibliography{yes}
\bibliography{bibliography}

\end{document}

