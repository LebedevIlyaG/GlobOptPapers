%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[sensors,article,submit,moreauthors,pdftex]{Definitions/mdpi} 

% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerospace, agriculture, agriengineering, agronomy, ai, algorithms, allergies, analytica, animals, antibiotics, antibodies, antioxidants, appliedchem, applmech, applmicrobiol, applnano, applsci, arts, asi, atmosphere, atoms, audiolres, automation, axioms, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomechanics, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, brainsci, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coatings, colloids, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, crops, cryptography, crystals, curroncol, cyber, dairy, data, dentistry, dermato, dermatopathology, designs, diabetology, diagnostics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, entropy, environments, environsciproc, epidemiologia, epigenomes, fermentation, fibers, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fractalfract, fuels, futureinternet, futuretransp, futurepharmacol, futurephys, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, hazardousmatters, healthcare, hearts, hemato, heritage, highthroughput, histories, horticulturae, humanities, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijtm, ijtpp, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jcdd, jcm, jcp, jcs, jdb, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jzbg, kidney, land, languages, laws, life, liquids, literature, livers, logistics, lubricants, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, metabolites, metals, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, minerals, mining, modelling, molbank, molecules, mps, mti, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nri, nursrep, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photochem, photonics, physchem, physics, physiolsci, plants, plasma, pollutants, polymers, polysaccharides, proceedings, processes, prosthesis, proteomes, psych, psychiatryint, publications, quantumrep, quaternary, qubs, radiation, reactions, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, risks, robotics, safety, sci, scipharm, sensors, separations, sexes, signals, sinusitis, smartcities, sna, societies, socsci, soilsystems, solids, sports, standards, stats, stresses, surfaces, surgeries, suschem, sustainability, symmetry, systems, taxonomy, technologies, telecom, textiles, thermo, tourismhosp, toxics, toxins, transplantology, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, vetsci, vibration, viruses, vision, water, wevj, women, world 

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%%%% ДЛЯ РУССКОГО ТЕКСТА закомментировать потом!
%\usepackage{inputenc}
%\usepackage[T2A,T1]{fontenc}
%\usepackage[english,russian]{babel}
%\usepackage{cmap}
%%%%


%=================================================================
% MDPI internal commands
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2022}
\copyrightyear{2022}
%\externaleditor{Academic Editor: Firstname Lastname} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{} 
\dateaccepted{} 
\datepublished{} 
\hreflink{https://doi.org/} % If needed use \linebreak
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, paracol, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{A Fast kNN Algorithm Using Multiple Space-Filling Curves}
		   %Acceleration of Global Optimization Algorithm by Detecting Local Extrema Based on Machine Learning

% MDPI internal command: Title for citation in the left column
\TitleCitation{}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0001-5273-2471} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0003-1809-7173} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0000-0003-1542-7624} % Add \orcidC{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Konstantin Barkalov $^{1}$*\orcidA{} , Anton Shtanyuk $^{1}$\orcidB{} and Alexander Sysoyev $^{1}$\orcidC{}}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Konstantin Barkalov, Anton Shtanyuk and Alexander Sysoyev}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Barkalov, K.; Shtanyuk, A.; Sysoyev, A.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address[1]{%
$^{1}$ \quad Department of Mathematical Software and Supercomputing Technologies, Lobachevsky University, 603950 Nizhny Novgorod, Russia; anton.shtanyuk@itmm.unn.ru (A.Sh.), sysoyev@vmk.unn.ru (A.S.)}
%$^{2}$ \quad Affiliation 2; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: konstantin.barkalov@itmm.unn.ru (K.B.)}
%; Tel.: (optional; include country code; if there are multiple corresponding authors, add author initials) +xx-xxxx-xxx-xxxx (F.L.)}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{
The paper considers a time-efficient implementation of the k-nearest neighborhood (kNN) algorithm. A well-known approach for accelerating the kNN algorithm is to utilize dimensionality reduction methods based on the use of space-filling curves. In this paper, we take this approach further and propose an algorithm that employs multiple space-filling curves and is faster (with comparable quality) compared to the kNN algorithm which uses kd-trees to determine the nearest neighbors. A specific method for constructing multiple Peano curves is outlined, and statements are given about the preservation of object proximity information in the course of dimensionality reduction. An experimental comparison with known kNN implementations using kd-trees was performed using test and real-life data.
}

% Keywords
\keyword{space-filling curves} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Currently, machine learning (ML) methods are being used successfully to solve a wide range of problems in various application areas. One example of a class of problems where ML has demonstrated its effectiveness are the tasks of identifying the main properties of the phenomena, which are characterized by their stochastic nature or the presence of hidden parameters \cite{Golovenkin2020, Gonoskov2019, Kastalskiy2021}.

In many cases, solving application problems comes down to the problem of classification, i.e. assigning the object under study to one of the available classes. One of the well-known classification methods is the kNN (k-nearest neighbourhood) method. Although the earliest papers related to this method appeared more than $50$ years ago \cite{Cover1967, Cover1968}, theoretical studies of various aspects of using kNN are still going on [Gorban, Pestov]. This is not to mention hundreds of publications in which this method is used to solve applied classification problems. Over many years of use, kNN has established itself as a simple and reliable method that yields acceptable results when solving many different problems. 

Among the undoubted merits of kNN is its ``explainability'', since the decision about whether a test object belongs to one class or another is clearly explained by similar properties of the test object and its nearest (in some metric) neighbours.
Along with the study of the method's theoretical properties, the issue of its effective implementation is also investigated. Thus, direct implementation of the method by using the  ``brute-force'' approach has computational complexity $O(n)$, where $n$ is the number of objects. Fast algorithms, which have computational complexity $O(log(n))$, use complex data structures of kd-tree type [kdtree-1,2] \cite{Bentley1975MultidimensionalBS}.

One approach to accelerating the kNN algorithm is to use dimensionality reduction methods involving space-filling curves (Peano-Hilbert curves) \cite{Liao2001, Schubert2015FastAS}. In this paper, we develop this approach further and propose an algorithm that employs multiple space-filling curves and is faster (with comparable quality) compared to the algorithm using kd-trees. A specific method for constructing multiple Peano curves is outlined, and statements are given about the preservation of object proximity information in the course of dimensionality reduction. Comparison with known kNN implementations (in particular, those using kd-trees) was performed using test and real-life data with small dimensionality, i.e., just those data for which kd-trees show the best performance in terms of speed.

The paper is organized as follows. Section 2 describes the scheme for constructing a Peano-Hilbert curve approximation (evolvent) and the kNN method, which uses the evolvent to reduce the dimensionality of the data. The fundamental drawbacks of this dimensionality reduction method are noted. Section 3 proposes a way to overcome the drawbacks mentioned, based on the use of multiple evolvents. A theoretical statement is formulated about preserving some proximity information in the multidimensional space on one of the one-dimensional scales. A kNN scheme using multiple evolvents is presented. Section 4 contains experimental results comparing different implementations of kNN using synthetic and real-life data. Section 5 concludes the paper.


\section{Problem Statement}

In this study, the classification problem will be understood as the problem of assigning an object $\omega$  to one of the predetermined classes based on its features. We will assume that each of the objects $\omega$ is represented as a feature vector 
\begin{equation}
y = (y_1, y_2, ..., y_N) \in R^N,
\end{equation} 
where the value of the $j$-th vector coordinate corresponds to the numerical value of the $j$-th feature of the object $\omega$.

We will also assume that there is already some number of objects exactly classified (a test sample), i.e. for each object we know which class it belongs to.

The $k$ nearest neighbours (kNN) method is based on the following simple rule: an object is considered to belong to the class to which most of its nearest neighbours belong. Here, "neighbours" refers to objects that are close (in one sense or another) to the one being studied.
Note that in order to apply this method, some metric $L(\omega_i, \omega_j)$, i.e. the distance function, should be introduced in the feature space of objects. As a rule, the Euclidean distance is employed here, although other metrics may also be used [Gorban].

The general scheme of object classification using kNN can be formulated as follows:
\begin{itemize}
\item calculate the distance from the object being classified to each of the objects in the training set;
\item select k objects of the training set with the minimum distance to them;
\item the class of the object being classified is the class most often found among the $k$ nearest neighbours.
\end{itemize}

In the practical implementation of kNN, an important indicator is the estimate of the time complexity of the neighbour finding procedure. The algorithms and data structures on which the search procedure is based include, in particular, the following.


\begin{itemize}
\item Brute force method. It is based on calculating all distances from the test object to other objects of the class and determining the smallest value. This method has complexity $O(n)$, where $n$ is the number of objects.
\item KD-Tree method. It is based on a special kind of binary tree, where each node represents a point in the multidimensional space. The search procedure has complexity $O(log n)$.
\item Ball-Tree method. It is yet another kind of tree structure, which has logarithmic complexity. This method is applicable to problems that have a large dimensionality.
\end{itemize}

The main drawback of the brute force method is the unacceptable running time and the rapid growth of the computation volume as the number of objects increases. This method can be applied when the number of objects is relatively small and the dimension of $y$ is small, and when the ``curse of dimensionality'' is not yet in full effect.

For KD-Tree, the main drawback is that it slows down when the number of objects grows, which is caused by increasing complexity of the internal tree structure. However, KD-trees have proven to be a good solution for problems with small dimensionality, which will be considered in this study.

As it was mentioned, the computational cost of searching for nearest objects increases with increasing dimensionality, both in the case of exhaustive search for all distances and when using tree structures. One mechanism for speeding up the search is dimensionality reduction, by reducing the problem of searching in a multidimensional space to the search in a one-dimensional space (over an interval). This is possible through the use of space-filling curves (Peano-Hilbert curves) that fill the multidimensional space. Such curves are used, for example, in multidimensional global optimization problems \cite{Strongin2018,Gergel2021}, in image processing \cite{Liang2008,Costa2012,Herrero2015}, etc.

An algorithm based on data curves can offer some advantages over tree-based algorithms by reducing the search time. Neighbour detection for test objects includes two phases: building and initialising the data structures (build step) and performing a search or query (query step). In most cases, the build phase occurs only once, while queries are repeated many times and can significantly affect the overall speed of the classification algorithm. 

Note that in practical classification tasks, different features can have different scales, which can significantly distort the real distance between objects. Therefore it will be assumed that prior to applying kNN data scaling has been performed 
\begin{equation}
y_j \equiv \frac{y_j - y_{min}} {y_{max} - y_{min}} - \frac{1} {2}.
\end{equation} 

Thus, the variation domain for all feature values will form a unit hypercube
\begin{equation}\label{D}
D = \{ y \in R^N: -1/2 \leq y_j \leq 1/2,  1 \leq j \leq N \}.
\end{equation} 
The $1/2$ hypercube offset  has been made for the convenience of further labelling.

\section{Dimensionality reduction using space-filling curves}

Let us briefly describe a general scheme for constructing a space-filling curve. We will consider here Hilbert's scheme for constructing such a curve.


1. Divide hypercube $D$ with edge length of $1$ by coordinate hyperplanes into $2^N$ hypercubes of the first partition (with edge length of $1/2$).

Then divide every hypercube of the first partition into $2^N$  hypercubes of the second partition (with edge length of $1/4$) by hyperplanes parallel to coordinate hyperplanes and passing through midpoints of hypercube edges orthogonal to them.

By continuing the above process, i.e. by sequentially partitioning each subcube of the current partition into $2^N$ of the next partition, we can construct hypercubes of any $M$-th partition with edge length of $(1/2)^M$. The total number of subcubes of the $M$-th partition will be equal to $2^{NM}$.

2. Now divide the segment $[0, 1]$ into $2^N$ equal parts, divide  each of them also into $2^N$ equal parts and so on. Denote the subinterval of the $M$-th partition by $d(M,v)$ , where $v$ is the coordinate of the left boundary point of the subinterval. Obviously, the length of the subinterval $d(M,v)$ will be equal to $2^{-NM}$. We will assume that the left boundary point belongs to the subinterval and the right boundary point does not. The only exception is the subinterval whose right boundary point is $1$, in which case it also belongs to the subinterval.

3. Establish a one-to-one correspondence between subintervals and subcubes of the $M$-th partition. We denote by $D(M,v)$ a subcube corresponding to the subinterval $d(M,v)$. This correspondence should satisfy the following requirements.

Condition 1. $D(M+1,v^\prime) \in D(M,v^{\prime\prime}) \;\: i.f.f. \;\:  d(M+1,v^\prime) \in d(M,v^{\prime\prime})$.

Condition 2. Two subintervals $d(M,v^\prime)$ and $d(M,v^{\prime\prime})$ share a common endpoint i.f.f. the corresponding subcubes $D(M,v^\prime)$ and $D(M,v^{\prime\prime})$  are  contiguous, i.e. share a common edge.

Condition 3.  If $x \in d(M,v)$ then $y(x) \in D(M,v), M \geq 1$.  

Note that the centre  $y^c(x)$ of the $M$-th partition subcube containing point $y(x)$ can be regarded as an approximation to $y(x)$ with the accuracy of $\varepsilon = 2^{-(M+1)}$, i.e.
\begin{equation}
\max_{1 \leq j \leq N} y_j^c(x)-y_j(x) \leq \varepsilon = 2^{-(M+1)}.
\end{equation} 

The function $y^c(x)$ maps a uniform grid with step $2^{-NM}$, constructed in the interval $[0,1]$, onto a uniform grid with step $2^{-M}$, constructed in the hypercube $D$. A constructive way of establishing such a correspondence is described and theoretically justified in \cite{Strongin2000, Sergeyev2013}.

Using the Peano-Hilbert curve, one can propose the following scheme for implementing the kNN algorithm.

Let $\omega_i, 1 \leq i \leq S$, be the $i$-th object from the set of objects correlated with class $C_j$. Previously, a feature vector $y = (y_1, y_2, ..., y_N) \in R^N$, corresponding to each object under consideration was defined, i.e. 
\begin{equation}
\omega_i \leftrightarrow y = (y_{i1}, y_{i2}, ..., y_{iN})
\end{equation} 

Using the Peano curve, each object is assigned the value of $x_i$, i.e.
\begin{equation}
\omega_i \leftrightarrow x_i, \; y(x_i) = (y_{i1}, y_{i2}, ..., y_{iN}).
\end{equation} 

Consider a test object $\omega^*$, to which the value $x^*$ is assigned. The degree of proximity of the test object $\omega^*$ to one of the objects $\omega_i$ is determined based on the distance on the one-dimensional scale $d_i = |x_i - x^* |$.
Thus, the closest object to the test object has the following property:
\begin{equation}
d_{min} = \min \{ d_i: 1 \leq i \leq S \}.
\end{equation} 

The operation of the algorithm to determine the nearest neighbour of the test object involves two phases: data preparation and the search itself. Data preparation consists of calculating the correspondence $\omega_i \rightarrow x_i$ for each input object, including the test object, and ordering the objects by the value of $x$.
\begin{equation}
x^1(\omega^1) \leq x^2(\omega^2) \leq ... \leq x^S(\omega^S).
\end{equation} 

Since the corresponding object preimages $x_i \in [0,1]$ are ordered, the closest object can be found using a fast binary search algorithm.

When $K$ neighbours need to be found, this search procedure can be repeated $K$ times. In this case it is necessary to exclude previously found items from the search. This can be done by excluding $x_i$ from the set being searched or by comparing the found $x_i$ with the previously found ones to find a match. For example, one possible implementation of searching for $K$ nearest neighbours could be as follows: in relation to $x^*$, we look for the nearest values to the left and right on the one-dimensional scale, choose the minimum one and continue searching sideways until exactly K neighbours are found.

One obvious drawback of using the Peano curve in the kNN method is the loss of much of the information about the proximity of objects in multidimensional space when constructing their preimages on the one-dimensional scale. 

Indeed, the point $x \in [0,1]$ on the one-dimensional scale has only left and right neighbours, whereas its corresponding object $y(x) \in R^N$ can have neighbours along $N$ coordinate directions. As a result, when using Peano curve type mappings, close objects $y^\prime$, $y^{\prime\prime}$ in the $N$-dimensional space can have their quite distant corresponding preimages $x^\prime$, $x^{\prime\prime}$ on the interval $[0,1]$. In Fig. 1 the green dots show two objects that are close in the two-dimensional space, while their preimages on the one-dimensional scale are far away from each other. The blue dots correspond to two objects, the distance between which is retained when changing to the one-dimensional scale.

\begin{figure}
\center
\includegraphics[width=0.5\linewidth]{fig1.png}
\caption{Peano curve}
\label{fig1}
\end{figure}   

This property makes dimensionality reduction using a single Peano curve practically inapplicable. To overcome this drawback, various approaches have been proposed, for example, the simultaneous use of two or more space-filling curves of different types \cite{Scott1999}, data shifting \cite{Liao2001}. In the following section, we propose a constructive way to preserve some information on object proximity during dimensionality reduction, based on the use of multiple same-type evolvents.

\section{Constructing a family of Peano curves}

Consider a family of Peano curves
\begin{equation}
Y_L(x)=\{ y^0(x), y^1(x), ..., y^L(x) \}
\end{equation} 
instead of a single curve $y(x)$. We will construct the family of curves as follows. Let us introduce a family of hypercubes
\begin{equation}\label{hypercubes}
D_i = \{ y \in R^N: -2^{-1} \leq y_i + 2^{-l} \leq 3 \cdot 2^{-1},  1 \leq i \leq N \}, \; 0 \leq l \leq L,
\end{equation} 
where the hypercube $D_{l+1}$ is obtained by "shifting" the hypercube $D_l$ along the main diagonal by step $2^{-l}$ for each coordinate.

Let a Peano curve type mapping $y^0(x)$ map the interval $[0,1]$ onto the hypercube $D_0$ of (\ref{hypercubes}), i.e.
\begin{equation}\label{hypercubeD0}
D_0 = \{y^0(x): x \in [0, 1] \}.
\end{equation} 

Any subcube of the $M$-th partition of the hypercube $D_0$ will have an edge of length $2^{-(M-1)}$ and will be denoted by $D_0(M, v)$, where $v$ is the left boundary point of the subinterval $d(M, v)$ corresponding to this subcube. Then the evolvents $y^l(x)=\{y_1^l(x), ..., y_N^l(x)\}$, whose coordinates are determined by the conditions
\begin{equation}\label{maps}
y_i^l(x) = y_i^{l-1}(x) + 2^{-l}, \; 1 \leq i \leq N, \; 1 \leq l \leq L,
\end{equation} 
map the interval $[0,1]$ onto the corresponding hypercubes  $D_l, \; 1 \leq l \leq L$ (the broken line in the figure shows the image of the interval $[0,1]$, which is obtained by using the evolvent $y^0(x), x \in [0,1]$).

For any subcube $D_0(M,v)$ of the $M$-th partition of the hypercube $D_0$, there will exist a subcube $D_l(M,v)$ of the $M$-th partition of the hypercube $D_l$, and $D_l(M,v)$ can be obtained by shifting $D_0(M,v)$ along the main diagonal by the distance of $2^{-1}+2^{-2}+...+2^{-l}$. 

It follows from formulae (\ref{hypercubeD0}) and (\ref{maps}) that if an interval $d(M,v)$ is mapped onto the subcube $D_0(M,v)$, then there exists a family of subcubes
\begin{equation}
D_l(M,v) = y^l(d(M,v)), 1 \leq l \leq L,
\end{equation} 
connected with the corresponding subintervals $d_l(M,v_l) \subset [0,1]$, where $d(M,v) = d_0(M,v_0), v_0 = v$, such that 
\begin{equation}
D_l(M,v) = y^l(d_l(M,v)), 1 \leq l \leq L,
\end{equation} 

Since the hypercube $D$ from (\ref{D}) belongs to the common part of the family of hypercubes (\ref{hypercubes}) (the boundary of the hypercube $D$ is highlighted in Fig.~\ref{fig2}), then, by introducing an additional function 
\begin{equation}
g_0(y) = \max \{ |y_i|-2^{-1}: \; 1 \leq i \leq N \},
\end{equation} 
the original hypercube $D$ can be represented as
\begin{equation}
D = \{y^l(x): x \in [0,1], \; g_0(y^l(x)) \leq 0 \}, \; 0 \leq l \leq L,
\end{equation} 
i.e. $g_0(y) \leq 0$, if $y \in D$, and $g_0(y) > 0$ if otherwise. Hence, any point $y \in D$ has its preimage $x^l \in [0,1]$ at every mapping $y^l(x), \; 0 \leq l \leq L$, i.e.
\begin{equation}
y = y^l(x^l), \; x^l \in [0, 1], \; 0 \leq l \leq L.
\end{equation} 

By using multiple mappings $y^l(x), \; 0 \leq l \leq L$, the following relationship between neighborhoods on one-dimensional scales and neighborhoods in the original multidimensional domain is defined.

\textbf{Theorem 1.} Let $y^*$ be an arbitrary point from the domain $D$ belonging to the interval with endpoints $y^\prime, y^{\prime\prime} \in D$, which differ in the values of the only coordinate, and let

\begin{equation}
|y_j^\prime - y^{\prime\prime}_j| \leq 2^{-p}, \; y^\prime_i = y^{\prime\prime}_i = y_i^*, \; 1 \leq i \leq N, i \ne j,
\end{equation} 
where $p$ is an integer, $1 \leq p \leq L-1$, and $j$ is the number of coordinate whose values for points $y^*, y^\prime, y^{\prime\prime}$ are different. Then there exists at least one correspondence $y^l(x), 0 \leq l \leq L$, and preimages $x^*, x^\prime, x^{\prime\prime} \in [0, 1]$ such that

\begin{equation}
 y^* = y^l(x^*), y^\prime = y^l(x^\prime), y^{\prime\prime} = y^l(x^{\prime\prime}), \\
\max \{ |x^\prime - x^*|, |x^{\prime\prime} - x^*|, |x^\prime - x^{\prime\prime}| \} \leq 2^{-pN}.
\end{equation} 

\textbf{Remark}. The conditions of the theorem distinguish a specific neighborhood of the point $y^*$. This neighborhood comprises only such neighbours of this point which can be obtained by  shifting $y^*$ parallel to one of the coordinate axes by a distance not exceeding $2^{-p}$. By varying the value of $j, 1 \leq j \leq N$, under the conditions of the theorem, one can identify the nearest neighbours of point $y^*$ along $N$ coordinate directions. According to the statement, the proximity of points in the $N$-dimensional space in a particular direction will be reflected by the proximity of their preimages on one of the one-dimensional segments. In this case, the corresponding one-dimensional intervals can be different for different directions.

The proof of this theorem is given in \cite{Strongin2000}.

As an illustration, Figure \ref{fig2} shows a family of space-filling curves in extended domains $D_l$, where a square with a dark border highlights the hypercube $D$ belonging to each extended domain. The dots indicate proximate objects in the domain $D$, the distance between which will vary depending on the one-dimensional scale used. The effect of maintaining distance proximity on one of the scales is clearly visible. For example, objects marked with green dots will be far away on the first two one-dimensional scales and close to each other on the third one. Objects marked in blue will be far away on the first and third scales and close to each other on the second scale. 

\end{paracol}

\begin{figure}	
\widefigure
\includegraphics[width=1.0\linewidth]{fig2.png}
\caption{Family of Peano curves\label{fig2}}
\end{figure}  

\begin{paracol}{2}
\linenumbers
\switchcolumn

The scheme of implementation of the kNN algorithm using multiple Peano curves will look as follows.

For every object $\omega_i$ a set of its preimages on different one-dimensional scales is assigned 

\begin{equation}
\omega_i \rightarrow \{ x_i^0, x_i^1, ..., x_i^L \}
\end{equation} 
where $l, \; 0 \leq l \leq L$, is the number of the one-dimensional interval.

The degree of proximity of a test object to one of the known objects can be expressed in terms of the distance on the $l$-th scale.

\begin{equation}
d_i^n = |x_i^n - x^{*n}|
\end{equation} 

Thus, the distance $d(\omega_i,\omega_j)$ between two objects $\omega_i$ and $\omega_j$ on a one-dimensional scale will correspond to the minimum distance among all the mappings. If we find the minimum distance among all these distances, we get the nearest neighbour

\begin{equation}
d_{min} = \min \{ d_i^n \} = \min \{ |x_i^n - x^{*n}| \}.
\end{equation} 

Next, the search procedure needs to be run as many times as there are nearest neighbours to be identified. The main problem is that an object previously found on one scale can be found again, including on another scale. This requires either removing the values of x found objects from all scales or searching for $K$ nearest neighbours within each scale and then merging the results.


\section{Special features of the implementation of the kNN method using multiple space-filling curves}

The procedure of finding $K$ nearest neighbours using multiple space-filling curves was implemented in two ways. The first of them (denoted hereafter as Algorithm A) involves searching for $K$ neighbours on each scale and then merging the results and selecting $K$ neighbours from all scales. The peculiarity of this algorithm is that there is no procedure to remove the objects found from the scales.

The second way (Algorithm B) involves finding the nearest object to the test object on each scale, determining the nearest object among those found, and then removing the selected object from all scales. In this case, the search for the next nearest neighbour is performed by repeatedly calling the same procedure. After all nearest neighbours are determined, the previously excluded objects are returned to their places and the algorithm is ready to work with the next test object.

Here is a more detailed description of these algorithms.
\\[12pt]
\textit{Data preparation}
\\[12pt]
To run Algorithms A and B, we need to compute the preimages $\{x_i^0, x_i^1, ..., x_i^L\}$ on one-dimensional scales for all objects $\omega_i$, and order them in ascending order. It is convenient to use RB-tree-based data structures to store ordered one-dimensional values. The number of instances of tree structures is equal to the number of scales.

The input data for kNN are the values of object features imported into the program from external files. A text label indicating the class to which the object belongs is read with each such object.

The input data of the algorithm also includes the feature vector $y^*$ corresponding to the test object $\omega^*$.

%\\[12pt]
\underline{Procedure A}

\begin{itemize}
\item For each scale, search for $K$ objects closest to $\omega^*$. The result of the search is a set of vectors of  the nearest objects on each scale.
\item The vectors of the objects found are inserted into an RB-tree according to the distance criterion $d_{min}$.
\item After processing all the scales, the first $K$ objects are extracted from the resulting tree and they are considered to be the search result.
\item The number of objects belonging to each class is counted and the decision is made whether   the test object belongs to a particular class.
\end{itemize}

%\\[12pt]
\underline{Procedure B}

\begin{itemize}
\item For each scale, search for the closest object to $\omega^*$; the result is a set of distances between the test object and the closest object for each scale.
\item The object with minimal distance on all scales is selected.
\item This object is saved in a special vector and excluded from the set of objects.
\item The whole procedure is repeated $K$ times, resulting in a vector of K nearest neighbours.
\item The number of objects belonging to each class is counted and the decision is made whether   the test object belongs to a particular class.
\item If re-classification is required, the procedure is called to recover the previously excluded objects and search for neighbours.
\end{itemize}


\section{Experimental Results}


\begin{specialtable}[H] 
	\caption{The average number of tests when minimizing Shekel test functions (the number of unsolved problems is indicated in parentheses)}\label{table:average_Shekel}
	\center
\begin{tabular}{cccc}
\toprule
        & \textbf{$\epsilon = 10^{-4}$} & \textbf{$\epsilon = 10^{-3}$} & \textbf{$\epsilon = 10^{-2}$} \\
\midrule													
DIRECT         & 64(1) &  34(6)   & 20(17)    \\
GSA            & 106  & 53  &  31   \\ 
GSA-DT         & 49   & 43  &  35   \\

\bottomrule
\end{tabular}
\end{specialtable}




\end{paracol}


\begin{paracol}{2}
\linenumbers
\switchcolumn




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Patents}

%This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization and methodology, K.B.; software and validation, A.S., A.Sh.; formal analysis, K.B.; investigation, A.S.; data curation, A.Sh.; writing --- original draft preparation, K.B.; writing- -- review and editing, K.B.; visualization, A.S.; funding acquisition, K.B. All authors have read and agreed to the published version of the manuscript.}

\funding{This research was funded by the Ministry of Science and Higher Education of the Russian Federation, agreement number 075-15-2020-808.}

\institutionalreview{Not applicable.}

\informedconsent{Not applicable.}

\dataavailability{The data analyzed in this study are openly available at https:// .}

%\acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments).}

\conflictsofinterest{The authors declare no conflict of interest.} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{paracol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\externalbibliography{yes}
\bibliography{bibliography}

\end{document}

