%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
\usepackage{marvosym}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\usepackage{graphicx}

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\usepackage[english, russian]{babel} 
\usepackage{amsmath} 
\def\UrlFont{\rmfamily}
\providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
%
\title{On the use of decision trees to identify the local extrema in parallel global optimization algorithms}
%
\titlerunning{Global Optimization ...}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{K.~A.~Barkalov\inst{1}\letter\orcidID{0000-0001-5273-2471} \and I.~G.~Lebedev\inst{1}\letter\orcidID{0000-0002-8736-0652} \and D.~I.~Silenko\inst{1}\letter\orcidID{0000-0002-2578-9699}}
%
\authorrunning{K.~A.~Barkalov et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{K.~A.~Barkalov and I.~G.~Lebedev, and D.~I.~Silenko}
%
\institute{Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia \\
\email{konstantin.barkalov@itmm.unn.ru, ilya.lebedev@itmm.unn.ru, rotor12587@mail.ru}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
In the present work, the solving of the multidimensional global optimization problems using decision  tree to reveal the attractor regions of the local minima is considered. The objective function of the  problem is defined as a 'black box''.  We assume it only to satisfy Lipschitz condition with unknown constant.  Global search algorithm is applied for the search of global minimum in the problems of such type. In the present work, we propose a method for selecting the nearness of local extrema of the objective  function based on analysis of accumulated search information. Conducting such an analysis using machine learning tools allows making a decision to run a local  method, which can speed up the convergence of the algorithm.  This suggestion was confirmed by the results of numerical experiments demonstrating the speedup  when solving a series of test problems.
% We would like to encourage you to list your keywords within
% the abstract section using the \keywords{...} command.
\keywords{Global optimization $\cdot$ Multiextremal functions $\cdot$ Parallel computing $\cdot$  Decision tree}
\end{abstract}
%

\section{Introduction}

In the present work, we consider the parallel algorithms for solving the multidimensional global  optimization problems. Such problems arise often in the cases, when it is necessary to select the values  of parameters of the mathematical model being investigated, which the results of modeling fit best to  the experimental data at. For solving the problems of the class specified, many algorithms are known: from the ones based on the  idea of random search \cite{fio_bib1, fio_bib2, fio_bib3} metaheuristic algorithms to the deterministic  algorithm guarantying convergence to the global minimum \cite{fio_bib4, fio_bib5, fio_bib6}.  

Since in real global optimization problems each computing of the function value (hereafter called  \textit{a trial}) is a very computation-costly operation, one has to reduce the number of such trials. It  can be achieved by intentional choice of variants in the course of search for the optimal solution cutting  off unpromising search subdomains and investigating only these ones, which the solution of the  problem can be found in. The global search algorithm (GSA) is based on this idea \cite{fio_bib7}. In  the present work, we tried to combine GSA and a local optimization method (Hooke-Jeeves method)  to reduce the number of trials executed. The decision on the run of the local method will be made using  a decision tree.

The local optimization algorithms are intended for determining only one of local extrema within a set  of feasible solutions, which the objective function takes the extremal (maximum or minimum) value in.  Among the local methods,  the zero-order methods and the gradient methods (of the 1\textsuperscript{st} order and of the  2\textsuperscript{nd} one) are distinguished traditionally \cite{fio_bib8,fio_bib9}. Note that the application of the first-order methods (not to mention the second one) in the problems  with the “black-box” type functions is difficult since the problem of numerical estimating the gradient  arises here. Therefore, further we will consider the zero-order methods only. 

In order to determine, which moment it is best to run the local method within the framework of the  global search at we will use an algorithm based on a decision tree. At the first search stage, the search information is accumulated in the form of the results of the trials  executed. We used the trial results for training the decision tree that allows obtaining a piecewise linear  approximation of the objective function and predicting the objective function behavior on its base.  To do so, we will compare the function values at the points neighboring to the one, which we consider  to be suspicious for a local minimum and, based on the piecewise linear approximation, make a decision  whether the next trial point falls into the attractor area of a local minimum  or not.

The main part of the paper has the following structure.  In Section \ref{SecPS}, the global optimization problem statement is considered, the basic suggestions  on the objective function are made. In Section \ref{SecA}, the description of the methods and approaches used is given. In particular, in Section \ref{SecGSA},  the main idea of global search algorithm is discussed, its computational rules are given. Section \ref{SecHG} contains the description of the local search method used. The method of constructing the approximation of the function using the decision trees is described in  Section \ref{SecDT}. The novel algorithm combining the global search and the local one on the base of using the decision  trees is presented in Section \ref{SecGSAL}. In the next section,  the features of the asynchronous parallelization scheme of the new algorithm are discussed. Section \ref{SecR} contains the results of experiments , carried out using a parallel computer system  with distributed memory. In Conclusion, main conclusions on the work done are presented.


\section{Problem statement}\label{SecPS}

Let us consider a problem of searching the global minimum of a function $\varphi(y)$ in a hyperinterval  $D=\{ y\in\ R^N:\ a_i\le\ y_i\le\ b_i,\ 1\le\ i\le\ n \}$. 

\begin{equation} \label{sec:problem}   
	\varphi(y^*) = min\{\varphi(y):y\in D\}, D = \{y \in R^N : a_i \leq y_i \leq b_i, 1 \leq i \leq N \},
\end{equation}

where $a,b \in R$ are given vectors.

Also, assume the function to satisfy the Lipschitz condition with \textit{a priori} unknown constant  $L$ that corresponds to limited variation of the function values at limited variation of the argument.  generating the changes in the system being simulated.   

\begin{displaymath} 
	|\varphi(y_1)-\varphi(y_2)|\leq L\parallel y_1-y_2 \parallel ,y_1,y_2 \in D, 0<L< \infty. 
\end{displaymath}

This suggestion can be interpreted (with respect to the applied problems ) as the reflection of limited  power 

The numerical solving of the problem (\ref{sec:problem})  is reduced to constructing an estimate  $ y_k^\ast\in\ D$ matching some understanding of nearness to the point $y^\ast$ (for example,  ${||y^\ast-y}_k^\ast||\le\ \varepsilon$ where $\varepsilon\geq0$ is a predefined precision) based on a  finite number $k$ of the objective function values computed. With respect to the class of the problems  considered, it is suggested that the optimized function $\varphi(y)$ can be defined algorithmically, as  a result of executing some subroutine or library.

The solving of the multiextremal optimization problems is much more computation-costly  as compared  to other types of optimization problems since the global optimum is an integral characteristic of the  problem being solved and requires investigating the whole search domain. As a result, the search of the  global optimum is reduced to constructing some coverage (grid) in the search domain and the choice of  the best function value on this grid. When using the uniform grids, the computation costs of solving the  problem grow exponentially with increasing dimensionality.



\section{The algorithms used}\label{SecA}

The main idea of the approach to constructing more efficient nonuniform grid is that its points are  computed sequentially while the objective function is considered as a realization of some random  process. The decision rules of the grid building algorithm are constructed in such a way that the next  grid point corresponds to the global minimum point of the mathematical expectation of the function  values. This point is stored in the list of known values, and the iterations are repeated until one of  selected stop criteria is satisfied: either the distance between the trial points becomes less than a  predefined value or a preset maximum number of iterations is achieved \cite{fio_bib10}.

When solving the multidimensional problems, the dimensionality reduction (i.e. the reduction of the  multidimensional problem to an equivalent one-dimensional one) using Peano curves is applied. These  ones allow reducing a multidimensional optimization problem in the domain $D$  to a one-dimensional  minimization problem within the interval $[0, 1]$

\begin{displaymath}
	\varphi(y(x^\ast))\ =\ min\{\varphi(y(x)):\ x\epsilon[0,\ 1]\}
\end{displaymath}


where the function $\varphi(y(x^\ast))$ satisfies more general H{\"o}lder condition

\begin{displaymath}
	\left|\varphi (y \left(x_1\right))- \varphi (y \left(x_2\right)\right )|\le\ H\left|x_1-x_2\right|^\frac{1}{N},\ x_1,\ x_2\epsilon[0,1].
\end{displaymath} 


Therefore, instead of the initial problem of minimizing the function $\varphi(y)$ in the domain $D$,  one can consider the minimization of the one-dimensional function $f(x)\ =\ \varphi(y(x))$ satisfying  the H{\"o}lder condition for $ x\in [0,1]$.



\subsection{Multidimensional parallel global search algorithm}\label{SecGSA}

The main steps of the parallel global search algorithm are as follows.

At the preliminary step, $p$ trials are executed in parallel in arbitrary internal points $x^1, ...,x^p$ of  the interval $[0,1]$ that corresponds to the first iteration of the algorithm. 

If $n \geq 1$ iterations are completed, which $k=k(n)$ executed trials in the points $x^i, 1\leq i\leq  k$ correspond to, the points $x^{k+1},\ldots,x^{k+p}$ of the search trials at the next $(n+1)^{\rm  th}$ iteration will be computed as the result of performing the following operations.

\begin{enumerate}
	
	\item  Renumber (by the lower indices) the points $x^i, 1\leq i\leq k$ as well as the boundary points of  the interval [0,1] in the order of increasing coordinate:  
	\begin{equation} 
		\label{agp1_sort} 	0=x_0<\ x_1<\ ...\ <x_{k+1}=1. 	
	\end{equation} 
	and juxtapose them with the values $z_i=f(x_i)$. 
	
	\item  Compute current lower estimate $M$ of the unknown H{\"o}lder constant $H$:  
	\begin{equation} 
		\label{agp2_mu} 	\mu=max\left\{\frac{|z_i-z_{i-1}|}{{{(x}_i-x_{i-1})}^{1/N}},\ i=1,\ldots,k\right\},\ M=\  \left\{\begin{matrix}r\mu,\ \mu>0,\\1,\ \mu=0,\\\end{matrix}\right.\ 	
	\end{equation} 
	
	
	where $r>1$ is a parameter of algorithm. This parameter controls the reliability of the algorithm: the  higher values of $r$ ensure guaranteed finding of the global minimum, the choice of lower value ---  speeds up the convergence of the algorithm. 
	
	\item  For each interval $(x_{i-1},x_i), 1\leq i\leq k+1,$ compute the value $R(i)$ called \textit{a  characteristic} of the interval according to the formulae 
	
	
	\begin{equation} 
		\label{agp3_R1} R(1)=2\Delta_1-4\dfrac{z_1}{M}, \; R(k+1)=2\Delta_{k+1}-4\dfrac{z_k}{M}, 
	\end{equation} 
	
	
	
	\begin{equation} 
		\label{agp3_Ri} R(i)=\Delta_i+\dfrac{(z_i-z_{i-1})^2}{M^2\Delta_i}-2\dfrac{z_i+z_{i-1}}{M},1<i<k+1, 
	\end{equation} 
	
	
	where \(\Delta_i=(x_i-x_{i-1})^\frac{1}{N}\).
	
	\item   Arrange the characteristics $R\left(i\right),\ 1\leq i \leq k+1,$ in the order of non-increasing 
	
	
	\begin{equation} 
		\label{agp4_R_sort} 	R\left(t_1\right)\geq\ R\left(t_2\right)\geq...\geq\ R\left(t_k\right)\geq\ R(t_{k+1}),\  
	\end{equation} 
	
	
	and select $p$ intervals with the indices $t_j,\ 1\le\ j\le\ p$ with the highest values of characteristics.
	
	\item Compute the points $x^{k+j},\ 1\leq j\leq p$ in the selected intervals according to the formulae
	
	 
\begin{equation}
	\label{agp5_x1}
	x^{k+j}=\frac{x_{t_j}+x_{t_j-1}}{2},\ t_j=1,\ t_j=k+1,
\end{equation}	
\begin{equation}
	\label{agp4_xi}	
	x^{k+1}=\frac{x_{t_j}+x_{t_j-1}}{2}-sign\left(z_{t_j}-z_{t_j-1}\right)\frac{1}{2r}\left[\frac{\left|z_{t_j}-z_{t_j-1}\right|}{\mu}\right]^N,\ 1<t_j<k+1.
\end{equation}
	
	
\end{enumerate}

The next $p$ trials are executed in parallel in the points $x^{k+j},\ 1\leq j\leq p$ computed according  to the formulae (\ref{agp5_x1}), (\ref{agp4_xi}). Upon completing the trials, the results of these ones  are stored in the information database, and the algorithm goes to computing new trial points.

Note that as a rule the process of executing a trial in the applied optimization problems is much more  computation-costly as compared to computing the trial point.

The algorithm stops in the case if the condition \(\Delta_{t_j} < \varepsilon\) is satisfied for even a  single value $t_j,\ 1\le\ j\le\ p$ from (\ref{agp4_R_sort}). This stop criterion (along with the criterion limiting the number of executed iterations usual for the  iteration methods) is used in the applied optimization problems, which the global minimum point  $x^*$ is unknown \textit{a priori} in. 

When solving the test problems, which the global minimum point $x^*$ is known in one can use the  stop criterion upon falling into the nearness of the global minimum as well. In this case, the method  stops if the condition $\left|x_{t_j}-\ x^\ast\right| < \varepsilon$ is satisfied even for one value of $t_j,\  1\le\ j\le\ p$ from (\ref{agp4_R_sort}). 

As the final estimate of the globally optimized solution of the considered problem, the values 


\begin{equation} 
	f_k^*=\min_{1\leq i \leq k}f(x_i), \; x_k^*=arg \min_{1\leq i \leq k}f(x_i). 
\end{equation} 


are taken. 

The substantiation of this method of organization of computations see in \cite{fio_bib20}. The  modifications taking into account the presence of  constraints (inequalities) in the problem as well as  the information on the derivative of the objective function  are presented in \cite{fio_bib12, fio_bib9,  fio_bib11}.



\subsection{Hooke--Jeeves method}\label{SecHG}

The Hooke--Jeeves method belongs to the class of zero-order methods. Its computation rules are a  combination of the investigating search (to select the direction) and search in the direction selected  \cite{fio_bib14, fio_bib15}.

The investigating search is performed as follows:  \begin{enumerate} \item	The step value is determined (it is different for each coordinate and can be varied in the course  of search).  \item	The search step is considered to be successful if the value of the objective function in the check  point doesn't exceed the value of the objective function in the initial point.  \item	Otherwise, it is necessary to return to the previous item and make a step in the reverse direction.  \item	After making the steps in all $N$ coordinates, the investigating search is completed. The point  obtained is called the base point. \end{enumerate}

With regard to the search in a direction, it consists in performing a step from the base point found  during the investigating search along the straight line direction connecting this one with the previous  base point. The magnitude of this step is defined by a parameter set in advance.

\begin{figure}[!h] 
	\begin{center} 
		\begin{minipage}[h]{0.8\linewidth} 
			\includegraphics[width=1\linewidth]{figure/fig1.png} 
			\caption{An example of iterations of the Hooke-Jeeves algorithm} %%  подпись к рисунку 
			\label{fig:fig1} 
		\end{minipage} 
	\end{center} 
\end{figure}	



\subsection{Decision tree}\label{SecDT}

The decision tree is a tool used for an automated analysis of big data arrays, which is applied in the  machine learning. Decision tree is a binary tree (a tree, which each non-leaf node has two child nodes  in). It can be used for solving the classification problems as well as the regression ones. When solving  the classification problems, each leaf of the tree is marked by a class label, and several leafs can have  the same labels. In the case of constructing a regression, a constant is assigned to each leaf of the tree.   Therefore, the approximating function obtained is a piecewise-constant one. In our case, the decision tree constructs the function $\varphi(y)$ as a piecewise-constant  approximation $\varphi(y)$  in multidimensional space. Let us denote the value  computed by the  decision tree in the point $y$ as $z' = \psi(y)$.

When implementing the algorithm for solving the multidimensional global optimization problems using  decision tree, we used the algorithms from OpenCV library to find the attractor areas of the local  minima. OpenCV is a library of the algorithms of computer vision, image processing, and  general-purpose numerical algorithms with open code. More details on the decision tree can be found  in \cite{fio_bib16}.

\section{Combination of local optimization method and GSA for solving multidimensional  problems}\label{SecGSAL}

In the current section, let us present a detailed description how we used the decision tree for finding  the attractor areas of the local extrema.  In the course of running GSA, it is necessary to determine whether it is worth to use current point as a  start one for the local method or not. To do so, one can check the points of adjacent trials. If among  these ones there are no points such that the function values in which are lower than in current one, one  can suggest that we are in the attractor of a local minimum. In this case, one can run the local method  from current point, which will converge to a local minimum of the function rapidly.  One can do it in the multidimensional space only, not in one-dimensional interval after the  dimensionality reduction.  First, the reduced function $\varphi(y(x))$ changes its properties: one local minimum in the  multidimensional space may divide into a set of minima after the dimensionality reduction.  Second, after the use of the mapping $y(x)$, we can lose the information on the mutual arrangement of  the points in the original space. The points located close to each other in the multidimensional space  may appear to be separated essentially in the one-dimensional interval.

\begin{figure}[ht!] 
	\begin{center} 
		\begin{minipage}[h]{0.6\linewidth} 
			\includegraphics[width=1\linewidth]{figure/fig2.png} 
			\caption{A function from the GKLS generator before using Peano curves} %%  подпись к рисунку 
			\label{fig:fig2} 
		\end{minipage} 
	\end{center} 
\end{figure}	

\begin{figure}[ht!] 
	\begin{center} 
		\begin{minipage}[h]{0.6\linewidth} 
			\includegraphics[width=1\linewidth]{figure/fig2.png} 
			\caption{A figure with a two-dimensional decision tree} %% подпись к  рисунку 
			\label{fig:fig2_2} 
		\end{minipage} 
	\end{center} 
\end{figure}


Therefore, we will determine the attractor areas of the local extrema in the original multidimensional  space. However, the determining of the adjacent points in the multidimensional space is  computational-costly enough. To do so, we will use the decision tree. 

After completing a definite number of trials (for example, $100\ \ast\ N$), let us use all accumulated  points to initialize a suitable data structure and training the decision tree on its base. To make the  determining the adjacent points easier, we construct a uniform grid with certain step in each coordinate.  Afterwards, we compute the values of the approximation of the objective function in these points  using the decision tree. Now, having a uniform grid of points and knowing the function values in each  point, let us find the closest point to the initial one (from the viewpoint of the Euclidian distance). This  closest point is a projection of the initial point onto the uniform grid, which its neighbors can be  determined easily on. Since the decision tree constructs the approximation of the initial function (even  piecewise constant), we can estimate the function values in the areas, which the trials haven’t been  executed earlier in.

When considering the neighbors, it is necessary to take into account the following. If even a single  neighbor has a lower function value than current point, there is no need to run the local method. If in  even a single neighboring point the function value is the same as in the current point, it is also necessary  to check (i. e. to find all its neighbors and to heck these ones in the same way). Only in the case when  all function values in the neighboring points are greater than in the current point, we can run the local  method.

Fig. \ref{fig:fig2} presents a contour plot of an objective function and the trial points; Fig.  \ref{fig:fig2_2} presents corresponding decision tree. 



\section{Asynchronous algorithm parallelization scheme}\label{SecASP}

The method of parallel computing organization described in  previous section implies synchronous  performing of $p$ search trials in the points computed at current iteration. The next iteration will be  started upon completing all trials only. In the case of different computation costs of executing the trials in different points of the search  domain, this approach may lead to a misbalance of the computing jobs.  This drawback can be corrected by introduction of asynchrony. The idea of the asynchronous scheme is not to wait for completing all $p$ trials but to initiate the  execution of a new trial immediately upon completing at one trial. 

Let us assume that the master process computes one point of the next trial at every iteration and sends  it to the performing process to execute the trial. Note that executing a trial by the performing process in  the applied optimization problems is much more computation costly than the choice of a new trial point  by the master process. Therefore, the idle time of the performing processes will be negligible. In this  case (unlike the synchronous parallel algorithms) total number of trials executed by each performing  process will depend on the computation costs for executing particular trials and cannot be estimated in  advance. When describing the parallel algorithm, let us assume that we have $p+1$ computational  processes at our disposal: one master process and $p$ performing ones.

In the beginning of search, the master process (assume it to have the identifier $0$) initiates parallel  execution of $p$ trials in $p$ different points of the search domain.

Two of these points are the boundary ones, the rest are the internal points i. e. in the points  $\{y\left(x^1\right),y\left(x^2\right),\ldots,y\left(x^p\right)\}$ where $x^1=,x^p=,x^i\in\left(0,1\right),i=2,\ldots,p$.

Now assume $k\geq0$ trials to be completed, and the performing processes are performing the trials in  the points $y\left(x^{k+1}\right),\;y\left(x^{k+2}\right),\ldots,\;y\left(x^{k+p}\right)$.

Each performing process having completed the computing of the function at some time moment sends  the computed value to the master process. In turn, the master process selects a new point  $x^{k+p+1}$ for the performing process according to the rules described below. Note that in this case  we will have a set of preimages of the trial points  $I_k=\left\{x^{k+1},x^{k+2},\ldots,x^{k+p}\right\}$ which the trials have been started already but haven’t been completed yet in.

So far, the parallel asynchronous global optimization algorithm using decision tree to find the local  extrema attractors consists of the following steps:

\begin{enumerate} \item Arrange n the increasing order (by the lower indices) the set of preimages of the trial points
	
	\begin{displaymath} 
		X_k=\left\{x^1,x^2,\ldots,x^{k+p}\right\}, 
	\end{displaymath} 
	
	
	containing all points, which the trials either have been completed or are being executed in i. e. get an  ordered set 
	
	\begin{displaymath} 
		0=x_1<x_2<\ldots<x_{k+p}=1. 
	\end{displaymath} 
	
	
	\item Compute the values 
	
	\begin{displaymath} 
		M_1=\max{\left\{\frac{\left|z_i-z_{i-1}\right|}{\left(x_i-x_{i-1}\right)^{1/N}}:x_{i-1}\notin  I_k,x_i\notin I_k,2\le i\le k+p\right\}}, 
	\end{displaymath} 
	
	
	\begin{displaymath} 
		M_2=\max{\left\{\frac{\left|z_{i+1}-z_{i-1}\right|}{\left(x_{i+1}-x_{i-1}\right)^{1/N}}:x_i\in  I_k,2\le i<k+p\right\}}, 
	\end{displaymath} 
	
	
	\begin{displaymath} 
		M=\max{\{}M_1,M_2\}, 
	\end{displaymath} 
	
	
	where $ z_i=\varphi\left(y\left(x_i\right)\right)$ if $x_i\notin I_k,\;1\le i\le k+p$. The values $z_i$ in  the points $x_i\in I_k$ are undefined since the trials in the points $x_i\in I_k$ haven’t been completed  yet. If the value of $M$ equals to $0$, set $M=1$.
	
	\item Juxtapose each interval $\left(x_{i-1},x_i\right),\;x_{i-1}\notin I_k,x_i\notin I_k,\;2\le i\le  k+p$ with its characteristic $R\left(i\right)$ computed according to the formula 
	
	\begin{displaymath} 
		R\left(i\right)=rM\Delta_i+\frac{\left(z_i-z_{i-1}\right)^2}{rM\Delta_i}-2\left(z_i+z_{i-1}\right), 
	\end{displaymath} 
	
	
	where $\Delta_i=\left(x_i-x_{i-1}\right)^{1/N}$ and $ r>1$ is the reliability parameter of the method.
	
	\item Select the interval $\left[x_{t-1},x_t\right]$, which the highest characteristics correspond to, i. e.
	
	\begin{displaymath} 
		R\left(t\right)=\max{\left\{R\left(i\right):\;x_{i-1}\notin I_k,x_i\notin I_k,\;2\le i\le k+p\right\}}. 
	\end{displaymath} 
	
	
	\item Define a new trial point $y^{k+p+1}=y\left(x^{k+p+1}\right)$, the preimage of which is  $x^{k+p+1}\in\left(x_{t-1},x_t\right)$ according to the formula 
	
\begin{displaymath}
	x^{k+p+1}=\frac{x_t+x_{t-1}}{2}-\mathrm{sign}\left(z_t-z_{t-1}\right)\frac{1}{2r}\left[\frac{\left|z_t-z_{t-1}\right|}{M}\right]^N.
\end{displaymath}
	
	
	\item Get the computed function value from the process $j$, add new trial $z_j = f(y(x_j))$ to the set  $V$. 
	
	%X_k=\left\{x^1,x^2,\ldots,x^{k+p}\right\}
	
	\item If $k\ <\ 100\ast\ N$, return to Step 1.
	
	%\item  If the decision tree is used not in the first time, go to Step 15.
	
	\item Create the decision tree using the set $I_k$, get the approximating function $\psi(y)$.
	
	\item If the decision tree is used for the first time, construct a uniform grid 
	
	\begin{displaymath} 
		Y'=\{ y'\in\ R^N:\ a_i\le\  {y'_i}^k \le\ b_i,\ 1\le\ i\le\ N,\ 1\le k\le\ \sqrt[N]{300}  \} 
	\end{displaymath} 
	
	
	\item Compute the approximation values: $Z' = \{ z'=  \psi(y'), y' \in Y'\}$
	
	\item For all points $y'\in V$:
	
	Find the points $y'_q$ closest to $y'$,
	
	Make a detour of the neighbors $y'_q$ according to the principle described above.
	
	If no one neighbor point has a lower value than in $y'_q$, run the local method.
	
	Clear the set $V$.
	
\end{enumerate}

Upon computing the next trial point, the master process adds it to the set $I_k$ and sends to the  performing process, which initiates the new trial in this point. The master process stops the algorithm, if one of the two conditions is satisfied: $\Delta_t<$ or  $k+p>K_{max}$. The real value $\epsilon>0$ and the integer value $K_{max}>0$ are the parameters of the algorithm  and correspond to the precision of the search for the solution and the maximum number of trials,  respectively.

\begin{figure}[ht!]
	
	\begin{center} 
				\begin{minipage}[h]{0.9\linewidth} 			\includegraphics[width=1\linewidth]{figure/fig3.png}
					 			\caption{Block scheme of combined GSA and Hooke-Jeeves method using the  decision tree} %% подпись к рисунку 			\label{fig:fig3}
				 \end{minipage}
	\end{center}
\end{figure}





% переводить до этого раздела
\section{Результаты экспериментов}\label{SecR}


Вычислительные эксперименты были проведены на Lobachevsky supercomputer. Один узел кластера состоит из двух процессоров Intel Sandy Bridge E5-2660 2.2 GHz, 64 Gb RAM. 

В экспериментах мы использовали генератор тестовых задач GKLS, который может порождать многоэкстремальные задачи оптимизации с известными свойствами: точкой глобального минимума, количеством локальных минимумов, и т.п.

Для демонстрации эффективности исследуемого алгоритма глобального поиска приведем результаты его сравнения с известным методами DIRECT и DIRECTl
В таблице \ref{tab:1} приведено среднее количество итераций, выполненных соответствующими методами при решении серии задач GKLS. Знак «>» указывает на ситуацию, когда не все задачи были решены, здесь в скобках указано количество нерешенных задач. Как видно, GSA превосходит методы DIRECT и DIRECTl по среднему числу итераций, требующихся для решения задач с одинаковой точностью.

\begin{table}[!ht]
    \caption{Среднее число итераций}
    \label{tab:1}
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        N & Problem class & DIRECT & DIRECTl & АГП  \\ \hline
        4 & Simple & >47282 (4) & 18983 & 11953  \\ \hline
        ~ & Hard & >95708 (7) & 68754 & 25263  \\ \hline
        5 & Simple & >16057 (1) & 16758 & 15920  \\ \hline
        ~ & Hard & >217215 (16) & >269064 (4) & >148342 (4)  \\ \hline
    \end{tabular}
\end{table}


Ниже приведены результаты сравнения двух параллельных алгоритмов --- асинхронного GSA и его модификации с использованием decision tree и локального метода Хука-Дживса. Численное сравнение проводилось на классах функций Simple и Hard размерности 2, 3, 4 и 5 из \cite{fio_bib19}. Критерием остановки служило попадание точки очередного испытания trial в $\varepsilon$ окрестность истинного глобального минимума. 

В таблице \ref{tab:2} представлено среднее количество итераций и ускорение по времени относительно последовательного запуска. Распараллеливание  осуществлялось с применением  MPI. При этом запуск производился на 4 процессах, а следовательно, на каждой итерации вычислялось 3 точки. Значение в скобках (если оно есть) показывает количество не решенных задач. Это означает, что было достигнут максимум по числу проводимых итераций, но в окрестность глобального минимума точка так и не попала.


В таблице \ref{tab:2} приведены результаты сравнения двух алгоритмов –-- асинхронного алгоритма глобального поиска (АГП) и АГП с использованием деревьев решений для выявления областей притяжения локальных минимумов (Деревья решений). Численное сравнение проводилось на классах функций Simple и Hard размерности 2, 3, 4 и 5 из \cite{fio_bib19}. Алгоритм прекращал свою работу как только поражалась точка испытания в эпсилон окрестность истинного глобального минимума. В таблице сравнивается среднее количество итераций, и ускорение по времени относительно последовательного запуска. Распараллеливание  осуществлялось с применением асинхронного MPI алгоритма. При этом, запуск производился на 8 процессах, а следовательно, на каждой итерации вычислялось 7 точек испытаний.


\begin{table}[h!]
	\caption{Среднее число итераций и среднее число trial, проводимое разными алгоритмами}
	\label{tab:2}
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		
		N & Класс задачи & \multicolumn{2}{c|}{Среднее число итераций} & \multicolumn{2}{c|}{Ускорение} \\ \hline
		& ~ & АГПА & decision tree & АГПА & decision tree \\ \hline
		& Simple & 3823,1   & 485,6  & 7,4   & 0,5 \\ \hline
		2  & Hard & 787,6    & 304,2  & 13,8  & 1,0 \\ \hline
		& Simple & 5700,5   & 720,1  & 5,6   & 0,1 \\ \hline
		3  & Hard & 2411,9   & 634,3  & 5,0   & 0,1 \\ \hline
		& Simple & 31101,5  & 1037,4 & 5,1   & 0,2 \\ \hline
		4  & Hard & 10418,1  & 816,2  & 6,9   & 0,2 \\ \hline
		& Simple & 163313,5 & 1204,2 & 4,8   & 1,0 \\ \hline
		5  & Hard & 27524,5  & 999,4  & 4,6   & 0,2 \\ \hline
	\end{tabular}
\end{table}

Как видно из таблицы ускорение по числу итераций достаточно велико, причем на задачах любой размерности. 

\section{Заключение}\label{SecC}

Методы глобальной оптимизации являются областью активных научных исследований.  В результате работы удалось успешно объединить алгоритм глобального поиска вместе с поиском локального минимума методом Хука-Дживса. С целью экспериментального подтверждения теоретических свойств рассматриваемого алгоритма были так же проведены вычислительные эксперименты на серии из сотни тестовых задач. 

При использовании в качестве определяющего правила о необходимости запуска локального метода decision tree, нам удалось получить достаточно хорошее ускорение. Применяя параллельную версию Алгоритма Глобального поиска ускорение удалось сохранить, что является несомненным преимуществом. Таким образом, наша схема позволяет получить преимущество от обоих из подходов (и от распараллеливания, и от запуска локального метода).





%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\begin{thebibliography}{6}
%

\bibitem{fio_bib1}
Ferreiro A.M., Garcia J.A., Lopez-Salas J.G., Vazquez C. An efficient implementation of parallel simulated annealing algorithm in GPUs // Journal of global optimization. — 2013. — Vol. 57, No. 3. — P. 863–890.

\bibitem{fio_bib2}
Garcia-Martinez J.M., Garzon E.M., Ortigosa P.M. A GPU implementation of a hybrid evolutionary algorithm: GPuEGO // Journal of super-computing. — 2014. — Vol. 70, No. 2. — P. 684–695.

\bibitem{fio_bib3}
Langdon W.B. Graphics processing units and genetic programming: an overview // Soft Computing. — 2011. — Vol. 15, No 8. — P. 1657–1669.

\bibitem{fio_bib4}
Евтушенко Ю.Г., Малкова В.У., Станевичюс А.А. Параллельный поиск глобального экстремума функций многих переменных // Ж. вычисл. матем. и матем. физ. — 2009. — Т. 49, №2. — С. 255–269.

\bibitem{fio_bib5}
He J., Verstak A., Watson L.T., Sosonkina M. Design and implementation of a mas-sively parallel version of DIRECT // Computational optimization and applications. — 2008. — Vol. 40, No. 2. — P. 217–245

\bibitem{fio_bib6}
Paulavicius R., Žilinskas J., Grothey A. Parallel branch and bound for global optimiza-tion with combination of Lipschitz bounds // Optimization methods and software. — 2011. — Vol. 26, No. 3. — P. 487–498.

\bibitem{fio_bib7}
Глобальная оптимизация: приложения и вычислительная сложность. %URL: http://hpc-education.unn.ru/ru/globopt/глобальная-оптимизация-приложения-и (дата обращения: 10.10.2020).

\bibitem{fio_bib8}
Захарова Е.М., Минашина И.К. Обзор методов многомерной оптимизации. Информационные процессы // том 14 - №3 – 2014 - С. 256-274.

\bibitem{fio_bib9}
Городецкий С. Ю. Методические материалы к лабораторной работе «Вычислительные методы поиска локальных минимумов функций», 2001.% URL: http://itmm.unn.ru/files/2016/09/MO_Lab2_LocOpt.pdf (дата обращения: 09.07.2022).

\bibitem{fio_bib10}
Шефов К. С., Степанова М. М. Реализация и применение параллельного алгоритма глобального поиска минимума к задаче оптимизации параметров молекулярно-динамического потенциала ReaxFF: %URL: http://crm.ics.org.ru/uploads/crmissues/crm_2015_3/15750.pdf, 2015 (дата обращения: 13.10.2020).

\bibitem{fio_bib11}
Gergel V.P. A global optimization algorithm for multivariate functions with lipschitzian first derivatives / Gergel V.P. // Journal of Global Optimization. – 1997. – Vol. 10, No. 3. – P. 257-281.

\bibitem{fio_bib12}
Barkalov K.A. A global optimization technique with an adaptive order of checking for constraints / Barkalov K.A., Strongin R.G. // Computational Mathematics and Mathematical Physics. – 2002. – Vol. 42, No. 9. – P. 1289-1300.

\bibitem{fio_bib13}
Gaviano, M. Software for generation of classes of test functions with known local and global minima for global optimization/ M. Gaviano, D. Lera, D. E. Kvasov, Y. D. Sergeyev // ACM Transactions on Mathematical Software. – 2003. – Vol. 29. – P. 469-480.

\bibitem{fio_bib14}
Химмельблау Д. Прикладное нелинейное программирование // М.: Мир - 1975 - 536 с. 

\bibitem{fio_bib15}
Nelder J., Mead R. A simplex method for function minimization // Computer Journal - 7(4) – 1965 - P. 308-313.

\bibitem{fio_bib16}
OpenCV (Open Source Computer Vision) documentation. %URL: https://docs.opencv.org/4.x/dc/dd6/ml_intro.html (дата обращения: 10.07.2022)

\bibitem{fio_bib17}
Сергеев, Я.Д. Диагональные методы глобальной оптимизации / Я.Д. Сергеев, Д.Е. Квасов – М.: Физматлит, 2008. – 352 c.

\bibitem{fio_bib18}
Sysoyev, A.,  Barkalov, K.,  Sovrasov, V.,  Lebedev, I.,  Gergel, V. Globalizer – A parallel software system for solving global optimization problems. Lecture Notes in Computer Science. Volume 10421 LNCS, 2017, Pages 492-499

\bibitem{fio_bib19}
Sergeyev, Ya.D. Global search based on efficient diagonal partitions and a set of Lipschitz constants / Ya.D. Sergeyev, D.E. Kvasov // SIAM Journal on Optimization. – 2006. Vol. 16, No. 3. – P. 910–937

\bibitem{fio_bib20}
Стронгин Р.Г. Параллельные вычисления в задачах глобальной оптимизации / Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А. – М.: Издательство Московского университета, 2013. 280 с.









\end{thebibliography}
\end{document}
