%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
\usepackage{marvosym}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\usepackage{graphicx}

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\usepackage[english, russian]{babel} 
\usepackage{amsmath} 
\def\UrlFont{\rmfamily}
\providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
%
\title{Combining Global and Local Optimization Algorithms Using Decision Trees}
%
\titlerunning{Global Optimization Using Decision Trees}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{K.~A.~Barkalov\inst{1}\letter\orcidID{0000-0001-5273-2471} \and I.~G.~Lebedev\inst{1}\letter\orcidID{0000-0002-8736-0652} \and D.~I.~Silenko\inst{1}\letter\orcidID{0000-0002-2578-9699}}
%
\authorrunning{K.~A.~Barkalov et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{K.~A.~Barkalov and I.~G.~Lebedev, and D.~I.~Silenko}
%
\institute{Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia \\
\email{konstantin.barkalov@itmm.unn.ru, ilya.lebedev@itmm.unn.ru, rotor12587@mail.ru}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
В работе рассматривается решение задач многомерной глобальной оптимизации с применением decision tree для выявления областей притяжения локальных минимумов. Целевая функция задачи задана ''как черный ящик''. 
Мы лишь предполагаем, что она удовлетворяет условию Липшица с неизвестной константой. 
Для поиска глобального минимума в задачах такого типа используется алгоритм глобального поиска.
В настоящей работе мы предлагаем способ выделения окрестностей локальных экстремумов целевой функции на основе анализа накопленной поисковой информации.
Проведение такого анализа средствами машинного обучения позволяет принять решение о запуске локального метода, который может ускорить сходимость алгоритма. Выдвинутое предположение подтверждается результатами вычислительных экспериментов, демонстрирующих ускорение при решении серии тестовых задач. 
% We would like to encourage you to list your keywords within
% the abstract section using the \keywords{...} command.
\keywords{Global optimization $\cdot$ Multiextremal functions $\cdot$ Parallel computing $\cdot$  Decision tree}
\end{abstract}
%








\section{Введение}

Для решения указанной задачи известен широкий спектр алгоритмов, он основаных на идее случайного поиска \cite{fio_bib1, fio_bib2, fio_bib3} метаэвристических алгоритмов, до детерминированных алгоритмов, гарантирующих сходимость к глобальному минимуму \cite{fio_bib4, fio_bib5, fio_bib6}. 
Поскольку в реальных задачах глобальной оптимизации каждое вычисление значения функции (называемое в дальнейшем trial) представляет собой весьма трудоемкую операцию, необходимо уменьшить количество таких trials. Этого можно добиться целенаправленным выбором вариантов в процессе поиска оптимального решения, отсекая бесперспективные подобласти поиска и исследуя только те, в которых может содержаться решение задачи. На этой идее основывается global search algorithm (GSA) \cite{fio_bib7}. В данной работе мы попробуем объединить АГП и метод локальной оптимизации (метод Хука-Дживса) с целью уменьшить количество проводимых испытаний. При этом принятие решения о запуске локального метода будет осуществляться с помощью decision tree.

Алгоритмы локальной оптмизации предназначены для определения лишь одного из локальных экстремумов на множестве допустимых решений, в котором целевая функция принимает экстремальное (максимальное или минимальное) значение. 
%При их построении могут использоваться как детерминированный спуск в область экстремума, так и случайный поиск. 
Среди локальных методов традиционно выделяют методы нулевого порядка и градиентные методы  (1-го и 2-го порядков) \cite{fio_bib8, fio_bib9}.
Отметим, что применение методов первого порядка (не говоря уже о втором) в задачах с функциями вида ''черный ящик'' затруднено, т.к. здесь возникает проблема численной оценки градиента. Поэтому мы будем рассматривать в дальнейшем только методы нулевого порядка.

Для того, чтобы понять, в какой момент лучше всего запускать локальный метод в рамках глобального поиска, мы будем использовать алгоритм на основе decision tree.
На первой стадии поиска происходит накоплении поисковой информации в виде результатов проведенных испытаний.
Результаты испытаний мы используем для тренировки decision tree, что позволяет получить кусочно-линейную аппроксимацию целевой функции и  предсказывать на ее основе поведение целевой функции. Для этого бы будем сравнивать значения функции в точках, соседних к той, которую считаем подозрительной на локальный минимум. И на основе кусочной-линейной аппроксимации принимать решение, находится очередная точка испытания в области притяжения локального минимума или нет.

%Структура статьи

\section{Постановка задачи}

Рассмотрим задачу поиска глобального минимума функции $\varphi(y)$ в гиперинтервале $D=\{ y\in\ R^N:\ a_i\le\ y_i\le\ b_i,\ 1\le\ i\le\ n \}$ При этом будем предполагать, что функция удовлетворяет условию Липшица с априори неизвестной константой $L$, что соответствует ограниченности изменения значений функции при ограниченной вариации аргумента. Это предположение можно интерпретировать (применительно к прикладным задачам) как отражение ограниченности мощностей, порождающих изменения в моделируемой системе. 




\begin{equation} \label{sec:problem}   
\varphi(y^*) = min\{\varphi(y):y\in D\}, D = \{y \in R^N : a_i \leq y_i \leq b_i, 1 \leq i \leq N \},
\end{equation}
где $a,b \in R$ есть заданные векторы.


\begin{displaymath}
|\varphi(y_1)-\varphi(y_2)|\leq L\parallel y_1-y_2 \parallel
,y_1,y_2 \in D, 0<L< \infty.
\end{displaymath}




Численное решение задачи (\ref{sec:problem})   сводится к построению оценки $ y_k^\ast\in\ D$ , отвечающей некоторому понятию близости к точке $y^\ast$ (например, ${||y^\ast-y}_k^\ast||\le\ \varepsilon$, где $\varepsilon\geq0$ есть заданная точность) на основе конечного числа $k$ вычислений значений целевой функции. Относительно класса рассматриваемых задач предполагается, что оптимизируемая функция $\varphi(y)$ может быть задана алгоритмически, как результат работы некоторой подпрограммы или библиотеки.

Задачи многоэкстремальной оптимизации имеют существенно более высокую трудоемкость решения по сравнению с другими типами оптимизационных задач, т.к. глобальный оптимум является интегральной характеристикой решаемой задачи и требует исследования всей области поиска. Как результат, поиск глобального оптимума сводится к построению некоторого покрытия (сетки) в области параметров, и выборе наилучшего значения функции на данной сетке. Вычислительные затраты на решение задачи растут экспоненциально с ростом размерности.

\section{Многомерный параллельный алгоритм глобального поиска}

Основная идея данного подхода заключается в том, что целевая функция рассматривается как реализация некоторого случайного процесса. Решающие правила алгоритма конструируются таким образом, что очередная итерация проводится в точке глобального минимума математического ожидания значений функции. Эта точка записывается в список известных значений и итерации повторяются. Так происходит до тех пор, пока не достигнут один из выбранных критериев остановки: расстояние между точками проведенных испытаний становится меньше заданного значения или же достигается установленное максимальное число итераций. \cite{fio_bib10}

При решении многомерных задач применяется редукция размерности (т.е. сведение многомерной задачи к эквивалентной одномерной) с использованием кривых Пеано. Они позволяют свести многомерную задачу минимизации в области $D$  к одномерной задаче минимизации на отрезке $[0, 1]$

\begin{displaymath}
\varphi(y(x^\ast))\ =\ min\{\varphi(y(x)):\ x\epsilon[0,\ 1]\}
\end{displaymath}

где функция $\varphi(y(x^\ast))$  удовлетворяют уже равномерному условию Гельдера

\begin{displaymath}
\left|\varphi (y \left(x_1\right))- \varphi (y \left(x_2\right)\right )|\le\ H\left|x_1-x_2\right|^\frac{1}{N},\ x_1,\ x_2\epsilon[0,1]		
\end{displaymath}

Поэтому, не ограничивая общности, можно рассматривать минимизацию одномерной функции $f(x)\ =\ \varphi(y(x)), \ x\epsilon [0,1]$, удовлетворяющей условию Гельдера.

Схема алгоритма:

На подготовительном шаге параллельно проводятся $p$ trials в произвольных внутренних точках $x^1, ...,x^p$ отрезка $[0,1]$, что соответствует первой итерации  алгоритма. 

Если выполнено $n\geq1$ итераций, которым соответствуют $k=k(n)$ проведенных trial in point $x^i, 1\leq i\leq k$, то trial $x^{k+1},\ldots,x^{k+p}$  следующей $(n+1)$-й итерации будут определять следующим образом.

\begin{enumerate}

\item  Перенумеровать (нижним индексом) trial ранее проведенных  $x^i, 1\leq i\leq k$, а также граничные точки отрезка [0,1] в порядке возрастания координаты:
 \begin{equation}
\label{agp1_sort}
	0=x_0<\ x_1<\ ...\ <x_{k+1}=1.
	\end{equation}
	и сопоставить им значения $z_i=f(x_i)$. 
	
\item  Вычислить текущие нижние оценки $M$ неизвестной константы Гельдера $H$:
 \begin{equation}
\label{agp2_mu}
	\mu=max\left\{\frac{|z_i-z_{i-1}|}{{{(x}_i-x_{i-1})}^{1/N}},\ i=1,\ldots,k\right\},\ M=\ \left\{\begin{matrix}r\mu,\ \mu>0,\\1,\ \mu=0,\\\end{matrix}\right.\
	\end{equation}
где $r>1$ --- заданный параметр алгоритма.
   
\item  Для каждого интервала $(x_{i-1},x_i), 1\leq i\leq k+1,$ вычислить величину $R(i)$, называемую \textit{характеристикой} интервала, в соответствии с формулами

\begin{equation}
\label{agp3_R1}
R(1)=2\Delta_1-4\dfrac{z_1}{M}, \; R(k+1)=2\Delta_{k+1}-4\dfrac{z_k}{M},
\end{equation}

\begin{equation}
\label{agp3_Ri}
R(i)=\Delta_i+\dfrac{(z_i-z_{i-1})^2}{M^2\Delta_i}-2\dfrac{z_i+z_{i-1}}{M},1<i<k+1,
\end{equation}
где \(\Delta_i=(x_i-x_{i-1})^\frac{1}{N}\).
   
\item   Упорядочить характеристики $R\left(i\right),\ 1\leq i \leq k+1,$ в порядке невозрастания 
\begin{equation}
\label{agp4_R_sort}
	R\left(t_1\right)\geq\ R\left(t_2\right)\geq...\geq\ R\left(t_k\right)\geq\ R(t_{k+1}),\ 
\end{equation}	
и выбрать $p$ интервалов с номерами $t_j,\ 1\le\ j\le\ p$, с наибольшими значениями характеристики.

\item В выбранных интервалах вычислить точки $x^{k+j},\ 1\leq j\leq p,$ в соответствии с формулами
\begin{equation}
\label{agp5_x1}
	x^{k+j}=\frac{x_{t_j}+x_{t_j-1}}{2},\ t_j=1,\ t_j=k+1,
\end{equation}	
	\begin{equation}
\label{agp4_xi}	
	x^{k+1}=\frac{x_{t_j}+x_{t_j-1}}{2}-sign\left(z_{t_j}-z_{t_j-1}\right)\frac{1}{2r}\left[\frac{\left|z_{t_j}-z_{t_j-1}\right|}{\mu}\right]^N,\ 1<t_j<k+1.
\end{equation}	

\end{enumerate}

Очередные trial проводятся параллельно, в вычисленных по формулам (\ref{agp5_x1}), (\ref{agp4_xi}) точках $x^{k+j},\ 1\leq j\leq p$.

Отметим, что в прикладных оптимизационных задачах процесс проведения trial является, как правило, значительно более трудоемким по сравнению с работой вычислительных правил алгоритма.

Алгоритм останавливает свою работу в случае, если хотя бы для одного значения $t_j,\ 1\le\ j\le\ p$ из (\ref{agp4_R_sort}) выполняется условие \(\Delta_{t_j} < \varepsilon\). Данный критерий остановки (наряду с обычным для итерационных методов критерием, ограничивающим число выполненных итераций) используется в задачах оптимизации, в которых точка глобального минимума $x^*$ заранее неизвестна. 
	 
При решении тестовых задач, в которых точка глобального минимума $x^*$ является  известной, можно использовать также и критерий остановки по попаданию в окрестность глобального минимума. В этом случае метод прекращает работу, если хотя бы для одного значения $t_j,\ 1\le\ j\le\ p$ из (\ref{agp4_R_sort}) выполняется условие $\left|x_{t_j}-\ x^\ast\right| < \varepsilon.$
	
В качестве итоговой оценки глобально-оптимального решения рассматриваемой задачи выбираются значения 
\begin{equation}
f_k^*=\min_{1\leq i \leq k}f(x_i), \; x_k^*=arg \min_{1\leq i \leq k}f(x_i).
\end{equation}



Обоснование данного способа организации вычислений см. в \cite{fio_bib20}. Модификации, учитывающие наличие ограничений-неравенств в задаче, а также информация о производной целевой функции, представлены в \cite{fio_bib12, fio_bib9, fio_bib11}.


\section{Метод Хука-Дживса}

Метод Хука–Дживса – это комбинация исследующего поиска по направлениям и поиска по образцу \cite{fio_bib14, fio_bib15}.



Исследующий поиск определяется следующим образом: 
\begin{enumerate}
\item	Определяется значение шага (оно различно для каждого направления координат и может измениться во время поиска). 
\item	Шаг поиска считается успешным, если значение целевой функции в контрольной точке не превышает значение исходной целевой функции. 
\item	В противном случае нужно вернуться к предыдущему пункту и сделать шаг в обратном направлении. 
\item	После перебора всех N координат исследовательский поиск заканчивается. Полученная точка называется базовой точкой.
\end{enumerate}




Что касается поиска по образцу, то он заключается в реализации шага из найденной в ходе исследующего поиска базовой точки вдоль прямой, соединяющей ее с предыдущей базовой точкой. Величина этого шага определяется задаваемым заранее параметром.

\begin{figure}[!h]
	\begin{center}
		\begin{minipage}[h]{0.8\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig1.png}
			\caption{Пример итераций алгоритма Хука-Дживса} %% подпись к рисунку
			\label{fig:fig1}
		\end{minipage}
	\end{center}
\end{figure}	

\section{Decision tree}

Decision tree — это метод автоматического анализа больших массивов данных, который применяется в машинном обучении. Decision tree представляет собой бинарное дерево (дерево, в котором каждый не листовой узел имеет два дочерних узла). Его можно использовать как для решения задач классификации, так и для регрессионных задач. При решении задач классификации, каждый лист дерева помечается меткой класса, при этом несколько листьев могут иметь одну и ту же метку. В случае построение регрессии, каждому листу дерева присваивается константа, как следствие  получаемая функция аппроксимации является кусочно-постоянной.

Процедура прогнозирования по decision tree начинается с корня. От каждого не листового узла процедура идет влево или вправо на основании значения некоторой переменной, индекс которой хранится в наблюдаемом узле. Возможны следующие переменные:
Упорядоченные переменные, значения которых сравнивается с порогом (который также хранится в узле). Если значение меньше порога, процедура идет влево. В противном случае - вправо.
Категориальные переменные - значение дискретной переменной проверяется на предмет того, принадлежит ли оно определенному подмножеству значений (также хранящемуся в узле) из набора допустимых величин. Если принадлежит, то идем влево. В противном случае - вправо.

Как только алгоритм достигает конечного узла, в качестве ответа процедуры прогнозирования выбирается значение, присвоенное этому узлу.

При реализации алгоритма  решения задач многомерной глобальной оптимизации с применением decision tree для выявления областей притяжения локальных минимумов нами были использованы  decision tree из библиотеки OpenCV (это библиотека алгоритмов компьютерного зрения, обработки изображений и численных алгоритмов общего назначения с открытым кодом). Подробнее с decision tree можно ознакомиться в \cite{fio_bib16}


\section{Объединение локального метода оптимизации и АГП для решения многомерных задач}

В текущем разделе приведем подробное описание того, как использовать decision tree в многомерном случае. В процессе работы АГП необходимо определить, стоит ли использовать текущую точку, в качестве стартовой для локального метода или нет. Для этого можно осмотреть соседние trial и если среди них нет ни одной такой, значение функции в которой меньше, чем в текущей, то можно предположить, что мы находимся в области притяжения локального минимума и можно запустить локальный метод, чтобы быстрее сойтись к минимуму. Использование проекций многомерных точек на одномерный отрезок после редукции размерности (то, что позволяют сделать кривые Пеано) нам для этих целей не подойдут по нескольким причинам. Во-первых, функция после отображения сильно изменяется, один локальный минимум в многомерном пространстве может разделиться на множество минимумов после отображения. Во-вторых, после отображения мы можем потерять информацию о взаимном расположении точек исходной функции. Точки, расположенные близко в многомерном пространстве, могут оказаться на разных концах одномерного отрезка и наоборот).



\begin{figure}[ht!]
	\begin{center}
		\begin{minipage}[h]{0.8\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig2.png}
			\caption{Функция из генератора gkls до использования кривых Пеано} %% подпись к рисунку
			\label{fig:fig2}
		\end{minipage}
	\end{center}
\end{figure}	

\begin{figure}[ht!]
	\begin{center}
		\begin{minipage}[h]{0.8\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig2.png}
			\caption{Рисунок с двумерным decision tree} %% подпись к рисунку
			\label{fig:fig2_2}
		\end{minipage}
	\end{center}
\end{figure}	




Поэтому мы будем работать именно с исходными точками. Однако определить соседние точки в многомерном пространстве достаточно трудоемко, и для этого мы будем использовать decision tree. 

После проведения определенного числа trial (например, $100\ \ast\ N$), используем все накопившиеся точки для инициализации подходящей структуры данных и тренировки на ее основе decision tree. Чтобы проще было определить соседние точки, мы построим равномерную сетку с определенным шагом по каждой из размерностей, после чего передадим эти точки на вход предсказанию по дереву. Теперь, имея равномерную сетку точек и зная значение функции в каждой из них, найдем ближайшую, с точки зрения евклидова расстояния, к исходной. Эта ближайшая точка является проекцией изначальной точки на равномерную сетку и что бы определить  ее соседей, достаточно  просто перебрать индексы соседей. К тому же поскольку decision tree строит аппроксимацию исходной задачи (пусть и кусочно постоянную) мы можем оценить значение функции в тех областях, в которых trial ранее не проводились.

При рассмотрении соседей необходимо учитывать следующее: если хоть один из соседей имеет значение функции меньшее, чем в текущей точке, то запуск локального метода не нужен; если какой-то из соседей имеет значение функции такое же, как в текущей, то его также необходимо проверить (найти всех его соседей и проверять их аналогично); и только в том случае, если все значения функций соседей больше (включая соседей равных точек) мы можем запускать локальный метод.

На рисунке  (\ref{fig:fig2}) отображены линии уровней оптимизационной задачи с отображением текущих  trial, и на рисунке  (\ref{fig:fig2_2}) отображено соответствующее decision tree. 

\section{Асинхронная схема распараллеливания с использованием MPI }

Идея асинхронной схемы в том, чтобы не дожидаться окончания вычисления всей отправленной группы точек, а отправлять новую точку, как только была закончена работа с первой из старых. 

Предположим, что главный процесс вычисляет одну точку следующего trial на каждой итерации и отправляет ее рабочему процессу для выполнения trial. В то же время выполнение trial рабочим процессом – гораздо более затратная в вычислительном отношении операция, чем выбор мастером новой trial, исключающей простаивающие рабочие процессы. В этом случае (в отличие от синхронных параллельных алгоритмов) общее количество trial, выполняемых каждым рабочим процессом, будет зависеть от вычислительных затрат на выполнение конкретного trial и не может быть оценено заранее. При описании параллельного алгоритма предположим, что в нашем распоряжении имеется $p+1$ вычислительных процессов: один главный и $p$ рабочих.

 В начале поиска главный процесс (допустим, это процесс $0$) инициирует параллельное выполнение $p$ trial в $p$ разных точках области поиска.

Две из этих точек являются граничными, а остальные – внутренними, т. е. в точках $\{y\left(x^1\right),y\left(x^2\right),\ldots,y\left(x^p\right)\}$, где
$x^1=,x^p=,x^i\in\left(0,1\right),i=2,\ldots,p$

Теперь предположим, что выполнено $k\geq0$ trial и рабочие процессы выполняют trial в точках $y\left(x^{k+1}\right),\;y\left(x^{k+2}\right),\ldots,\;y\left(x^{k+p}\right)$.

Каждый рабочий процесс, завершив в какой-то момент вычисление функции, отправляет главному процессу значение. В свою очередь, главный процесс выбирает новую пробную точку $x^{k+p+1}$ для рабочего процесса в соответствии с описанными ниже правилами. Заметим, что в этом случае мы будем иметь набор прообразов пробных точек 
$I_k=\left\{x^{k+1},x^{k+2},\ldots,x^{k+p}\right\}$
на котором trial уже начаты, но еще не завершены.

Таким образом параллельный асинхронный алгоритм глобальной оптимизации с использованием decision tree для выявления локальных областей состоит из следующих шагов:

\begin{enumerate}
\item  Перенумеровать множество прообразов точек trial

\begin{displaymath}
X_k=\left\{x^1,x^2,\ldots,x^{k+p}\right\},
\end{displaymath}

содержащие все прообразы, на которых либо завершились, либо идут trial в порядке возрастания (по нижнему индексу) так, что

\begin{displaymath}
0=x_1<x_2<\ldots<x_{k+p}=1.
\end{displaymath}

\item  Вычислите значения

\begin{displaymath}
M_1=\max{\left\{\frac{\left|z_i-z_{i-1}\right|}{\left(x_i-x_{i-1}\right)^{1/N}}:x_{i-1}\notin I_k,x_i\notin I_k,2\le i\le k+p\right\}},
\end{displaymath}

\begin{displaymath}
M_2=\max{\left\{\frac{\left|z_{i+1}-z_{i-1}\right|}{\left(x_{i+1}-x_{i-1}\right)^{1/N}}:x_i\in I_k,2\le i<k+p\right\}},
\end{displaymath}

\begin{displaymath}
M=\max{\{}M_1,M_2\},
\end{displaymath}

где $ z_i=\varphi\left(y\left(x_i\right)\right)$, если $x_i\notin I_k,\;1\le i\le k+p$. Значения $z_i$ в точках $x_i\in I_k$ не определены, так как trial в точках $x_i\in I_k$ еще не завершены. Если значение $M$ равно $0$, то установите $M=1$.

\item  Сопоставьте каждый интервал $\left(x_{i-1},x_i\right),\;x_{i-1}\notin I_k,x_i\notin I_k,\;2\le i\le k+p$ величине $R\left(i\right)$, называемой характеристикой интервала и вычисляемой по формуле

\begin{displaymath}
R\left(i\right)=rM\Delta_i+\frac{\left(z_i-z_{i-1}\right)^2}{rM\Delta_i}-2\left(z_i+z_{i-1}\right),
\end{displaymath}

где $\Delta_i=\left(x_i-x_{i-1}\right)^{1/N}$ и $ r>1$ — параметр надежности метода.

\item  Выбрать интервал $\left[x_{t-1},x_t\right]$, которому соответствует максимальная характеристика, т.е.

\begin{displaymath}
R\left(t\right)=\max{\left\{R\left(i\right):\;x_{i-1}\notin I_k,x_i\notin I_k,\;2\le i\le k+p\right\}}.
\end{displaymath}

\item  Определить новую пробную точку $y^{k+p+1}=y\left(x^{k+p+1}\right)$, прообразом которой является $x^{k+p+1}\in\left(x_{t-1},x_t\right)$ по формуле

\begin{displaymath}
x^{k+p+1}=\frac{x_t+x_{t-1}}{2}-\mathrm{sign}\left(z_t-z_{t-1}\right)\frac{1}{2r}\left[\frac{\left|z_t-z_{t-1}\right|}{M}\right]^N.
\end{displaymath}

\item 	Если $ k\ <\ 100\ast\ N$, то возвращаемся к шагу 1.
\item 	Если используем decision tree не впервые, то переходим к пункту 15.
\item 	Инициализируем decision tree (заполняем массив многомерных точек, массив значений функции, а так же создаем decision tree).
\item 	Тренируем дерево с помощью массивов полученных в предыдущем пункте.
\item 	Строим равномерную сетку, откладывая по каждой координате $\sqrt[N]{300}$ точек
\item 	Предсказываем значения функции в узлах сетки с помощью decision tree.
\item 	Находим ближайшую с точки зрения Евклидова расстояния точку к текущей.
\item 	Обходим соседей по описанному ранее принципу.
\item 	Если не нашлось значений функции меньших, чем в текущей точке, то запускаем локальный метод.
\item 	Накапливаем массив из $100\ \ast\ N$ точек.
\item 	Все 100*N точек обрабатывает аналогично пункту 8-14, но без пункта 9 (без заполнения равномерной сетки, т.к. она у нас уже подготовлена).

\end{enumerate}

При вычислении очередной trial, главный процесс добавляет ее к множеству $I_k$ и отправляет рабочему процессу, который инициирует новую пробную точку в этой точке.
Главный процесс завершает алгоритм, если выполняется одно из двух условий: $\Delta_t<$ или $k+p>K_{max}$.
Вещественное число $\epsilon>0$ и целое число $K_{max}>0$ являются параметрами алгоритма и соответствуют точности поиска решения и максимальному количеству попыток соответственно.


Все полученные результаты были добавлены в систему globalizer \cite{fio_bib18}.
Была реализована объединенная схема с запуском локальных методов при обнаружении областей притяжения локальных минимумов, для чего была внедрена система  использующая decision tree.



\begin{figure}[ht!]

	\begin{center}
		\begin{minipage}[h]{0.9\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig3.png}
			\caption{Блок-схема объединения АГП и метода Хука-Дживса с использованием decision tree} %% подпись к рисунку
			\label{fig:fig3}
		\end{minipage}
	\end{center}
\end{figure}



\section{Эксперименты}

The numerical experiments were carried out on Lobachevsky supercomputer. Each supercomputer node was a two-processor one (two Intel Sandy Bridge E5-2660 2.2 GHz, 64 Gb RAM). All methods considered in the above sections were implemented using Globalizer software \cite{globalizerSystem} (development language – C++). Intel C++ 17.0.0 compiler was used to assembly the system.

В работе \cite{fio_bib13, fio_bib17} описан GKLS-генератор, позволяющий порождать задачи многоэкстремальной оптимизации с заранее известными свойствами: количеством локальных минимумов, размерами их областей притяжения, точкой глобального минимума, значением функции в ней и т.п.

В таблице \ref{tab:1} отражено среднее число итераций , которые выполнил метод при решении серии задач из данных классов. Символ «>» отражает ситуацию, когда не все задачи класса были решены каким-либо методом. Количество нерешенных задач указано в скобках.


\begin{table}[!ht]
    \caption{Среднее число итераций}
    \label{tab:1}
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        N & Problem class & DIRECT & DIRECTl & АГП  \\ \hline
        4 & Simple & >47282 (4) & 18983 & 11953  \\ \hline
        ~ & Hard & >95708 (7) & 68754 & 25263  \\ \hline
        5 & Simple & >16057 (1) & 16758 & 15920  \\ \hline
        ~ & Hard & >217215 (16) & >269064 (4) & >148342 (4)  \\ \hline
    \end{tabular}
\end{table}

Как видно из таблицы 1, АГП превосходит методы DIRECT и DIRECTl на всех классах задач по среднему числу итераций. 


Ниже приведены результаты сравнения двух алгоритмов – асинхронного АГП и его объединение с локальным методом (методом Хука-Дживса) при использовании decision tree. Численное сравнение проводилось на классах функций Simple и Hard размерности 2, 3, 4 и 5 из \cite{fio_bib19}. Критерием остановки служило попадание trial в эпсилон окрестность истинного глобального минимума. 

В таблице \ref{tab:2} представлено среднее количество итераций, и ускорение по времени относительно последовательного запуска. Распараллеливание  осуществлялось с применением асинхронного MPI алгоритма. При этом, запуск производился на 4 процессах, а следовательно, на каждой итерации вычислялось 3 точки. Значение в скобках (если оно есть) показывает количество не решенных задач. Это означает, что было достигнут максимум по числу проводимых итераций, но в окрестность глобального минимума точка так и не попала.



\begin{table}[h!]
    \caption{Среднее число итераций и среднее число trial, проводимое разными алгоритмами}
    \label{tab:2}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
	
        N & Класс задачи & \multicolumn{2}{c|}{Среднее число итераций} & \multicolumn{2}{c|}{Ускорение} \\ \hline
          & ~ & АГПА & decision tree & АГПА & decision tree \\ \hline
          & Simple & 2350.0 & 247.4 & 2348.0 & 390.0  \\ \hline
        2  & Hard & 4732.3 & 353.6 & 4730.3 & 1101.1  \\ \hline
          & Simple & 2115.5 & 579.5 & 2113.5 & 2441.4  \\ \hline
        3  & Hard & 5347.4 & 588.1 & 5345.4 & 2532.7  \\ \hline
          & Simple & 12168.9 & 768.2 & 12166.9 & 3582.2  \\ \hline
        4  & Hard & 25636.5 & 960.0 & 52634.5 & 5171.6  \\ \hline
          & Simple & 20633.5 & 978.0 & 20631.5 & 4840.4  \\ \hline
        5  & Hard & 161094.9 & 1217.0 & 161092.9 (4) & 7115.5  \\ \hline
    \end{tabular}
\end{table}

Как видно из таблицы ускорение по числу итераций достаточно велико, причем на задачах любой размерности. 

\section{Заключение}

Методы глобальной оптимизации являются областью активных научных исследований.  В результате работы удалось успешно объединить алгоритм глобального поиска вместе с поиском локального минимума методом Хука-Дживса. С целью экспериментального подтверждения теоретических свойств рассматриваемого алгоритма были так же проведены вычислительные эксперименты на серии из сотни тестовых задач. 

При использовании в качестве определяющего правила о необходимости запуска локального метода decision tree, нам удалось получить достаточно хорошее ускорение. Применяя параллельную версию Алгоритма Глобального поиска ускорение удалось сохранить, что является несомненным преимуществом. Таким образом, наша схема позволяет получить преимущество от обоих из подходов (и от распараллеливания, и от запуска локального метода).





%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\begin{thebibliography}{6}
%

\bibitem{fio_bib1}
Ferreiro A.M., Garcia J.A., Lopez-Salas J.G., Vazquez C. An efficient implementation of parallel simulated annealing algorithm in GPUs // Journal of global optimization. — 2013. — Vol. 57, No. 3. — P. 863–890.

\bibitem{fio_bib2}
Garcia-Martinez J.M., Garzon E.M., Ortigosa P.M. A GPU implementation of a hybrid evolutionary algorithm: GPuEGO // Journal of super-computing. — 2014. — Vol. 70, No. 2. — P. 684–695.

\bibitem{fio_bib3}
Langdon W.B. Graphics processing units and genetic programming: an overview // Soft Computing. — 2011. — Vol. 15, No 8. — P. 1657–1669.

\bibitem{fio_bib4}
Евтушенко Ю.Г., Малкова В.У., Станевичюс А.А. Параллельный поиск глобального экстремума функций многих переменных // Ж. вычисл. матем. и матем. физ. — 2009. — Т. 49, №2. — С. 255–269.

\bibitem{fio_bib5}
He J., Verstak A., Watson L.T., Sosonkina M. Design and implementation of a mas-sively parallel version of DIRECT // Computational optimization and applications. — 2008. — Vol. 40, No. 2. — P. 217–245

\bibitem{fio_bib6}
Paulavicius R., Žilinskas J., Grothey A. Parallel branch and bound for global optimiza-tion with combination of Lipschitz bounds // Optimization methods and software. — 2011. — Vol. 26, No. 3. — P. 487–498.

\bibitem{fio_bib7}
Глобальная оптимизация: приложения и вычислительная сложность. %URL: http://hpc-education.unn.ru/ru/globopt/глобальная-оптимизация-приложения-и (дата обращения: 10.10.2020).

\bibitem{fio_bib8}
Захарова Е.М., Минашина И.К. Обзор методов многомерной оптимизации. Информационные процессы // том 14 - №3 – 2014 - С. 256-274.

\bibitem{fio_bib9}
Городецкий С. Ю. Методические материалы к лабораторной работе «Вычислительные методы поиска локальных минимумов функций», 2001.% URL: http://itmm.unn.ru/files/2016/09/MO_Lab2_LocOpt.pdf (дата обращения: 09.07.2022).

\bibitem{fio_bib10}
Шефов К. С., Степанова М. М. Реализация и применение параллельного алгоритма глобального поиска минимума к задаче оптимизации параметров молекулярно-динамического потенциала ReaxFF: %URL: http://crm.ics.org.ru/uploads/crmissues/crm_2015_3/15750.pdf, 2015 (дата обращения: 13.10.2020).

\bibitem{fio_bib11}
Gergel V.P. A global optimization algorithm for multivariate functions with lipschitzian first derivatives / Gergel V.P. // Journal of Global Optimization. – 1997. – Vol. 10, No. 3. – P. 257-281.

\bibitem{fio_bib12}
Barkalov K.A. A global optimization technique with an adaptive order of checking for constraints / Barkalov K.A., Strongin R.G. // Computational Mathematics and Mathematical Physics. – 2002. – Vol. 42, No. 9. – P. 1289-1300.

\bibitem{fio_bib13}
Gaviano, M. Software for generation of classes of test functions with known local and global minima for global optimization/ M. Gaviano, D. Lera, D. E. Kvasov, Y. D. Sergeyev // ACM Transactions on Mathematical Software. – 2003. – Vol. 29. – P. 469-480.

\bibitem{fio_bib14}
Химмельблау Д. Прикладное нелинейное программирование // М.: Мир - 1975 - 536 с. 

\bibitem{fio_bib15}
Nelder J., Mead R. A simplex method for function minimization // Computer Journal - 7(4) – 1965 - P. 308-313.

\bibitem{fio_bib16}
OpenCV (Open Source Computer Vision) documentation. %URL: https://docs.opencv.org/4.x/dc/dd6/ml_intro.html (дата обращения: 10.07.2022)

\bibitem{fio_bib17}
Сергеев, Я.Д. Диагональные методы глобальной оптимизации / Я.Д. Сергеев, Д.Е. Квасов – М.: Физматлит, 2008. – 352 c.

\bibitem{fio_bib18}
Sysoyev, A.,  Barkalov, K.,  Sovrasov, V.,  Lebedev, I.,  Gergel, V. Globalizer – A parallel software system for solving global optimization problems. Lecture Notes in Computer Science. Volume 10421 LNCS, 2017, Pages 492-499

\bibitem{fio_bib19}
Sergeyev, Ya.D. Global search based on efficient diagonal partitions and a set of Lipschitz constants / Ya.D. Sergeyev, D.E. Kvasov // SIAM Journal on Optimization. – 2006. Vol. 16, No. 3. – P. 910–937

\bibitem{fio_bib20}
Стронгин Р.Г. Параллельные вычисления в задачах глобальной оптимизации / Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А. – М.: Издательство Московского университета, 2013. 280 с.









\end{thebibliography}
\end{document}
