%%%%%%%%%%%%%%%%%%%% author.tex 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%
\usepackage{graphicx}
\usepackage{marvosym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}

\usepackage[russian]{babel}

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\def\UrlFont{\rmfamily}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
% Решение обратных задач химической кинетки с помощью асинхронного алгоритма глобального поиска
% Решение обратных задач химической кинетки с помощью смешанного локально-глобального поискового алгоритма 
%\title{Solving the Inverse Problems of Chemical Kinetics Using the Asynchronous Global Optimization Algorithm}
\title{
Solving ...
}
\titlerunning{Solving the Inverse Problems of Chemical Kinetics}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{
Irek Gubaydullin$^{1,2}$\and
Leniza Enikeeva$^{2,3}$\orcidID{0000-0003-4219-4870}
\and
Konstantin Barkalov$^4$ \letter \orcidID{0000-0001-5273-2471}
\and
Marina Usova$^4$\orcidID{XXX} 
}

%
\authorrunning{I. Gubaydullin et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Konstantin Barkalov and Ilya Lebedev }
%

\institute{$^1$Institute of Petrochemistry and Catalysis – Subdivision of the Ufa Federal Research Centre of RAS, Ufa, Russia\\$^2$Ufa State Petroleum Technological University, Ufa, Russia \\$^3$Novosibirsk State University, Novosibirsk, Russia\\$^4$Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia\\
\email{leniza.enikeeva@yandex.ru},
\email{konstantin.barkalov@itmm.unn.ru},
\email{ilya.lebedev@itmm.unn.ru}
}

	
\maketitle              % typeset the title of the contribution

\begin{abstract}

The paper considers ...
%the application of parallel computing technology to the simulation of catalytic chemical reaction, which is widely used in the modern automobile industry to produce gasoline with high octane number. As a chemical reaction, the process of alkylation of isobutane with mixed C4 olefins catalyzed by sulfuric acid is assumed. To simulate a chemical process, it is necessary to develop a kinetic model of the process, that is, to determine the kinetic parameters. To do this, the inverse problem of chemical kinetics is solved, which predicts the values of kinetic parameters based on laboratory data. From a mathematical point of view, the inverse problem of chemical kinetics is a global optimization problem. A parallel information-statistical global search algorithm was used to solve it. The use of the parallel algorithm has significantly reduced the search time to find the optimum. The found optimal parameters of the model made it possible to adequately simulate the process of alkylation of isobutane with mixed C4 olefins catalyzed by sulfuric acid.

%обновить ключевые слова
\keywords{Global optimization $\cdot$ Multiextremal functions $\cdot$ Parallel computing $\cdot$ Chemical kinetics $\cdot$ Inverse problems }
\end{abstract}

\section{Introduction}

%Часть УГНТУ
%Currently, there is a tendency to improve the environmental characteristics of automobile fuel while maintaining a high octane number. Sulfuric acid alkylation of isobutane with olefins makes it possible to obtain a high-octane component of gasoline with a minimum content of aromatic hydrocarbons. The alkylate, which is produced by the alkylation of isobutane with C3-C5 olefins in the presence of strong acid, has the advantages of high octane number, low vapor pressure, and zero content of olefins and aromatics that allow it to be a desirable blending component for high-quality gasoline. Alkylates will continue to act as a desirable blending component for high-quality gasoline as the quality of gasoline continues to increase [ссылка на китайскую статью]. Therefore, it is a significant process of the modern refinery [ссылка на catalyst]. To optimize the chemical process in industry, it is necessary to first develop its model, which in this case means building a mathematical model of the chemical process and then its kinetic model, that is, numerically calculate the kinetic constants of the reaction. 
%Стыковочный абзац

Так как математической моделью реакции является система дифференциальных уравнений, найти значения констант, входящих в нее, можно только численно (см., например, \cite{RSD}). Отметим, что в таких задачах целевая функция, как правило, является многоэкстремальной, т.е. имеет много локальных экстремумов наряду с глобальным. 
%Часть ННГУ
Численные методы решения подобных многоэкстремальных задач (методы глобальной оптимизации) существенно отличаются от методов локального поиска (see, e.g., \cite{Sergeyev2017,PaulaviciusZilinskas2014}).  
С алгоритмической точки зрения их можно разделить на два класса: метаэвристические и детерминированные. Метаэвристические алгоритмы, в основном, основаны на имитации процессов, протекающих в живой природе.
Характерными примерами таких алгоритмов являются simulated annealing, evolution and genetic algorithms и т.д. (see, e.g., \cite{Battiti2009,Eiben2015}). В силу своей относительной простоты метаэвристические алгоритмы более популярны у исследователей, чем детерминированные.  Однако решение задачи, найденное метаэвристическим алгоритмом, является, вообще говоря, локальным и может располагаться далеко от глобального \cite{Kvasov2018}. 

Предполагая наличие некоторых дополнительных свойств целевой функции, можно построить эффективные детерминированные методы поиска глобального решения.
Например, можно предположить, что отношение приращения функции к соответствующему приращению аргумента не может превышать некоторого порога. В таком случае функции называются липшицевыми, а сама задача -- задачей липшицевой глобальной оптимизации . 
Данная статья продолжает цикл работ, в которых проводится исследование и модификация параллельных методов липшицевой оптимизации, предложенных в \cite{Strongin2000}, при их использовании для решения обратных задач химической кинетики. 

Основная часть статьи имеет следующую структуру. Описание математической модели исследуемой химической реакции приведено в Section 2. Формальная постановка задачи липшицевой глобальной оптимизации и общая схема поисковых алгоритмов приводятся в Section 3. В Section 4 изложена схема предлагаемого асинхронного параллельного алгоритма для решения многоэкстремальных задач. Результаты численного решения the inverse problem of chemical kinetics обсуждаются в Section 5.


\section{Problem Statement}\label{Sec_math_mod}
%Содержательная постановка задачи


\section{Parallel Algorithm for Solving Global Optimization Problems }\label{Sec_GSA}

\subsection{Global Optimization Problem}

Как уже говорилось выше, задача идентификации значений параметров математической модели может рассматриваться как Lipschitz global optimization problem. 
С формальной точки зрения  данная задача является задачей математического программирования вида
\begin{gather}
 \varphi^* = \varphi(y^\ast)=\min{\left\{\varphi(y):y\in D\right\}}, \label{problemN}\\
 D=\left\{y\in R^N: a_i\leq y_i \leq b_i, \;  1\leq i \leq N\right\} \label{D},
\end{gather}
где $a,\; b$ есть заданные векторы, $a,b\in R^N$, $\varphi(y)$ целевая функция, которая удовлетворяет the Lipschitz condition
\begin{equation}\label{Lip}
\left|\varphi(y_1)-\varphi(y_2)\right|\leq L\left\|y_1-y_2\right\|,\; y_1,y_2 \in D.
\end{equation}


Функция $\varphi(y)$ предполагается многоэкстремальной и заданной в виде ``black box'' (т.е. в виде некоторой subroutine, на вход которой подается вектор параметров, а на выходом является вычисленное значение функции). Кроме того, подразумевается, что каждое применение этой процедуры (называемое в дальнейшем search trial) является трудоемкой операцией. Такая постановка задачи полностью соответствует the inverse problem of chemical kinetics.

Известен ряд алгоритмов, применимых для решения задач липшицевой оптимизации. Среди них можно отметить non-uniform space covering method \cite{Evtushenko2013,Evtushenko2009}, diagonal and simplicial partitions methods \cite{Zilinskas2010,Paulavicius2011}. 
В данной работе мы использовали global search algorithm, proposed by Strongin \cite{Strongin2000}.
В рамках данного подхода исходная многомерная задача (\ref{problemN}) редуцируется к задаче одномерной оптимизации при помощи Peano-Hilbert curves. 

В самом деле, используя непрерывное однозначное отображение (Peano-Hilbert curve) $y(x)$ отрезка $[0,1]$ вещественной оси на гиперинтервал $D$ из (\ref{D}) можно свести многомерную задачу (\ref{problemN}) к одномерной задаче
\[
\varphi(y^\ast)=\varphi(y(x^\ast))=\min{\left\{\varphi(y(x)): x\in[0,1]\right\}},
\]
where the function $\varphi(y(x))$ will satisfy a uniform H{\"o}lder condition
\[
\left|\varphi(y(x_1))-\varphi(y(x_2))\right|\leq H\left|x_1-x_2\right|^{1/N}
\]
with $ H=2 L \sqrt{N+3}$.
Вопросы численного построения различных аппроксимаций Peano-Hilbert curve рассмотрены в \cite{Strongin2000,Sergeyev2013}.

Таким образом, search trial в некоторой точке $x'\in[0,1]$ будет включать в себя сначала the construction of the image $y'=y(x')$, и лишь затем вычисление значения функции $ z' = \varphi(y')$.

\subsection{Characteristic Algorithms}

%Характеристический алгоритм
Примененный нами алгоритм глобального поиска принадлежит классу characteristic algorithms, что существенно упрощает процесс его распараллеливания.
Напомним, что а numerical optimization method belongs to the class of characteristical algorithms if the algorithmic scheme of this method can be described as follows.

At the first iteration the trials are performed at the points $x^1 = 0$ and $x^2 = 1$. Then any next $(k+1)$, $k \geq 2$, iteration og global search is executed in the following way.

\begin{enumerate}
	\item 
Renumber points of previous trials by increasing their $x$-coordinates (the new order is marked with subscript)
\[
0=x_0<x_2<...<x_{k}=1.
\]

	\item 
Calculate for every interval $(x_{i-1},x_i), 1\leq i\leq k$, a value $R(i)$ called \textit{the interval characteristic}. In general case, $R(i)$ can depend on the points $x^i$ and the trial rezults $z^i=f(x^i), 1 \leq i \leq k$.

	\item 
Find the interval $(x_{t-1},x_t)$ which has the largest characteristic $R(t)$, i.e.
\[
R(t) = \max \left\{ R(i): \; 1\leq i\leq k \right\}.
\]

	\item 
Examine the stop condition
\[
\Delta_t \leq \epsilon ,
\]
where $\epsilon>0$ is a given accuracy. If the stop condition is satisfied, then the global search has to be terminated; otherwise the calculations should continue.

	\item 
Select a point $x^{k+1}$ of the current iteration within the interval $(x_{t-1},x_t)$ in accordance with some rule $S(t)$, i.e.
\[
x^{k+1} = S(t)\in(x_{t-1},x_t).
\]
	
	\item 
Calculate the function value $z^{k+1} = f(x^{k+1})$

	\item 
Evaluate a global minimum estimate 

	\item 
Increase the iteration number $k=k+1$ and proceed to the next iteration.
\end{enumerate}


As a possible interpretation of this scheme, we can consider an interval characteristic $R(i)$ as a measure of the global minimum being within the interval
$(x_{i-1},x_i)$.
To construct a concrete form of the interval characteristic we can use a lower envelope (or minorant) of the function to be minimized or a mathematical expectation of function values, etc. 
Most well-known global optimization methods can be formulated in accordance with this characteristical scheme, e.g.,
\begin{itemize}
\item a uniform grid method with a successively reduced step;
\item a random search (Monte-Carlo) algorithm;
\item the Piyavskii method;
\item one-step bayesian methods proposed by Kushner and Zilinskas;
\item information algorithms proposed by Strongin.
\end{itemize}
All these methods are based on different mathematical models but are presented in the general characteristical scheme.

It should be also noted that the length of the interval with the largest characteristic is examined at the stop condition.
It is possible if the optimization method converges to the global minimum.

Details of these methods are given in the sources cited. %Здесь же отметим основные моменты

For the Piyavskii method the interval characteristic $R(i)$ is an estimate (with the inverse sign) of the least value of the objective function $f(x)$ within the
interval $(x_{i-1},x_i)$. As a result, the point of a new trial is taken within the interval containing the estimate of the least value of $f(x)$ over the search domain.

The Kushner technique and the Zilinskas method have been constructed in the framework of the approach when the objective function is considered as
a sample of some Wiener process. For the Kushner technique the point $x^{k+1}$ of the current iteration is the most probable point at which the function value
$f(x^{k+1})$ is not greater than the value
\[
z_k^* -\gamma (z_k^+-z_k^*),
\]
where $z_k^+$ and $z_k^*$ are estimates of maximum and minimum function values respectively. For the Zilinskas method $x^{k+1}$ is the point where the maximum average improvement of the current estimate of the global extremum is expected.

The Strongin algorithm has been constructed in the framework of the information approach to global optimization (see Strongin (1978)). This method has an adaptive scheme to evaluate a numerical estimate of the unknown Lipschitz constant.


\subsection{Parallel Asynchronous Global Search Algorithm}

Принадлежность Global Search Algorithm (GSA) к классу caracteristic algorithms подсказывает возможный путь его распараллеливания. 
Как уже было отмечено, характеристика интервала $R(i)$ может рассматриваться как некоторая мера нахождения (локализации?) в нем точки глобального минимума. 
Тогда вместо одного лучшего интервала можно выбрать сразу несколько интервалов с наибольшими характеристиками и провести в них очередные испытания параллельно.
Более того, схема характеристических алгоритмов допускает и асинхронное распараллеливание, которое минимизирует простой процессоров в случае, если трудоемкость проведения испытаний зависит от конкретной точки области поиска. 

Рассмотрим более детально асинхронный алгоритм глобального поиска. В нем реализована схема распараллеливания вида ``master/worker''. В процессе-мастере проводится накопление поисковой информации, оценка на ее основе константы Липшица для целевой функции, определение точек новых испытаний и распределение их по процессам-рабочим. Процессы-рабочие получают от мастера точки, проводят в них новые испытания и отсылают мастеру их результаты. 

При описании параллельного алгоритма будем предполагать, что в нашем распоряжении имеется $p+1$ вычислительный процесс: один мастер и $p$ рабочих.
 
В начале поиска процесс-мастер (пусть это будет процесс No 0) инициирует параллельное проведение $p$ испытаний в $p$ различных точках области поиска, две из которых являются граничными, а остальные -- внутренними, т.е. в точках $\{y(x^1), y(x^2), ...,y(x^p)\}$, где $x^1 = 0$, $x^p = 1$, $x^i\in(0,1), i=2,..., p-1$.

Предположим теперь, что выполнено $k$ испытаний (в частности, $k$ может быть равно 0), и процессы-рабочие проводят испытания в точках $\{y(x^{k+1}), y(x^{k+2}), ...,y(x^{k+p})\}$. 

Если процесс-рабочий завершил проведение испытания в некоторой точке (пусть это будет точка $y(x^{k+1})$, соответствующая процессу No 1), то он пересылает процессу-мастеру результаты испытания. 
Отметим, что в данном случае мы будем иметь set of preimages of the trial points
\[
I_k = \left\{ x^{k+1},x^{k+2},...,x^{k+p} \right\},
\]
в которых испытания уже начались, но еще не завершены.

Получив от рабочего процесса результаты испытания в точке $y(x^{k+1})$ мастер, в свою очередь, выбирает для него точку нового испытания $x^{k+p+1}$ в соответствии с правилами, которые соответствуют схеме характеристического алгоритма.

1. Перенумеровать в порядке возрастания (нижним индексом) set of preimages of the trial points 
\[
X_k = \left\{x^1, x^2,...,x^{k+p} \right\},
\]
которое содержит все preimages, в которых либо проведены, либо проводятся испытания, т.е.
\[
0=x_1<x_2<...<x_{x+p}=1.
\]
2. Вычислить значения 
\[
M_1=\max \left\{ \frac{ \left|z_i - z_{i-1} \right|}{(x_i-x_{i-1})^{1/N}} : x_{i-1} \notin I_k, x_i \notin I_k, 2\leq i\leq k+p \right\},
\]
\[
M_2=\max \left\{ \frac{ \left|z_{i+1} - z_{i-1} \right|}{(x_{i+1}-x_{i-1})^{1/N}} : x_i \in I_k, 2\leq i < k+p \right\},
\]
\[
M=\max\{M_1,M_2\},
\]
где $z_i=\varphi(y(x_i))$, if $x_i \notin I_k, \; 1\leq i \leq k+p$. Значения $z_i$ в точках $x_i \in I_k$ являются неопределенными, т.к. испытания в точках $x_i \in I_k$ еще не завершены. Если значение $M$ получилось равным 0, то положить $M=1$.

3. Каждому интервалу $(x_{i-1},x_i), \; x_{i-1} \notin I_k, x_i \notin I_k, \; 2\leq i\leq k+p$, поставить в соответствие число $R(i)$, которое называется характеристикой интервала и вычисляется по формуле
\[
R(i)=rM\Delta_i+\frac{(z_i-z_{i-1})^2}{rM\Delta_i}-2(z_i+z_{i-1}),
\]
где $\Delta_i=\left(x_i-x_{i-1}\right)^{1/N}$, and $r>1$ is the reliability parameter of the method.

4. Выбрать интервал $[x_{t-1},x_t]$, которому соответствует максимальная характеристика, т.е.
\[
R(t) = \max \left\{ R(i): \; x_{i-1} \notin I_k, x_i \notin I_k, \; 2\leq i\leq k+p \right\}.
\]

5. Определить preimage $x^{k+p+1} \in (x_{t-1},x_t)$ точки нового испытания в соответствии с формулой
\[
x^{k+p+1} = \frac{x_{t}+x_{t-1}}{2} - \mathrm{sign}(z_{t}-z_{t-1})\frac{1}{2r}\left[\frac{\left|z_{t}-z_{t-1}\right|}{M}\right]^N.
\]

Сразу после вычисления точки очередного испытания $y^{k+p+1} = y(x^{k+p+1})$ процесс-мастер добавляет ее в множество $I_k$ и пересылает процессу-рабочему, который инициирует проведение испытания в ней. 

Процесс-мастер завершает работу алгоритма при выполнении одного из двух условий: $\Delta_{t}<\epsilon$, $k+p>K_{max}$.
Первое из них соответствует остановке алгоритма по точности, второе --- по числу испытаний. 
Вещественное число $0<\epsilon<1$ и целое число $K_{max}>0$ являются параметрами алгоритма.



\section{Numerical Experiments}\label{Sec_Exp}

Предварительные эксперименты позволили оценить примерное время одного вычисления целевой функции при использовании последовательного алгоритма. Эксперименты проводились на архитектуре Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz 2.59 GHz. Среднее время вычисления целевой функции составило порядка $0.2$ сек., что характеризует задачу как вычислительно сложную. Решение задачи последовательным алгоритмом потребовало бы не менее 1 млн. испытаний и, соответственно, не менее 54 часов (оценочное значение).
Поэтому для проведения расчетов использовался только параллельный алгоритм с критерием остановки по числу испытаний $K_{max}=10^6$. 

Численные эксперименты проводились при помощи параллельного асинхронного алгоритма, изложенного в Section 4, с использованием $160$ процессов ($159$ процессов вычисляли значения целевой функции, $1$ процесс управлял алгоритмом). Для проведения экспериментов использовался UNN supercomputer Lobachevsky (CentOS 7.2, SLURM, two CPU Intel Sandy Bridge E5-2660 2.2 GHz and 64 Gb RAM on the node). Асинхронный алгоритм глобальной оптимизации был реализован на C++ (GCC 9.5.0 and Intel MPI were used); вычисление значений целевой функции было реализовано в Python 3.9.

%Ускорение составило примерно $230$ раз.
Малое время решения задачи при использовании параллельного алгоритма (примерно 14 минут на 1 задачу) позволило провести полноценное исследование зависимости решения от параметра регуляризации $\alpha$. В таблице \ref{table_1} приведены минимальные значения objective function, найденные при соответствующем значении параметра регуляризации, время решения задачи (в секундах) и time speedup по сравнению с оценкой времени решения задачи последовательным методом. При запусках были использованы параметр метода $r=4.0$ из (\ref{R}) и точность $\epsilon = 10^{-4}\left\|b-a\right\|$. После выполнения заданного числа итераций или достижения методом глобального поиска заданной точности решение уточнялось Hook-Jeeves local method \cite{HookJeeves} с точностью $\epsilon = 10^{-4}\left\|b-a\right\|$.

Детальное исследование области значений параметра регуляризации в окрестности $\alpha$ равного нулю (от 0.1 до 0.01 с шагом 0.01 и от 0.01 до 0.001 с шагом 0.001) выявило убывание значения минимума с уменьшением параметра $\alpha$. Лучшее решение $0.540750$ было получено при параметре регуляризации $\alpha = 0.001$.


\begin{table}
\caption{Исследование влияния параметра регуляризации alpha на минимальное значение функции}
\label{table_1}
\begin{center}
\begin{tabular}{cccc}
\hline\noalign{\smallskip}
 Alpha      & Minimum  & Time (sec.) & Speedup \\
\hline\noalign{\smallskip}
0.1		&	0.810454	&	806.192	&	241       \\
0.09	&	0.837868	&	828.649	&	234       \\
0.08	&	0.808238	&	811.852	&	239       \\
0.07	&	0.759134	&	803.626	&	242       \\
0.06	&	0.723404	&	810.9	&	240       \\
0.05	&	0.729415	&	804.674	&	242       \\
0.04	&	0.736035	&	816.387	&	238       \\
0.03	&	0.68152		&	809.185	&	240       \\
0.02	&	0.669092	&	155.837	&	3         \\
0.01	&	0.632944	&	825.238	&	235       \\
0.009	&	0.63001		&	814.48	&	239       \\
0.008	&	0.604817	&	830.448	&	234       \\
0.007	&	0.619546	&	807.335	&	241       \\
0.006	&	0.612614	&	834.703	&	233       \\
0.005	&	0.615638	&	815.45	&	238       \\
0.004	&	0.579725	&	821.421	&	237       \\
0.003	&	0.583529	&	831.454	&	234       \\
0.002	&	0.547907	&	850.017	&	229       \\
0.001	&	0.54075		&	853.898	&	227       \\
\noalign{\smallskip}\hline
\end{tabular}\end{center}\end{table}



\section{Conclusions and Future Work}



\medskip

\textbf{Acknowledgments}. This study was supported by the Russian Science Foundation, project No.\,21-11-00204 and by RFBR, project No.\,19-37-60014.

%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}

\end{document}
