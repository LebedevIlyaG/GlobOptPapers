%%%%%%%%%%%%%%%%%%%% author.tex 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%
\usepackage{graphicx}
\usepackage{marvosym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}

\usepackage[russian]{babel}

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{hyperref}
\def\UrlFont{\rmfamily}

\def\orcidID#1{\unskip$^{[#1]}$}
\def\letter{$^{\textrm{(\Letter)}}$}

\begin{document}
\mainmatter              % start of a contribution
% Решение обратных задач химической кинетки с помощью асинхронного алгоритма глобального поиска
% Решение обратных задач химической кинетки с помощью смешанного локально-глобального поискового алгоритма 
%\title{Solving the Inverse Problems of Chemical Kinetics Using the Asynchronous Global Optimization Algorithm}
\title{
Solving ...
}
\titlerunning{Solving the Inverse Problems of Chemical Kinetics}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{
Irek Gubaydullin$^{1,2}$\and
Leniza Enikeeva$^{2,3}$\orcidID{0000-0003-4219-4870}
\and
Konstantin Barkalov$^4$ \letter \orcidID{0000-0001-5273-2471}
\and
Marina Usova$^4$\orcidID{XXX} 
}

%
\authorrunning{I. Gubaydullin et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Konstantin Barkalov and Ilya Lebedev }
%

\institute{$^1$Institute of Petrochemistry and Catalysis – Subdivision of the Ufa Federal Research Centre of RAS, Ufa, Russia\\$^2$Ufa State Petroleum Technological University, Ufa, Russia \\$^3$Novosibirsk State University, Novosibirsk, Russia\\$^4$Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia\\
\email{leniza.enikeeva@yandex.ru},
\email{konstantin.barkalov@itmm.unn.ru},
\email{ilya.lebedev@itmm.unn.ru}
}

	
\maketitle              % typeset the title of the contribution

\begin{abstract}

The paper considers ...
%the application of parallel computing technology to the simulation of catalytic chemical reaction, which is widely used in the modern automobile industry to produce gasoline with high octane number. As a chemical reaction, the process of alkylation of isobutane with mixed C4 olefins catalyzed by sulfuric acid is assumed. To simulate a chemical process, it is necessary to develop a kinetic model of the process, that is, to determine the kinetic parameters. To do this, the inverse problem of chemical kinetics is solved, which predicts the values of kinetic parameters based on laboratory data. From a mathematical point of view, the inverse problem of chemical kinetics is a global optimization problem. A parallel information-statistical global search algorithm was used to solve it. The use of the parallel algorithm has significantly reduced the search time to find the optimum. The found optimal parameters of the model made it possible to adequately simulate the process of alkylation of isobutane with mixed C4 olefins catalyzed by sulfuric acid.

%обновить ключевые слова
\keywords{Global optimization $\cdot$ Multiextremal functions $\cdot$ Parallel computing $\cdot$ Chemical kinetics $\cdot$ Inverse problems }
\end{abstract}

\section{Introduction}

%Часть УГНТУ
%Currently, there is a tendency to improve the environmental characteristics of automobile fuel while maintaining a high octane number. Sulfuric acid alkylation of isobutane with olefins makes it possible to obtain a high-octane component of gasoline with a minimum content of aromatic hydrocarbons. The alkylate, which is produced by the alkylation of isobutane with C3-C5 olefins in the presence of strong acid, has the advantages of high octane number, low vapor pressure, and zero content of olefins and aromatics that allow it to be a desirable blending component for high-quality gasoline. Alkylates will continue to act as a desirable blending component for high-quality gasoline as the quality of gasoline continues to increase [ссылка на китайскую статью]. Therefore, it is a significant process of the modern refinery [ссылка на catalyst]. To optimize the chemical process in industry, it is necessary to first develop its model, which in this case means building a mathematical model of the chemical process and then its kinetic model, that is, numerically calculate the kinetic constants of the reaction. 
%Стыковочный абзац


Since the mathematical model of the chemical reaction is a system of differential equations, it is only possible to find the values of the constants in this system numerically (see, e.g., \cite{RSD}). Note that the objective function in such problems is usually multiextremal, i.e. it has many local extrema along with a global one.
%Часть ННГУ
Numerical methods for solving such multiextremal problems (global optimization methods) differ significantly from local search methods (see, e.g., \cite{Sergeyev2017, PaulaviciusZilinskas2014}). From an algorithmic point of view, they can be divided into two classes: metaheuristic and deterministic. Metaheuristic algorithms are mainly based on the simulation of processes occurring in living nature. Typical examples of such algorithms are simulated annealing, evolution and genetic algorithms, etc. (see, e.g., \cite{Battiti2009,Eiben2015}).     Due to their relative simplicity, metaheuristic algorithms are more popular with researchers than deterministic ones.  However, the solution to a problem found by a metaheuristic algorithm is, generally speaking, local and may be well away from the global solution \cite{Kvasov2018}.

Assuming some additional properties of the objective function, it is possible to construct efficient deterministic methods for finding a global solution. For example, we can assume that the ratio of the increment of the function to the corresponding increment of the argument cannot exceed some threshold. In this case, the functions are called Lipschitz functions, and the problem itself is a Lipschitz global optimization problem.
This paper continues the series of papers where the parallel methods of Lipschitz optimization proposed in \cite{Strongin2000} are investigated and modified in their application to solving inverse problems of chemical kinetics.

The main part of the paper is organized as follows. Section 2 describes the mathematical model of the chemical reaction under study. The formal statement of the Lipschitz global optimization problem and the general scheme of search algorithms are given in Section 3. In Section 4, a scheme of the proposed asynchronous parallel algorithm for solving multiextremal problems is presented. The results of numerical solution of the inverse problem of chemical kinetics are discussed in Section 5.


\section{Problem Statement}\label{Sec_math_mod}
%Содержательная постановка задачи


\section{Parallel Algorithm for Solving Global Optimization Problems }\label{Sec_GSA}

\subsection{Global Optimization Problem}

As it was mentioned above, the problem of identifying mathematical model parameter values can be considered a Lipschitz global optimization problem. From the formal point of view this problem is a mathematical programming problem of the form
\begin{gather}
 \varphi^* = \varphi(y^\ast)=\min{\left\{\varphi(y):y\in D\right\}}, \label{problemN}\\
 D=\left\{y\in R^N: a_i\leq y_i \leq b_i, \;  1\leq i \leq N\right\} \label{D},
\end{gather}
where $a,\; b$ are the given vectors, and $a,b\in R^N$, $\varphi(y)$ is the objective function which satisfies the Lipschitz condition
\begin{equation}\label{Lip}
\left|\varphi(y_1)-\varphi(y_2)\right|\leq L\left\|y_1-y_2\right\|,\; y_1,y_2 \in D.
\end{equation}


The function $\varphi(y)$ is assumed to be multiextremal and is given as a ``black box'' (i.e. as some subroutine with a vector of parameters as its input and the calculated value of the function as its output). Moreover, it is assumed that each application of this procedure (hereinafter referred to as search trial) is a time-consuming operation. This formulation of the problem is fully consistent with the inverse problem of chemical kinetics.

There are several algorithms that can be applied to solve Lipschitz optimization problems. These include, among others, the non-uniform space covering method \cite{Evtushenko2013, Evtushenko2009}, diagonal and simplicial partitions methods \cite{Zilinskas2010, Paulavicius2011}. In this paper, we used the global search algorithm proposed by Strongin{Strongin2000}. Under this approach, the original multidimensional problem (\ref{problemN}) is reduced to a one-dimensional optimization problem using Peano-Hilbert curves.

In fact, using a continuous one-to-one mapping (Peano-Hilbert curve) $y(x)$ of the segment $[0,1]$ of the real axis onto the hyperinterval $D$ from (\ref{D}), we can reduce the multidimensional problem (\ref{problemN}) to a one-dimensional problem
\[
\varphi(y^\ast)=\varphi(y(x^\ast))=\min{\left\{\varphi(y(x)): x\in[0,1]\right\}},
\]
where the function $\varphi(y(x))$ will satisfy a uniform H{\"o}lder condition
\[
\left|\varphi(y(x_1))-\varphi(y(x_2))\right|\leq H\left|x_1-x_2\right|^{1/N}
\]
with $ H=2 L \sqrt{N+3}$.
The numerical construction of various approximations of the Peano-Hilbert curve is discussed in \cite{Strongin2000,Sergeyev2013}.

Thus, the search trial at some point $x'\in[0,1]$ will involve first the construction of the image $y'=y(x')$, and only then the computation of the function value $ z' = \varphi(y')$.

\subsection{Characteristic Algorithms}

%Характеристический алгоритм
The global search algorithm we used belongs to the class of characteristic algorithms, which greatly simplifies the process of its parallelization. Recall that a numerical optimization method belongs to the class of characteristic algorithms if the algorithmic scheme of this method can be described as follows.

At the first iteration the trials are performed at the points $x^1 = 0$ and $x^2 = 1$. Then any next $(k+1)$, $k \geq 2$, iteration of global search is executed in the following way.

\begin{enumerate}
	\item 
Renumber points of the previous trials by increasing their $x$-coordinates (the new order is marked with subscript)
\[
0=x_0<x_2<...<x_{k}=1.
\]

	\item 
Calculate for every interval $(x_{i-1},x_i), 1\leq i\leq k$, a value $R(i)$ called \textit{the interval characteristic}. In the general case, $R(i)$ can depend on the points $x^i$ and the trial results $z^i=f(x^i), 1 \leq i \leq k$.

	\item 
Find the interval $(x_{t-1},x_t)$ which has the largest characteristic $R(t)$, i.e.
\[
R(t) = \max \left\{ R(i): \; 1\leq i\leq k \right\}.
\]

	\item 
Examine the stop condition
\[
\Delta_t \leq \epsilon ,
\]
where $\epsilon>0$ is a given accuracy. If the stop condition is satisfied, then the global search has to be terminated; otherwise the calculations should continue.

	\item 
Select a point $x^{k+1}$ of the current iteration within the interval $(x_{t-1},x_t)$ in accordance with some rule $S(t)$, i.e.
\[
x^{k+1} = S(t)\in(x_{t-1},x_t).
\]
	
	\item 
Calculate the function value $z^{k+1} = f(x^{k+1})$

	\item 
Evaluate a global minimum estimate 

	\item 
Increase the iteration number $k=k+1$ and proceed to the next iteration.
\end{enumerate}


As a possible interpretation of this scheme, we can consider an interval characteristic $R(i)$ as a measure of the global minimum being within the interval
$(x_{i-1},x_i)$.
To construct a concrete form of the interval characteristic, we can use a lower envelope (or minorant) of the function to be minimized or a mathematical expectation of function values, etc. 
Most well-known global optimization methods can be formulated in accordance with this characteristical scheme, e.g.,
\begin{itemize}
\item a uniform grid method with a successively reduced step;
\item a random search (Monte-Carlo) algorithm;
\item the Piyavskii method;
\item one-step bayesian methods proposed by Kushner and Zilinskas;
\item information algorithms proposed by Strongin.
\end{itemize}
All these methods are based on different mathematical models but are presented in the general characteristical scheme.

It should be also noted that the length of the interval with the largest characteristic is examined at the stop condition.
It is possible if the optimization method converges to the global minimum.

Details of these methods are given in the sources cited. %Здесь же отметим основные моменты

For the Piyavskii method the interval characteristic $R(i)$ is an estimate (with the inverse sign) of the least value of the objective function $f(x)$ within the
interval $(x_{i-1},x_i)$. As a result, the point of a new trial is taken within the interval containing the estimate of the least value of $f(x)$ over the search domain.

The Kushner technique and the Zilinskas method have been constructed in the framework of the approach when the objective function is considered as
a sample of some Wiener process. For the Kushner technique the point $x^{k+1}$ of the current iteration is the most probable point at which the function value
$f(x^{k+1})$ is not greater than the value
\[
z_k^* -\gamma (z_k^+-z_k^*),
\]
where $z_k^+$ and $z_k^*$ are estimates of maximum and minimum function values respectively. For the Zilinskas method $x^{k+1}$ is the point where the maximum average improvement of the current estimate of the global extremum is expected.

The Strongin algorithm has been constructed in the framework of the information approach to global optimization (see \cite{Strongin1978})). This method has an adaptive scheme to evaluate a numerical estimate of the unknown Lipschitz constant.


\subsection{Parallel Asynchronous Global Search Algorithm}

As the Global Search Algorithm (GSA) belongs to the class of characteristic algorithms, this suggests a possible way to parallelize it. 
As previously mentioned, the characteristic of the interval $R(i)$ can be regarded as some measure of finding the global minimum point in this interval. Then, instead of a single best interval, several intervals with the highest characteristics can be chosen at once and successive trials can be carried out in these intervals in parallel. Moreover, the scheme of characteristic algorithms also allows asynchronous parallelization, which minimizes downtime of processors when trial complexity depends on a particular point in the search domain. 

Now, let us examine the asynchronous global search algorithm in more detail. It implements a paralleling scheme of the ``master/worker'' type. The master process accumulates search information, evaluates on its basis the Lipschitz constant for the objective function, determines new trial points and distributes them to the worker processes. Worker processes receive the points from the master, perform new trials in these points and send the results to the master.

When describing the parallel algorithm, we assume that we have $p+1$ computational processes: one master and $p$ worker processes.
 
At the beginning of the search, the master process (let it be process No. 0) initiates $p$ parallel  trials at $p$ different points of the search domain, two of which are boundary points and the rest are internal points, i.e. at the points $\{y(x^1), y(x^2), ...,y(x^p)\}$, where $x^1 = 0$, $x^p = 1$, $x^i\in(0,1), i=2,..., p-1$.

Suppose now that $k$ trials have been performed (in particular, $k$ can be 0), and the worker processes perform trials at the points $\{y(x^{k+1}), y(x^{k+2}), ...,y(x^{k+p})\}$. 

If a worker process completes the trial at some point (let it be the point $y(x^{k+1})$ corresponding to process No. 1) then it sends to the master process the results of the trial. Note that in this case we will have a set of preimages of the trial points
\[
I_k = \left\{ x^{k+1},x^{k+2},...,x^{k+p} \right\},
\]
where the trials have already started but have not yet been completed.

Having received the trial results at the point $y(x^{k+1})$ from the worker process, the master in turn selects a new trial point $x^{k+p+1}$ for it according to the rules that correspond to the scheme of the characteristic algorithm.

\begin{enumerate}
	\item 
Renumber the set of preimages of the trial points 
\[
X_k = \left\{x^1, x^2,...,x^{k+p} \right\},
\]
containing all preimages at which the trials have either been carried out or are being carried out in ascending order (marked with subscript) , i.e.
\[
0=x_1<x_2<...<x_{x+p}=1.
\]
	\item
Calculate the values
\[
M_1=\max \left\{ \frac{ \left|z_i - z_{i-1} \right|}{(x_i-x_{i-1})^{1/N}} : x_{i-1} \notin I_k, x_i \notin I_k, 2\leq i\leq k+p \right\},
\]
\[
M_2=\max \left\{ \frac{ \left|z_{i+1} - z_{i-1} \right|}{(x_{i+1}-x_{i-1})^{1/N}} : x_i \in I_k, 2\leq i < k+p \right\},
\]
\[
M=\max\{M_1,M_2\},
\]
where $z_i=\varphi(y(x_i))$, if $x_i \notin I_k, \; 1\leq i \leq k+p$. The values of $z_i$ at the points $x_i \in I_k$ are undefined, as the trials at the points $x_i \in I_k$ have not yet been completed. If the value of $M$ equals 0, assume $M=1$.

	\item
Assign to each interval $(x_{i-1},x_i), \; x_{i-1} \notin I_k, x_i \notin I_k, \; 2\leq i\leq k+p$, the number $R(i)$, which is called the interval characteristic and is calculated using the formula
\[
R(i)=rM\Delta_i+\frac{(z_i-z_{i-1})^2}{rM\Delta_i}-2(z_i+z_{i-1}),
\]
where $\Delta_i=\left(x_i-x_{i-1}\right)^{1/N}$, and $r>1$ is the reliability parameter of the method.

	\item
Select the interval $[x_{t-1},x_t]$ to which the maximum characteristic corresponds, i.e.
\[
R(t) = \max \left\{ R(i): \; x_{i-1} \notin I_k, x_i \notin I_k, \; 2\leq i\leq k+p \right\}.
\]
	\item
Determine the preimage $x^{k+p+1} \in (x_{t-1},x_t)$ of the new trial point according to the formula
\[
x^{k+p+1} = \frac{x_{t}+x_{t-1}}{2} - \mathrm{sign}(z_{t}-z_{t-1})\frac{1}{2r}\left[\frac{\left|z_{t}-z_{t-1}\right|}{M}\right]^N.
\]
\end{enumerate}

Immediately after calculating the next trial point $y^{k+p+1} = y(x^{k+p+1})$, the master process adds it to the set $I_k$ and forwards it to the worker process which initiates a trial at this point.

The master process completes the algorithm when one of two conditions is met: $\Delta_{t}<\epsilon$, $k+p>K_{max}$.
). The first one corresponds to stopping the algorithm by accuracy, the second one, by the number of trials. The real number $0<\epsilon<1$ and the integer $K_{max}>0$ are the algorithm parameters.

\section{Numerical Experiments}\label{Sec_Exp}

In preliminary experiments, we estimated the approximate time to calculate one objective function using the sequential algorithm. The computer architecture used for the experiments was Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz 2.59 GHz. The average calculation time of the objective function was about $0.2$ sec, which characterizes the problem as being computationally complex.  Solving the problem with a sequential algorithm would require at least 1 million trials and thus at least 54 hours (as an estimate). Therefore only a parallel algorithm with a stopping criterion based on the number of trials $K_{max}=10^6$ was used for the calculations. 

Numerical experiments were performed using the parallel asynchronous algorithm outlined in Section 4, running $160$ processes: $159$ processes calculated the values of the objective function (worker processes) while $1$ master process controlled the algorithm. The Lobachevsky  supercomputer (University of Nizhni Novgorod)  (CentOS 7.2, SLURM, two Intel Sandy Bridge E5-2660 2.2 GHz CPUs and 64 Gb RAM on the node) was used for the experiments. The asynchronous global optimization algorithm was implemented in C++ (GCC 9.5.0 and Intel MPI were used); the computation of objective function values was implemented in Python 3.9.

%Ускорение составило примерно $230$ раз.
The short time for solving the problem when using the parallel algorithm (approximately 14 minutes for 1 problem) allowed a thorough investigation of the dependence of the solution on the regularisation parameter $\alpha$. The table \ref{table_1} shows the minimum values of the objective function found for the corresponding value of the regularization parameter, the time for solving the problem (in seconds) and the time speedup in comparison to the estimated problem solution time when using the sequential method. The method parameter $r=4.0$ из (\ref{R}) and the accuracy $\epsilon = 10^{-4}\left\|b-a\right\|$ were used in the runs. After a given number of iterations or after reaching the specified accuracy with the global search method, the solution was refined by the Hooke-Jeeves local method \cite{HookJeeves} with an accuracy of $\epsilon = 10^{-4}\left\|b-a\right\|$.

A detailed study of the region of the regularization parameter values in the neighborhood of $\alpha$ equal to zero (from $0.1$ to $0.01$ in steps of $0.01$ and from $0.01$ to $0.001$ in steps of $0.001$) revealed that the value of the minimum decreases with decreasing $\alpha$. The best solution of $0.54075$ was obtained with the regularisation parameter $\alpha = 0.001$.


\begin{table}
\caption{Исследование влияния параметра регуляризации alpha на минимальное значение функции}
\label{table_1}
\begin{center}
\begin{tabular}{cccc}
\hline\noalign{\smallskip}
 Alpha      & Minimum  & Time (sec.) & Speedup \\
\hline\noalign{\smallskip}
0.1		&	0.810454	&	806.192	&	241       \\
0.09	&	0.837868	&	828.649	&	234       \\
0.08	&	0.808238	&	811.852	&	239       \\
0.07	&	0.759134	&	803.626	&	242       \\
0.06	&	0.723404	&	810.9	&	240       \\
0.05	&	0.729415	&	804.674	&	242       \\
0.04	&	0.736035	&	816.387	&	238       \\
0.03	&	0.68152		&	809.185	&	240       \\
0.02	&	0.669092	&	155.837	&	3         \\
0.01	&	0.632944	&	825.238	&	235       \\
0.009	&	0.63001		&	814.48	&	239       \\
0.008	&	0.604817	&	830.448	&	234       \\
0.007	&	0.619546	&	807.335	&	241       \\
0.006	&	0.612614	&	834.703	&	233       \\
0.005	&	0.615638	&	815.45	&	238       \\
0.004	&	0.579725	&	821.421	&	237       \\
0.003	&	0.583529	&	831.454	&	234       \\
0.002	&	0.547907	&	850.017	&	229       \\
0.001	&	0.54075		&	853.898	&	227       \\
\noalign{\smallskip}\hline
\end{tabular}\end{center}\end{table}



\section{Conclusions and Future Work}



\medskip

\textbf{Acknowledgments}. This study was supported by the Russian Science Foundation, project No.\,21-11-00204 and by RFBR, project No.\,19-37-60014.

%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}

\end{document}
